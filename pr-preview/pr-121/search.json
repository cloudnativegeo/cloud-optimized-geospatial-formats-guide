[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "",
    "text": "Geospatial data is experiencing exponential growth in both size and complexity. As a result, traditional data access methods, such as file downloads, have become increasingly impractical for achieving scientific objectives. With the limitations of these older methods becoming more apparent, cloud-optimized geospatial formats present a much-needed solution.\nCloud optimization enables efficient, on-the-fly access to geospatial data, offering several advantages:\n\nReduced Latency: Subsets of the raw data can be fetched and processed much faster than downloading files.\nScalability: Cloud-optimized formats are usually stored on cloud object storage, which is infinitely scalable. Object storage supports many parallel read requests when combined with metadata about where different data bits are stored, making it easier to work with large datasets.\nFlexibility: Cloud-optimized formats allow for high levels of customization, enabling users to tailor data access to their specific needs. Additionally, advanced query capabilities provide the freedom to perform complex operations on the data without downloading and processing entire datasets.\nCost-Effectiveness: Reduced data transfer and storage needs can lower costs. Many of these formats offer compression options, which reduce storage costs.\n\nIf you want to provide optimized access to geospatial data, this guide is designed to help you understand the best practices and tools available for cloud-optimized geospatial formats.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#why-cloud-optimize",
    "href": "index.html#why-cloud-optimize",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "",
    "text": "Geospatial data is experiencing exponential growth in both size and complexity. As a result, traditional data access methods, such as file downloads, have become increasingly impractical for achieving scientific objectives. With the limitations of these older methods becoming more apparent, cloud-optimized geospatial formats present a much-needed solution.\nCloud optimization enables efficient, on-the-fly access to geospatial data, offering several advantages:\n\nReduced Latency: Subsets of the raw data can be fetched and processed much faster than downloading files.\nScalability: Cloud-optimized formats are usually stored on cloud object storage, which is infinitely scalable. Object storage supports many parallel read requests when combined with metadata about where different data bits are stored, making it easier to work with large datasets.\nFlexibility: Cloud-optimized formats allow for high levels of customization, enabling users to tailor data access to their specific needs. Additionally, advanced query capabilities provide the freedom to perform complex operations on the data without downloading and processing entire datasets.\nCost-Effectiveness: Reduced data transfer and storage needs can lower costs. Many of these formats offer compression options, which reduce storage costs.\n\nIf you want to provide optimized access to geospatial data, this guide is designed to help you understand the best practices and tools available for cloud-optimized geospatial formats.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#built-for-the-community-by-the-community",
    "href": "index.html#built-for-the-community-by-the-community",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "Built for the Community, by the Community",
    "text": "Built for the Community, by the Community\nThere is no one-size-fits-all approach to cloud-optimized data. Still, the community has developed many tools for creating and assessing geospatial formats that should be organized and shared.\nThis guide provides the landscape of cloud-optimized geospatial formats and the best-known answers to common questions.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-to-get-involved",
    "href": "index.html#how-to-get-involved",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "How to Get Involved",
    "text": "How to Get Involved\nRead the Get Involved page if you want to contribute or modify content.\nIf you have a question or idea for this guide, please start a Github Discussion.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#the-opportunity",
    "href": "index.html#the-opportunity",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "The Opportunity",
    "text": "The Opportunity\nStoring data in the cloud does not, on its own, solve geospatial’s data problems. Users cannot reasonably wait to download, store, and work with large files on their machines. Large volumes of data must be available via subsetting methods to access data in memory.\nWhile it is possible to provide subsetting as a service, this requires ongoing maintenance of additional servers and extra network latency when accessing data (data has to go to the server where the subsetting service is running and then to the user). With cloud-optimized formats and the appropriate libraries, subsets of data can be accessed directly from an end user’s machine without introducing an additional server.\nRegardless, users will access data over a network, which must be considered when designing the cloud-optimized format. Traditional geospatial formats are optimized for on-disk access via small internal chunks. A network introduces latency, and the number of requests must be considered.\nAs a community, we have arrived at the following cloud-optimized format pattern:\n\nMetadata includes addresses for data blocks.\nMetadata is stored in a consistent format and location.\nMetadata can be read once.\nMetadata can read the underlying file format, which supports subsetted access via addressable chunks, internal tiling, or both.\n\nThese characteristics allow for parallelized and partial reading.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#data-type-to-traditional-to-cloud-optimized-geospatial-file-format-table",
    "href": "index.html#data-type-to-traditional-to-cloud-optimized-geospatial-file-format-table",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "Data Type to Traditional to Cloud-Optimized Geospatial File Format Table",
    "text": "Data Type to Traditional to Cloud-Optimized Geospatial File Format Table\nThe diagram below depicts how some of the cloud-optimized formats discussed in this guide are cloud-optimized formats of traditional geospatial file formats.\n\n\n\nCloud-Optimized Geospatial Formats\n\n\nNotes:\n\nSome data formats cover multiple data types, specifically:\n\nGeoJSON can be used for vector and point cloud data.\nHDF5 can be used for point cloud data or data cubes (or both via groups).\nGeoParquet and FlatGeobuf can be used for vector data or point cloud data.\n\nLAS files are intended for 3D points, not 2D points (since COPC files are compressed LAS files, the same goes for COPC files).\nTopoJSON (an extension of GeoJSON that encodes topology) and newline-delimited GeoJSON are types of GeoJSON worth mentioning but have yet to be explicitly represented in the diagram.\nGeoTIFF and GeoParquet are geospatial versions of the non-geospatial file formats TIFF and Parquet, respectively. FlatGeobuf builds upon the non-geospatial flatbuffers serialization library (though flatbuffers is not a standalone file format).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nOverview of Formats (slideshow)\nFormats\n\nCloud-Optimized GeoTIFFs\nZarr\nKerchunk\nCloud-Optimized NetCDF4/HDF5\nCloud-Optimized Point Clouds (COPC)\nGeoParquet\nFlatGeobuf\nPMTiles\n\nCookbooks",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#running-examples",
    "href": "index.html#running-examples",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "Running Examples",
    "text": "Running Examples\nMost of the data formats covered in this guide have a Jupyter Notebook example that covers the basics of reading and writing the given format. At the top of each notebook is a link to an environment.yml file describing what libraries must be installed to run correctly. You can use Conda or Mamba (a successor to Conda with faster package installs) to install the environment needed to run the notebook.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "Authors",
    "text": "Authors\n\nAimee Barciauskas\nAlex Mandel\nKyle Barron\nZac Deziel\nOverview Slide credits: Vincent Sarago, Chris Holmes, Patrick Quinn, Matt Hanson, Ryan Abernathey",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#questions-to-ask-when-generating-cloud-optimized-geospatial-data-in-any-format",
    "href": "index.html#questions-to-ask-when-generating-cloud-optimized-geospatial-data-in-any-format",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "Questions to Ask When Generating Cloud-Optimized Geospatial Data in Any Format",
    "text": "Questions to Ask When Generating Cloud-Optimized Geospatial Data in Any Format\n\nWhat variable(s) should be included in the new data format?\nWill you create copies to optimize for different needs?\nWhat is the intended use case or usage profile? Will this product be used for visualization, analysis, or both?\nWhat is the expected access method?\nHow much of your data is typically rendered or selected at once?",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#thank-you-to-our-supporters",
    "href": "index.html#thank-you-to-our-supporters",
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "section": "Thank you to our supporters",
    "text": "Thank you to our supporters\nThis guide has been made possible through the support of:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "flatgeobuf/hilbert-r-tree.html",
    "href": "flatgeobuf/hilbert-r-tree.html",
    "title": "FlatGeobuf Spatial Index",
    "section": "",
    "text": "FlatGeobuf optionally supports including a spatial index that enables random access for each geometry in the file.",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf Spatial Index"
    ]
  },
  {
    "objectID": "flatgeobuf/hilbert-r-tree.html#when-to-use",
    "href": "flatgeobuf/hilbert-r-tree.html#when-to-use",
    "title": "FlatGeobuf Spatial Index",
    "section": "When to use",
    "text": "When to use\nWhen writing a FlatGeobuf file, one must decide whether to include a spatial index. A spatial index cannot be added to a FlatGeobuf file after the file has been written.\nA spatial index can enable much more efficient reading from FlatGeobuf, by allowing the reader to skip over portions of the file that fall outside of a qiven spatial query region.",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf Spatial Index"
    ]
  },
  {
    "objectID": "flatgeobuf/hilbert-r-tree.html#technical-details",
    "href": "flatgeobuf/hilbert-r-tree.html#technical-details",
    "title": "FlatGeobuf Spatial Index",
    "section": "Technical details",
    "text": "Technical details\nIn this section we’ll get into some gory technical details of how FlatGeobuf’s spatial index works. Understanding the below isn’t necessary for using FlatGeobuf, but it may add context for understanding how to create and work with FlatGeobuf files, and why FlatGeobuf is performant.\nFlatGeobuf’s spatial index is a static packed Hilbert R-tree index. That’s a mouthful, so let’s break it down:\n\nR-tree index\nAn R-Tree is a hierarchical collection of bounding boxes. At the lowest level of the tree is a bounding box of every geometry. Then one level above the lowest level exists a collection of bounding boxes, each of which is formed as the union of all child boxes. This means that each box encompasses every child box. There are fewer boxes at this level, because each box contains many child boxes, each of which represents one original geometry. This process continues repeatedly until there’s only one bounding box that indirectly contains the entire dataset.\nThis index allows you to quickly search for features that intersect a given bounding box query. At the top level, compare the bounding box of each node to your query region. If those two don’t intersect, you can discard that node and all of its child nodes from the search, because you know that none of them could possibly fall within your search region.\nContinuing this process allows you to quickly find only the specific items that are candidates for your search query.\n\n\nR-Tree diagram from Wikipedia. From top to bottom, the three levels of this tree are the black, blue, and red boxes. The black boxes contain the most items and encompass the largest area, while the red boxes contain the fewest items and encompass a smaller area.\n\nThe Wikipedia article and this Mapbox blog post are great resources for better understanding how R-Trees work.\n\n\nHilbert\nThe elements of an R-Tree must be sorted before insertion to make the R-Tree useful. This is because the core benefit of an R-Tree is to exclude elements that aren’t within a spatial filter. If elements of each node are randomly drawn from different geographies, then each node’s bounding box will be so large that no nodes can be excluded.\nBut how do you sort geometries? They encompass two dimensions and a range of shapes. If you sort all geometries first on the x coordinate, then you may pair geometries that are far from each other on the y dimension. Instead, it’s ideal to use a space-filling curve. That’s math jargon, but essentially defines a way to sort elements in n dimensions using 1 dimensional numbers.\nA Hilbert R-Tree uses a Hilbert Curve, a special type of space-filling curve, to sort the centers of geometries. This ensures that geometries that are nearby on both the x and y dimensions are placed close to each other in the R-Tree. This ensures that the resulting bounding boxes of the R-Tree are as small as possible, which means that the maximum number of elements can be discarded for any given spatial query.\nThis Crunchy Data blog post has helpful examples for why sorting input is important.\n\n\nStatic\nFlatGeobuf files can’t be modified without rewriting the entire file, so this R-Tree is constructed in such a way that it can’t be modified, which allows for improved tree generation.\n\n\nPacked\nAn R-Tree has a series of nodes at each level, where each node can contain up to n children. If the R-Tree might be updated, not every node will have a total of n children, because some space needs to be reserved for future elements.\nBecause the index is static and immutable, we can construct a packed index, where every node is completely full. This achieves better space utilization, and is more efficient for queries because there are fewer nodes to traverse.",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf Spatial Index"
    ]
  },
  {
    "objectID": "flatgeobuf/flatgeobuf-in-js.html",
    "href": "flatgeobuf/flatgeobuf-in-js.html",
    "title": "FlatGeobuf in JavaScript",
    "section": "",
    "text": "FlatGeobuf is a cloud-native vector data format because it contains a built-in spatial index that allows reading a specific spatial region from within the file without downloading the entire file’s content.\nThis is very useful for browser-based applications, because it allows them to make use of large files hosted on commodity cloud object storage without maintaining a server.\nThis notebook provides an example of using FlatGeobuf with spatial filtering from JavaScript.",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf in JavaScript"
    ]
  },
  {
    "objectID": "flatgeobuf/flatgeobuf-in-js.html#downloading-vs-streaming-vs-range-reads",
    "href": "flatgeobuf/flatgeobuf-in-js.html#downloading-vs-streaming-vs-range-reads",
    "title": "FlatGeobuf in JavaScript",
    "section": "Downloading vs Streaming vs Range reads",
    "text": "Downloading vs Streaming vs Range reads\nFlatGeobuf supports a few different ways of loading data into the browser.\nDownloading refers to fetching the entire FlatGeobuf file and parsing it after the full file has finished downloading. This has the downside that the user must wait for the entire download to finish before they see any interaction on the web page. This may lead to a user wondering if the web page is broken if it takes a while to download their data.\nStreaming refers to making use of a file’s contents incrementally as it downloads. This approach still downloads the entire file from beginning to end, but enables e.g. rendering part of the data on a map quickly, before waiting for the full file to finish downloading. This has the benefit of increased responsiveness, but the downside that a large file will be loaded in full. FlatGeobuf supports streaming because the file’s metadata is located at the beginning. A good example of this in action is “Streaming FlatGeobuf” by Björn Harrtell.\nRange reads refers to fetching only specific parts of the file that are required by the user. In the context of FlatGeobuf, this usually means a spatial query. FlatGeobuf enables this through its spatial index at the beginning of the file. Web clients can read the header, and then make requests only for data in a specific location. This has the benefit that very large files can be used in situations where downloading them in full would be impractical. A downside is that it takes more individual HTTP requests to understand which byte range in the file contains the desired data, leading to a longer latency before data starts to display.",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf in JavaScript"
    ]
  },
  {
    "objectID": "flatgeobuf/flatgeobuf-in-js.html#example",
    "href": "flatgeobuf/flatgeobuf-in-js.html#example",
    "title": "FlatGeobuf in JavaScript",
    "section": "Example",
    "text": "Example\nThis example uses slightly-modified JavaScript syntax used in Observable notebooks.\nLoad the FlatGeobuf JavaScript library:\n\nflatgeobuf = require(\"flatgeobuf@3.26.2/dist/flatgeobuf-geojson.min.js\")\n\n\n\n\n\n\nThis library has two functions: deserialize to fetch a remote file and parse it to GeoJSON, and serialize, which converts GeoJSON to FlatGeobuf.\n\nflatgeobuf\n\n\n\n\n\n\nFor this demo, we’ll use the same data source as in the FlatGeobuf leaflet example. This data file represents every census block in the USA.\n\nurl = 'https://flatgeobuf.septima.dk/population_areas.fgb'\n\n\n\n\n\n\nThe above is a really big file at almost 12GB total size, so we don’t want to fetch the entire file. In this demo, we’ll choose a small bounding box representing an area over Manhattan in New York City.\nBeware: if you make this bounding box too big, FlatGeobuf will try to download a large amount of data into your browser and maybe crash the tab!\n\nbbox = {\n    return {\n        minX: -74.003802,\n        minY: 40.725756,\n        maxX: -73.981481,\n        maxY: 40.744008,\n    }\n}\n\n\n\n\n\n\nThe above bbox object represents a bounding box in the format required by the FlatGeobuf API, but Leaflet’s API requires an array-formatted bounding box, so we’ll define a function to convert between the two:\n\n// leaflet uses lat-lon ordering\nbboxObjectToArray = (obj) =&gt; [\n  [obj.minY, obj.minX],\n  [obj.maxY, obj.maxX],\n];\n\n\n\n\n\n\nNext we’ll fetch all the data from the FlatGeobuf file within this bounding box. Notice how we pass the bbox argument into deserialize.\n\nfeatures = {\n  const iter = flatgeobuf.deserialize(url, bbox);\n  const features = [];\n  for await (const feature of iter) {\n    features.push(feature);\n  }\n  return features;\n}\n\n\n\n\n\n\nThere are 354 individual features that match this query:\n\nfeatures\n\n\n\n\n\n\nAs in the FlatGeobuf example, we’ll define a color scale based on how many people live in the census block.\n\ncolorScale = (d) =&gt; {\n  return d &gt; 750\n    ? \"#800026\"\n    : d &gt; 500\n    ? \"#BD0026\"\n    : d &gt; 250\n    ? \"#E31A1C\"\n    : d &gt; 100\n    ? \"#FC4E2A\"\n    : d &gt; 50\n    ? \"#FD8D3C\"\n    : d &gt; 25\n    ? \"#FEB24C\"\n    : d &gt; 10\n    ? \"#FED976\"\n    : \"#FFEDA0\";\n};\n\n\n\n\n\n\nNext we load the Leaflet JavaScript library and fetch its CSS styling defintions if needed.\n\nL = {\n  const L = await require(\"leaflet@1/dist/leaflet.js\");\n  if (!L._style) {\n    const href = await require.resolve(\"leaflet@1/dist/leaflet.css\");\n    document.head.appendChild(L._style = html`&lt;link href=${href} rel=stylesheet&gt;`);\n  }\n  return L;\n}\n\n\n\n\n\n\nNext we instantiate the Leaflet map and include multiple layers:\n\nAn L.tileLayer to show basemap tiles on the map for context.\nAn L.rectangle to show the bounding box of our FlatGeobuf query.\nAn L.layerGroup to group all the FlatGeobuf features into a single layer.\nAn L.geoJSON item for each feature in the FlatGeobuf response.\n\n\nmap = {\n  const container = html`&lt;div style=\"height:600px;\"&gt;&lt;/div&gt;`;\n  yield container;\n  const map = L.map(container).setView([40.7299, -73.9923], 13);\n  L.tileLayer(\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\", {\n    attribution:\n      \"&copy; &lt;a href=https://www.openstreetmap.org/copyright&gt;OpenStreetMap&lt;/a&gt; contributors\",\n  }).addTo(map);\n\n  // Render the bounding box rectangle\n  L.rectangle(bboxObjectToArray(bbox), {\n    interactive: false,\n    color: \"blue\",\n    fillOpacity: 0.0,\n    opacity: 1.0,\n  }).addTo(map);\n\n  const results = L.layerGroup().addTo(map);\n  for (const feature of features) {\n    // Leaflet styling\n    const defaultStyle = {\n      color: colorScale(feature.properties[\"population\"]),\n      weight: 1,\n      fillOpacity: 0.4,\n    };\n    L.geoJSON(feature, {\n      style: defaultStyle,\n    })\n      .on({\n        mouseover: function (e) {\n          const layer = e.target;\n          layer.setStyle({\n            weight: 4,\n            fillOpacity: 0.8,\n          });\n        },\n        mouseout: function (e) {\n          const layer = e.target;\n          layer.setStyle(defaultStyle);\n        },\n      })\n      .bindPopup(\n        `${feature.properties[\"population\"]} people live in this census block.&lt;/h1&gt;`\n      )\n      .addTo(results);\n  }\n}\n\n\n\n\n\n\nVoilà! We just fetched data directly from a massive FlatGeobuf file, directly from the client, without a server in between.\n\nReferences\nThis notebook was created with help from several resources\n\nFlatGeobuf Leaflet example (and its source code)\n@bjornharrtell/streaming-flatgeobuf is a useful related resource for an example of a streaming load of FlatGeobuf.\n@observablehq/hello-leaflet for an example of loading and rendering a Leaflet map using Observable.",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf in JavaScript"
    ]
  },
  {
    "objectID": "copc/index.html",
    "href": "copc/index.html",
    "title": "Cloud-Optimized Point Clouds (COPC)",
    "section": "",
    "text": "The LASER (LAS) file format is designed to store 3-dimensional (x,y,z) point cloud data typically collected from LiDAR. An LAZ file is a compressed LAS file and a Cloud-Optimized Point Cloud (COPC) file is a valid LAZ file.\nCOPC files are similar to COGs for GeoTIFFs: Both are valid versions of the original file format but with additional requirements to support cloud-optimized data access. In the case of COGs, there are additional requirements for tiling and overviews. For COPC, data must be organized into a clustered octree with a variable-length record (VLR) describing the octree structure.\nRead more at https://copc.io/.\nStay tuned for more information on COPC in future releases of this guide.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Cloud-Optimized Point Clouds (COPC)"
    ]
  },
  {
    "objectID": "kerchunk/kerchunk-in-practice.html",
    "href": "kerchunk/kerchunk-in-practice.html",
    "title": "Kerchunk in Practice",
    "section": "",
    "text": "In this notebook, we demonstrate how to create a kerchunk reference file for one and then multiple publicly available NetCDF files and how to open a kerchunk store with xarray.\nGenerally, NetCDF should work with kerchunk. Some nested data structures and data types, such as those that can exist in HDF5, won’t work with kerchunk. A future release of this guide will provide further information and/or resources on limitations of kerchunk.",
    "crumbs": [
      "Formats",
      "Kerchunk",
      "Kerchunk in Practice"
    ]
  },
  {
    "objectID": "kerchunk/kerchunk-in-practice.html#environment",
    "href": "kerchunk/kerchunk-in-practice.html#environment",
    "title": "Kerchunk in Practice",
    "section": "Environment",
    "text": "Environment\nThe packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda create -f environment.yml\nor\nmamba create -f environment.yml\nThis notebook has been tested to work with the listed Conda environment.",
    "crumbs": [
      "Formats",
      "Kerchunk",
      "Kerchunk in Practice"
    ]
  },
  {
    "objectID": "kerchunk/kerchunk-in-practice.html#how-to-create-a-kerchunk-store",
    "href": "kerchunk/kerchunk-in-practice.html#how-to-create-a-kerchunk-store",
    "title": "Kerchunk in Practice",
    "section": "How to create a kerchunk store",
    "text": "How to create a kerchunk store\nWe can use the publicly available NEX GDDP CMIP6 dataset for this example. This dataset is provided by NASA and publicly available on AWS S3. You can browse that data in the nex-gddp-cmip6 file browser.\n\nimport json\nimport os\nfrom tempfile import TemporaryDirectory\n\nimport fsspec\nimport imagecodecs.numcodecs\nimport xarray as xr\nfrom kerchunk.combine import MultiZarrToZarr\nfrom kerchunk.hdf import SingleHdf5ToZarr\n\nimagecodecs.numcodecs.register_codecs() \n\n\n# Set variables\n## Since there are a number of CMIP6 models and variables to chose from, we make the model and variable selections variables.\nmodel = \"ACCESS-CM2\"\n# `tasmax` is daily-maximum near-surface air temperature, see https://pcmdi.llnl.gov/mips/cmip3/variableList.html.\nvariable = \"tasmax\"\n## Note we are only reading historical data here, but model data is available for SSPs (Shared Socio-economic Pathways) as well.\n## SSPs are scenarios are used to model the future, so SSP files predict environment variables into the future.\ns3_path = f\"s3://nex-gddp-cmip6/NEX-GDDP-CMIP6/{model}/historical/r1i1p1*/{variable}/*\"\n\n# Initiate fsspec filesystem for reading.\n## We set anon=True because this specific dataset on AWS does not require users to be logged in to access.\nfs_read = fsspec.filesystem(\"s3\", anon=True)\n\n# Retrieve list of available data.\nfile_paths = fs_read.glob(s3_path)\nprint(f\"{len(file_paths)} discovered from {s3_path}\")\n\n65 discovered from s3://nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1*/tasmax/*\n\n\nTo start, we are just going to create a single reference file for a single NetCDF file.\n\nnetcdf_file = file_paths[0]\nnetcdf_file\n\n'nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1950.nc'\n\n\n\n# Define a function to generate the kerchunk file so we can use it again for other files.\ndef generate_json_reference(input_file, temp_dir: str):\n    \"\"\"\n    Use Kerchunk's `SingleHdf5ToZarr` method to create a `Kerchunk` index from a NetCDF file.\n    \"\"\"\n    with fs_read.open(input_file, **dict(mode=\"rb\")) as infile:\n        print(f\"Running kerchunk generation for {input_file}...\")\n        # Chunks smaller than `inline_threshold` will be stored directly in the reference file as data (as opposed to a URL and byte range).\n        h5chunks = SingleHdf5ToZarr(infile, input_file, inline_threshold=300)\n        fname = input_file.split(\"/\")[-1].strip(\".nc\")\n        outf = os.path.join(temp_dir, f\"{fname}.json\")\n        with open(outf, \"wb\") as f:\n            f.write(json.dumps(h5chunks.translate()).encode())\n        return outf\n\n\n# Create a temporary directory to store the .json reference files.\n# Alternately, you could write these to cloud storage.\ntd = TemporaryDirectory()\ntemp_dir = td.name\nprint(f\"Writing single file references to {temp_dir}\")\n\nWriting single file references to /var/folders/42/5jr6891d4ds4xysz7q0rsghw0000gn/T/tmpn1bas0mo\n\n\n\nsingle_kerchunk_reference = generate_json_reference(netcdf_file, temp_dir)\n\nRunning kerchunk generation for nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1950.nc...\n\n\nWe might also want to inspect what was just created. Below we load just the first few keys and values of the “refs” part of the kerchunk reference file.\n\n# Read and load the JSON file\nwith open(single_kerchunk_reference, 'r') as file:\n    data = json.load(file)\nkeys_to_select = ['.zgroup', 'tasmax/.zarray', 'tasmax/0.0.0']\n\n# Pretty print JSON data\ndata_to_print = {}\nfor key, value in data['refs'].items():\n    if key in keys_to_select:\n        if isinstance(value, str):\n            data_to_print[key] = json.loads(value)\n        else:\n            data_to_print[key] = value\nprint(json.dumps(data_to_print, indent=4))\n\n{\n    \".zgroup\": {\n        \"zarr_format\": 2\n    },\n    \"tasmax/.zarray\": {\n        \"chunks\": [\n            1,\n            600,\n            1440\n        ],\n        \"compressor\": {\n            \"id\": \"zlib\",\n            \"level\": 5\n        },\n        \"dtype\": \"&lt;f4\",\n        \"fill_value\": 1.0000000200408773e+20,\n        \"filters\": [\n            {\n                \"elementsize\": 4,\n                \"id\": \"shuffle\"\n            }\n        ],\n        \"order\": \"C\",\n        \"shape\": [\n            365,\n            600,\n            1440\n        ],\n        \"zarr_format\": 2\n    },\n    \"tasmax/0.0.0\": [\n        \"nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1950.nc\",\n        18097,\n        674483\n    ]\n}\n\n\nWe can also check that our reference file works with xarray.\n\n# Open dataset as zarr object using fsspec reference file system and Xarray\nfs_single = fsspec.filesystem(\n    \"reference\", fo=single_kerchunk_reference, remote_protocol=\"https\"\n)\nsingle_map = fs_single.get_mapper(\"\")\n\n\nds_single = xr.open_dataset(single_map, engine=\"zarr\", backend_kwargs=dict(consolidated=False))\nds_single\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lat: 600, lon: 1440, time: 365)\nCoordinates:\n  * lat      (lat) float64 0.0 1.23e-321 0.0 ... -3.218e+163 -3.218e+163\n  * lon      (lon) float64 0.0 2.164e-314 0.0 ... -2.022e-53 -1.699e+282\n  * time     (time) datetime64[ns] 1950-01-01T12:00:00 ... 1950-12-31T12:00:00\nData variables:\n    tasmax   (time, lat, lon) float32 ...\nAttributes: (12/22)\n    Conventions:           CF-1.7\n    activity:              NEX-GDDP-CMIP6\n    cmip6_institution_id:  CSIRO-ARCCSS\n    cmip6_license:         CC-BY-SA 4.0\n    cmip6_source_id:       ACCESS-CM2\n    contact:               Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget...\n    ...                    ...\n    scenario:              historical\n    source:                BCSD\n    title:                 ACCESS-CM2, r1i1p1f1, historical, global downscale...\n    tracking_id:           f85d4c2e-48e4-484f-aad4-6a3f30a04326\n    variant_label:         r1i1p1f1\n    version:               1.0xarray.DatasetDimensions:lat: 600lon: 1440time: 365Coordinates: (3)lat(lat)float640.0 1.23e-321 ... -3.218e+163axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([ 0.000000e+000,  1.230223e-321,  0.000000e+000, ..., -3.218047e+163,\n       -3.218047e+163, -3.218047e+163])lon(lon)float640.0 2.164e-314 ... -1.699e+282axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 0.000000e+000,  2.163912e-314,  0.000000e+000, ...,  1.902013e-242,\n       -2.022208e-053, -1.698612e+282])time(time)datetime64[ns]1950-01-01T12:00:00 ... 1950-12-...axis :Tlong_name :timestandard_name :timearray(['1950-01-01T12:00:00.000000000', '1950-01-02T12:00:00.000000000',\n       '1950-01-03T12:00:00.000000000', ..., '1950-12-29T12:00:00.000000000',\n       '1950-12-30T12:00:00.000000000', '1950-12-31T12:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)tasmax(time, lat, lon)float32...cell_measures :area: areacellacell_methods :area: mean time: maximumcomment :maximum near-surface (usually, 2 meter) air temperature (add cell_method attribute 'time: max')long_name :Daily Maximum Near-Surface Air Temperaturestandard_name :air_temperatureunits :K[315360000 values with dtype=float32]Indexes: (3)latPandasIndexPandasIndex(Index([                     0.0,                1.23e-321,\n                            0.0,                1.23e-321,\n       -3.2180465730379564e+163, -3.2180465730379564e+163,\n       -3.2180465730379564e+163, -3.2180465730379564e+163,\n       -3.2180465730379564e+163, -3.2180465730379564e+163,\n       ...\n       -3.2180465730379564e+163, -3.2180465730379564e+163,\n       -3.2180465730379564e+163, -3.2180465730379564e+163,\n       -3.2180465730379564e+163, -3.2180465730379564e+163,\n       -3.2180465730379564e+163, -3.2180465730379564e+163,\n       -3.2180465730379564e+163, -3.2180465730379564e+163],\n      dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Index([                    0.0,        2.163911906e-314,\n                           0.0,                     nan,\n                           0.0,                     0.0,\n                           0.0,                     0.0,\n                           0.0,                     0.0,\n       ...\n                           0.0,                     0.0,\n        1.5390572997222847e+73,  1.0494093556865241e-86,\n        7.328222560480262e-213,  3.493934932025909e-195,\n        7.981962361089973e-296,   1.90201295465319e-242,\n        -2.022208454662242e-53, -1.698612219286841e+282],\n      dtype='float64', name='lon', length=1440))timePandasIndexPandasIndex(DatetimeIndex(['1950-01-01 12:00:00', '1950-01-02 12:00:00',\n               '1950-01-03 12:00:00', '1950-01-04 12:00:00',\n               '1950-01-05 12:00:00', '1950-01-06 12:00:00',\n               '1950-01-07 12:00:00', '1950-01-08 12:00:00',\n               '1950-01-09 12:00:00', '1950-01-10 12:00:00',\n               ...\n               '1950-12-22 12:00:00', '1950-12-23 12:00:00',\n               '1950-12-24 12:00:00', '1950-12-25 12:00:00',\n               '1950-12-26 12:00:00', '1950-12-27 12:00:00',\n               '1950-12-28 12:00:00', '1950-12-29 12:00:00',\n               '1950-12-30 12:00:00', '1950-12-31 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=365, freq=None))Attributes: (22)Conventions :CF-1.7activity :NEX-GDDP-CMIP6cmip6_institution_id :CSIRO-ARCCSScmip6_license :CC-BY-SA 4.0cmip6_source_id :ACCESS-CM2contact :Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget Thrasher: bridget@climateanalyticsgroup.orgcreation_date :2021-10-04T14:00:55.510838+00:00disclaimer :This data is considered provisional and subject to change. This data is provided as is without any warranty of any kind, either express or implied, arising by law or otherwise, including but not limited to warranties of completeness, non-infringement, accuracy, merchantability, or fitness for a particular purpose. The user assumes all risk associated with the use of, or inability to use, this data.external_variables :areacellafrequency :dayhistory :2021-10-04T14:00:55.510838+00:00: install global attributesinstitution :NASA Earth Exchange, NASA Ames Research Center, Moffett Field, CA 94035product :outputrealm :atmosreferences :BCSD method: Thrasher et al., 2012, Hydrol. Earth Syst. Sci.,16, 3309-3314. Ref period obs: latest version of the Princeton Global Meteorological Forcings (http://hydrology.princeton.edu/data.php), based on Sheffield et al., 2006, J. Climate, 19 (13), 3088-3111.resolution_id :0.25 degreescenario :historicalsource :BCSDtitle :ACCESS-CM2, r1i1p1f1, historical, global downscaled CMIP6 climate projection datatracking_id :f85d4c2e-48e4-484f-aad4-6a3f30a04326variant_label :r1i1p1f1version :1.0\n\n\nIt worked! But we can do even better. What if you want to open multiple NetCDF files with xarray? Let’s create kerchunk references for 3 files and then combine them.\n\nsubset_files = file_paths[0:3]\nsubset_files\n\n['nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1950.nc',\n 'nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1951.nc',\n 'nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1952.nc']\n\n\n\n# Iterate through filelist to generate Kerchunked files. Good use for `dask.bag`, see: https://docs.dask.org/en/stable/bag.html.\noutput_files = []\nfor single_file in subset_files:\n    out_file = generate_json_reference(single_file, temp_dir)\n    output_files.append(out_file)\n\nRunning kerchunk generation for nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1950.nc...\nRunning kerchunk generation for nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1951.nc...\nRunning kerchunk generation for nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1952.nc...\n\n\n\noutput_files\n\n['/var/folders/42/5jr6891d4ds4xysz7q0rsghw0000gn/T/tmpn1bas0mo/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1950.json',\n '/var/folders/42/5jr6891d4ds4xysz7q0rsghw0000gn/T/tmpn1bas0mo/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1951.json',\n '/var/folders/42/5jr6891d4ds4xysz7q0rsghw0000gn/T/tmpn1bas0mo/tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1952.json']\n\n\n\n# combine individual references into single consolidated reference\nmzz = MultiZarrToZarr(\n    output_files,\n    remote_protocol='s3',\n    remote_options={'anon': True},\n    concat_dims=['time'],\n    coo_map={'time': 'cf:time'},\n    # inline_threshold=0 means don't story any raw data in the kerchunk reference file.\n    inline_threshold=0\n)\nmulti_kerchunk = mzz.translate()\n\n\n# Write kerchunk .json record\noutput_fname = os.path.join(temp_dir, f\"combined_CMIP6_daily_{model}_{variable}_kerchunk.json\")\nwith open(f\"{output_fname}\", \"wb\") as f:\n    print(f\"Writing combined kerchunk reference file {output_fname}\")\n    f.write(json.dumps(multi_kerchunk).encode())\n\nWriting combined kerchunk reference file /var/folders/42/5jr6891d4ds4xysz7q0rsghw0000gn/T/tmpn1bas0mo/combined_CMIP6_daily_ACCESS-CM2_tasmax_kerchunk.json\n\n\n\n# open dataset as zarr object using fsspec reference file system and Xarray\nfs_multi = fsspec.filesystem(\n    \"reference\",\n    fo=multi_kerchunk,\n    remote_protocol=\"s3\"\n)\nmulti_map = fs_multi.get_mapper(\"\")\n\n\nds_multi = xr.open_dataset(multi_map, engine=\"zarr\", backend_kwargs=dict(consolidated=False))\nds_multi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lat: 600, lon: 1440, time: 1096)\nCoordinates:\n  * lat      (lat) float64 0.0 2.164e-314 0.0 ... 2.961e-314 2.961e-314\n  * lon      (lon) float64 0.0 2.164e-314 0.0 ... -6.915e+193 -4.603e+95\n  * time     (time) datetime64[ns] 1950-01-01T12:00:00 ... 1952-12-31T12:00:00\nData variables:\n    tasmax   (time, lat, lon) float32 ...\nAttributes: (12/22)\n    Conventions:           CF-1.7\n    activity:              NEX-GDDP-CMIP6\n    cmip6_institution_id:  CSIRO-ARCCSS\n    cmip6_license:         CC-BY-SA 4.0\n    cmip6_source_id:       ACCESS-CM2\n    contact:               Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget...\n    ...                    ...\n    scenario:              historical\n    source:                BCSD\n    title:                 ACCESS-CM2, r1i1p1f1, historical, global downscale...\n    tracking_id:           f85d4c2e-48e4-484f-aad4-6a3f30a04326\n    variant_label:         r1i1p1f1\n    version:               1.0xarray.DatasetDimensions:lat: 600lon: 1440time: 1096Coordinates: (3)lat(lat)float640.0 2.164e-314 ... 2.961e-314axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([0.000000e+000, 2.163912e-314, 0.000000e+000, ..., 2.961067e-314,\n       2.960919e-314, 2.961067e-314])lon(lon)float640.0 2.164e-314 ... -4.603e+95axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 0.000000e+000,  2.163912e-314,  0.000000e+000, ...,  2.334981e+006,\n       -6.914611e+193, -4.603478e+095])time(time)datetime64[ns]1950-01-01T12:00:00 ... 1952-12-...axis :Tlong_name :timestandard_name :timearray(['1950-01-01T12:00:00.000000000', '1950-01-02T12:00:00.000000000',\n       '1950-01-03T12:00:00.000000000', ..., '1952-12-29T12:00:00.000000000',\n       '1952-12-30T12:00:00.000000000', '1952-12-31T12:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)tasmax(time, lat, lon)float32...cell_measures :area: areacellacell_methods :area: mean time: maximumcomment :maximum near-surface (usually, 2 meter) air temperature (add cell_method attribute 'time: max')long_name :Daily Maximum Near-Surface Air Temperaturestandard_name :air_temperatureunits :K[946944000 values with dtype=float32]Indexes: (3)latPandasIndexPandasIndex(Index([              0.0,  2.163911906e-314,               0.0,\n                     nan,               0.0,  2.847840319e-314,\n        2.847840477e-314,            5e-324, 2.8478403307e-314,\n       2.8478403347e-314,\n       ...\n        2.960919408e-314, 2.9610663864e-314,  2.960919313e-314,\n       2.9610664496e-314, 2.9609193446e-314,  2.961066513e-314,\n       2.9609192497e-314,  2.961066576e-314, 2.9609192813e-314,\n       2.9610666394e-314],\n      dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Index([                     0.0,         2.163911906e-314,\n                            0.0,                      nan,\n        1.8178640317427325e+185,  1.0640025030406259e+248,\n          6.01334685394558e-154,   9.363931581572749e+252,\n        1.2064976717019484e+285,   2.582765705848744e-144,\n       ...\n         2.7454590140292026e+40,  -3.255930979178767e-308,\n        1.5281971544072024e-111,   -7.088607689435405e+42,\n         1.1472324330854862e+22,  3.6014577529949115e+106,\n          9.851096278175061e+67,       2334981.4421286285,\n       -6.9146108782833415e+193,   -4.603477998061419e+95],\n      dtype='float64', name='lon', length=1440))timePandasIndexPandasIndex(DatetimeIndex(['1950-01-01 12:00:00', '1950-01-02 12:00:00',\n               '1950-01-03 12:00:00', '1950-01-04 12:00:00',\n               '1950-01-05 12:00:00', '1950-01-06 12:00:00',\n               '1950-01-07 12:00:00', '1950-01-08 12:00:00',\n               '1950-01-09 12:00:00', '1950-01-10 12:00:00',\n               ...\n               '1952-12-22 12:00:00', '1952-12-23 12:00:00',\n               '1952-12-24 12:00:00', '1952-12-25 12:00:00',\n               '1952-12-26 12:00:00', '1952-12-27 12:00:00',\n               '1952-12-28 12:00:00', '1952-12-29 12:00:00',\n               '1952-12-30 12:00:00', '1952-12-31 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=1096, freq=None))Attributes: (22)Conventions :CF-1.7activity :NEX-GDDP-CMIP6cmip6_institution_id :CSIRO-ARCCSScmip6_license :CC-BY-SA 4.0cmip6_source_id :ACCESS-CM2contact :Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget Thrasher: bridget@climateanalyticsgroup.orgcreation_date :2021-10-04T14:00:55.510838+00:00disclaimer :This data is considered provisional and subject to change. This data is provided as is without any warranty of any kind, either express or implied, arising by law or otherwise, including but not limited to warranties of completeness, non-infringement, accuracy, merchantability, or fitness for a particular purpose. The user assumes all risk associated with the use of, or inability to use, this data.external_variables :areacellafrequency :dayhistory :2021-10-04T14:00:55.510838+00:00: install global attributesinstitution :NASA Earth Exchange, NASA Ames Research Center, Moffett Field, CA 94035product :outputrealm :atmosreferences :BCSD method: Thrasher et al., 2012, Hydrol. Earth Syst. Sci.,16, 3309-3314. Ref period obs: latest version of the Princeton Global Meteorological Forcings (http://hydrology.princeton.edu/data.php), based on Sheffield et al., 2006, J. Climate, 19 (13), 3088-3111.resolution_id :0.25 degreescenario :historicalsource :BCSDtitle :ACCESS-CM2, r1i1p1f1, historical, global downscaled CMIP6 climate projection datatracking_id :f85d4c2e-48e4-484f-aad4-6a3f30a04326variant_label :r1i1p1f1version :1.0\n\n\nCool! Now we have 1096 days (3 years) of data.",
    "crumbs": [
      "Formats",
      "Kerchunk",
      "Kerchunk in Practice"
    ]
  },
  {
    "objectID": "kerchunk/kerchunk-in-practice.html#how-to-read-a-kerchunk-store",
    "href": "kerchunk/kerchunk-in-practice.html#how-to-read-a-kerchunk-store",
    "title": "Kerchunk in Practice",
    "section": "How to read a Kerchunk Store",
    "text": "How to read a Kerchunk Store\nWe’ve already demonstrated how to open the datasets with Xarray:\nfs_multi = fsspec.filesystem(\n    \"reference\",\n    fo=multi_kerchunk,\n    remote_protocol=\"s3\"\n)\nLet’s take it line by line to understand what’s happening.\n\nfsspec.filesystem is used to open the kerchunk reference. It is not necessary to have kerchunk installed to read data.\nThe first argument to fsspec.filesystem is the protocol. In the case of a kerchunk reference the protocol is the string \"reference\".\nThe fo argument is the set of reference files used to create a ReferenceFileSystem instance.\nThe remote_protocol argument is the protocol of the filesystem on which the references will be evaluated (unless fs is provided). If not given, will be derived from the first URL that has a protocol in the templates or in the references.\n\nNotice how the fs_multi object we’ve created is a fsspec.implementations.reference.ReferenceFileSystem.\n\ntype(fs_multi)\n\nfsspec.implementations.reference.ReferenceFileSystem\n\n\nRead about all the options for a fsspec.ReferenceFileSystem in the fsspec docs.\nOne other common situation is to load data over HTTP (as opposed to a local filesystem or via the S3 protocol). Here’s an example from the kerchunk case studies that loads a reference file and data files over HTTP:\n\nzarr_all_url='https://sentinel-1-global-coherence-earthbigdata.s3.us-west-2.amazonaws.com/data/wrappers/zarr-all.json'\n\nmapper = fsspec.get_mapper(\n    'reference://',\n    fo=zarr_all_url,\n    target_protocol='http',\n    remote_protocol='http'\n)\ndataset = xr.open_dataset(\n    mapper, engine='zarr', backend_kwargs={'consolidated': False}\n)\ndataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (season: 4, polarization: 4, latitude: 193200,\n                      longitude: 432000, coherence: 6, flightdirection: 2,\n                      orbit: 175)\nCoordinates:\n  * coherence        (coherence) float32 6.0 12.0 18.0 24.0 36.0 48.0\n  * flightdirection  (flightdirection) object 'A' 'D'\n  * latitude         (latitude) float32 82.0 82.0 82.0 ... -79.0 -79.0 -79.0\n  * longitude        (longitude) float32 -180.0 -180.0 -180.0 ... 180.0 180.0\n  * orbit            (orbit) float64 1.0 2.0 3.0 4.0 ... 172.0 173.0 174.0 175.0\n  * polarization     (polarization) object 'vv' 'vh' 'hv' 'hh'\n  * season           (season) object 'winter' 'spring' 'summer' 'fall'\nData variables:\n    AMP              (season, polarization, latitude, longitude) float32 ...\n    COH              (season, polarization, coherence, latitude, longitude) float32 ...\n    inc              (orbit, flightdirection, latitude, longitude) float32 ...\n    lsmap            (orbit, flightdirection, latitude, longitude) float32 ...\n    rho              (season, polarization, latitude, longitude) float32 ...\n    rmse             (season, polarization, latitude, longitude) float32 ...\n    tau              (season, polarization, latitude, longitude) float32 ...xarray.DatasetDimensions:season: 4polarization: 4latitude: 193200longitude: 432000coherence: 6flightdirection: 2orbit: 175Coordinates: (7)coherence(coherence)float326.0 12.0 18.0 24.0 36.0 48.0array([ 6., 12., 18., 24., 36., 48.], dtype=float32)flightdirection(flightdirection)object'A' 'D'array(['A', 'D'], dtype=object)latitude(latitude)float3282.0 82.0 82.0 ... -79.0 -79.0array([ 81.99958,  81.99875,  81.99792, ..., -78.99792, -78.99875, -78.99958],\n      dtype=float32)longitude(longitude)float32-180.0 -180.0 ... 180.0 180.0array([-179.99959, -179.99875, -179.99791, ...,  179.99791,  179.99875,\n        179.99959], dtype=float32)orbit(orbit)float641.0 2.0 3.0 ... 173.0 174.0 175.0array([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,  12.,\n        13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,\n        25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,\n        37.,  38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,\n        49.,  50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.,\n        61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,  72.,\n        73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,\n        85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,\n        97.,  98.,  99., 100., 101., 102., 103., 104., 105., 106., 107., 108.,\n       109., 110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131., 132.,\n       133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143., 144.,\n       145., 146., 147., 148., 149., 150., 151., 152., 153., 154., 155., 156.,\n       157., 158., 159., 160., 161., 162., 163., 164., 165., 166., 167., 168.,\n       169., 170., 171., 172., 173., 174., 175.])polarization(polarization)object'vv' 'vh' 'hv' 'hh'array(['vv', 'vh', 'hv', 'hh'], dtype=object)season(season)object'winter' 'spring' 'summer' 'fall'array(['winter', 'spring', 'summer', 'fall'], dtype=object)Data variables: (7)AMP(season, polarization, latitude, longitude)float32...[1335398400000 values with dtype=float32]COH(season, polarization, coherence, latitude, longitude)float32...[8012390400000 values with dtype=float32]inc(orbit, flightdirection, latitude, longitude)float32...[29211840000000 values with dtype=float32]lsmap(orbit, flightdirection, latitude, longitude)float32...[29211840000000 values with dtype=float32]rho(season, polarization, latitude, longitude)float32...[1335398400000 values with dtype=float32]rmse(season, polarization, latitude, longitude)float32...[1335398400000 values with dtype=float32]tau(season, polarization, latitude, longitude)float32...[1335398400000 values with dtype=float32]Indexes: (7)coherencePandasIndexPandasIndex(Index([6.0, 12.0, 18.0, 24.0, 36.0, 48.0], dtype='float32', name='coherence'))flightdirectionPandasIndexPandasIndex(Index(['A', 'D'], dtype='object', name='flightdirection'))latitudePandasIndexPandasIndex(Index([ 81.99958038330078,  81.99874877929688,  81.99791717529297,\n        81.99708557128906,  81.99624633789062,  81.99541473388672,\n        81.99458312988281,   81.9937515258789,    81.992919921875,\n        81.99208068847656,\n       ...\n       -78.99208068847656,   -78.992919921875,  -78.9937515258789,\n       -78.99458312988281, -78.99541473388672, -78.99624633789062,\n       -78.99708557128906, -78.99791717529297, -78.99874877929688,\n       -78.99958038330078],\n      dtype='float32', name='latitude', length=193200))longitudePandasIndexPandasIndex(Index([ -179.9995880126953, -179.99874877929688, -179.99790954589844,\n       -179.99708557128906, -179.99624633789062, -179.99542236328125,\n        -179.9945831298828, -179.99374389648438,   -179.992919921875,\n       -179.99208068847656,\n       ...\n        179.99208068847656,    179.992919921875,  179.99374389648438,\n         179.9945831298828,  179.99542236328125,  179.99624633789062,\n        179.99708557128906,  179.99790954589844,  179.99874877929688,\n         179.9995880126953],\n      dtype='float32', name='longitude', length=432000))orbitPandasIndexPandasIndex(Index([  1.0,   2.0,   3.0,   4.0,   5.0,   6.0,   7.0,   8.0,   9.0,  10.0,\n       ...\n       166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0],\n      dtype='float64', name='orbit', length=175))polarizationPandasIndexPandasIndex(Index(['vv', 'vh', 'hv', 'hh'], dtype='object', name='polarization'))seasonPandasIndexPandasIndex(Index(['winter', 'spring', 'summer', 'fall'], dtype='object', name='season'))Attributes: (0)\n\n\nBecause xarray uses fsspec to read data, you can also bypass creating a fsspec object explicitly. Here’s an example using of opening a kerchunk reference generated with pangeo-forge for the NOAA 1/4° daily Optimum Interpolation Sea Surface Temperature (or daily OISST) Climate Data Record (CDR).\n\nurl = \"https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/pangeo-forge/aws-noaa-oisst-feedstock/aws-noaa-oisst-avhrr-only.zarr/reference.json\"\nds = xr.open_dataset(\n    \"reference://\",\n    engine='zarr',\n    backend_kwargs={\n        'consolidated': False,\n        'storage_options': {\n            'fo': url,\n            'remote_options': {'anon': True},\n            'remote_protocol': 's3'}},\n    chunks={})\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 15044, zlev: 1, lat: 720, lon: 1440)\nCoordinates:\n  * lat      (lat) float32 -89.88 -89.62 -89.38 -89.12 ... 89.38 89.62 89.88\n  * lon      (lon) float32 0.125 0.375 0.625 0.875 ... 359.1 359.4 359.6 359.9\n  * time     (time) datetime64[ns] 1981-09-01T12:00:00 ... 2022-11-08T12:00:00\n  * zlev     (zlev) float32 0.0\nData variables:\n    anom     (time, zlev, lat, lon) float32 dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n    err      (time, zlev, lat, lon) float32 dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n    ice      (time, zlev, lat, lon) float32 dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n    sst      (time, zlev, lat, lon) float32 dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\nAttributes: (12/37)\n    Conventions:                CF-1.6, ACDD-1.3\n    cdm_data_type:              Grid\n    comment:                    Data was converted from NetCDF-3 to NetCDF-4 ...\n    creator_email:              oisst-help@noaa.gov\n    creator_url:                https://www.ncei.noaa.gov/\n    date_created:               2020-05-08T19:05:13Z\n    ...                         ...\n    source:                     ICOADS, NCEP_GTS, GSFC_ICE, NCEP_ICE, Pathfin...\n    standard_name_vocabulary:   CF Standard Name Table (v40, 25 January 2017)\n    summary:                    NOAAs 1/4-degree Daily Optimum Interpolation ...\n    time_coverage_end:          1981-09-01T23:59:59Z\n    time_coverage_start:        1981-09-01T00:00:00Z\n    title:                      NOAA/NCEI 1/4 Degree Daily Optimum Interpolat...xarray.DatasetDimensions:time: 15044zlev: 1lat: 720lon: 1440Coordinates: (4)lat(lat)float32-89.88 -89.62 ... 89.62 89.88grids :Uniform grid from -89.875 to 89.875 by 0.25long_name :Latitudeunits :degrees_northarray([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)lon(lon)float320.125 0.375 0.625 ... 359.6 359.9grids :Uniform grid from 0.125 to 359.875 by 0.25long_name :Longitudeunits :degrees_eastarray([1.25000e-01, 3.75000e-01, 6.25000e-01, ..., 3.59375e+02, 3.59625e+02,\n       3.59875e+02], dtype=float32)time(time)datetime64[ns]1981-09-01T12:00:00 ... 2022-11-...long_name :Center time of the dayarray(['1981-09-01T12:00:00.000000000', '1981-09-02T12:00:00.000000000',\n       '1981-09-03T12:00:00.000000000', ..., '2022-11-06T12:00:00.000000000',\n       '2022-11-07T12:00:00.000000000', '2022-11-08T12:00:00.000000000'],\n      dtype='datetime64[ns]')zlev(zlev)float320.0actual_range :0, 0long_name :Sea surface heightpositive :downunits :metersarray([0.], dtype=float32)Data variables: (4)anom(time, zlev, lat, lon)float32dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;long_name :Daily sea surface temperature anomaliesunits :Celsiusvalid_max :1200valid_min :-1200\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n58.11 GiB\n3.96 MiB\n\n\nShape\n(15044, 1, 720, 1440)\n(1, 1, 720, 1440)\n\n\nDask graph\n15044 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                        15044 1                          1440 720 1\n\n\n\n\nerr(time, zlev, lat, lon)float32dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;long_name :Estimated error standard deviation of analysed_sstunits :Celsiusvalid_max :1000valid_min :0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n58.11 GiB\n3.96 MiB\n\n\nShape\n(15044, 1, 720, 1440)\n(1, 1, 720, 1440)\n\n\nDask graph\n15044 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                        15044 1                          1440 720 1\n\n\n\n\nice(time, zlev, lat, lon)float32dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;long_name :Sea ice concentrationunits :%valid_max :100valid_min :0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n58.11 GiB\n3.96 MiB\n\n\nShape\n(15044, 1, 720, 1440)\n(1, 1, 720, 1440)\n\n\nDask graph\n15044 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                        15044 1                          1440 720 1\n\n\n\n\nsst(time, zlev, lat, lon)float32dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;long_name :Daily sea surface temperatureunits :Celsiusvalid_max :4500valid_min :-300\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n58.11 GiB\n3.96 MiB\n\n\nShape\n(15044, 1, 720, 1440)\n(1, 1, 720, 1440)\n\n\nDask graph\n15044 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                        15044 1                          1440 720 1\n\n\n\n\nIndexes: (4)latPandasIndexPandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float32', name='lat', length=720))lonPandasIndexPandasIndex(Index([  0.125,   0.375,   0.625,   0.875,   1.125,   1.375,   1.625,   1.875,\n         2.125,   2.375,\n       ...\n       357.625, 357.875, 358.125, 358.375, 358.625, 358.875, 359.125, 359.375,\n       359.625, 359.875],\n      dtype='float32', name='lon', length=1440))timePandasIndexPandasIndex(DatetimeIndex(['1981-09-01 12:00:00', '1981-09-02 12:00:00',\n               '1981-09-03 12:00:00', '1981-09-04 12:00:00',\n               '1981-09-05 12:00:00', '1981-09-06 12:00:00',\n               '1981-09-07 12:00:00', '1981-09-08 12:00:00',\n               '1981-09-09 12:00:00', '1981-09-10 12:00:00',\n               ...\n               '2022-10-30 12:00:00', '2022-10-31 12:00:00',\n               '2022-11-01 12:00:00', '2022-11-02 12:00:00',\n               '2022-11-03 12:00:00', '2022-11-04 12:00:00',\n               '2022-11-05 12:00:00', '2022-11-06 12:00:00',\n               '2022-11-07 12:00:00', '2022-11-08 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=15044, freq=None))zlevPandasIndexPandasIndex(Index([0.0], dtype='float32', name='zlev'))Attributes: (37)Conventions :CF-1.6, ACDD-1.3cdm_data_type :Gridcomment :Data was converted from NetCDF-3 to NetCDF-4 format with metadata updates in November 2017.creator_email :oisst-help@noaa.govcreator_url :https://www.ncei.noaa.gov/date_created :2020-05-08T19:05:13Zdate_modified :2020-05-08T19:05:13Zgeospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :360.0geospatial_lon_min :0.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_easthistory :Final file created using preliminary as first guess, and 3 days of AVHRR data. Preliminary uses only 1 day of AVHRR data.id :oisst-avhrr-v02r01.19810901.ncinstitution :NOAA/National Centers for Environmental Informationinstrument :Earth Remote Sensing Instruments &gt; Passive Remote Sensing &gt; Spectrometers/Radiometers &gt; Imaging Spectrometers/Radiometers &gt; AVHRR &gt; Advanced Very High Resolution Radiometerinstrument_vocabulary :Global Change Master Directory (GCMD) Instrument Keywordskeywords :Earth Science &gt; Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :Global Change Master Directory (GCMD) Earth Science Keywordsmetadata_link :https://doi.org/10.25921/RE9P-PT57naming_authority :gov.noaa.nceincei_template_version :NCEI_NetCDF_Grid_Template_v2.0platform :Ships, buoys, Argo floats, MetOp-A, MetOp-Bplatform_vocabulary :Global Change Master Directory (GCMD) Platform Keywordsprocessing_level :NOAA Level 4product_version :Version v02r01references :Reynolds, et al.(2007) Daily High-Resolution-Blended Analyses for Sea Surface Temperature (available at https://doi.org/10.1175/2007JCLI1824.1). Banzon, et al.(2016) A long-term record of blended satellite and in situ sea-surface temperature for climate monitoring, modeling and environmental studies (available at https://doi.org/10.5194/essd-8-165-2016). Huang et al. (2020) Improvements of the Daily Optimum Interpolation Sea Surface Temperature (DOISST) Version v02r01, submitted.Climatology is based on 1971-2000 OI.v2 SST. Satellite data: Pathfinder AVHRR SST and Navy AVHRR SST. Ice data: NCEP Ice and GSFC Ice.sensor :Thermometer, AVHRRsource :ICOADS, NCEP_GTS, GSFC_ICE, NCEP_ICE, Pathfinder_AVHRR, Navy_AVHRRstandard_name_vocabulary :CF Standard Name Table (v40, 25 January 2017)summary :NOAAs 1/4-degree Daily Optimum Interpolation Sea Surface Temperature (OISST) (sometimes referred to as Reynolds SST, which however also refers to earlier products at different resolution), currently available as version v02r01, is created by interpolating and extrapolating SST observations from different sources, resulting in a smoothed complete field. The sources of data are satellite (AVHRR) and in situ platforms (i.e., ships and buoys), and the specific datasets employed may change over time. At the marginal ice zone, sea ice concentrations are used to generate proxy SSTs.  A preliminary version of this file is produced in near-real time (1-day latency), and then replaced with a final version after 2 weeks. Note that this is the AVHRR-ONLY DOISST, available from Oct 1981, but there is a companion DOISST product that includes microwave satellite data, available from June 2002time_coverage_end :1981-09-01T23:59:59Ztime_coverage_start :1981-09-01T00:00:00Ztitle :NOAA/NCEI 1/4 Degree Daily Optimum Interpolation Sea Surface Temperature (OISST) Analysis, Version 2.1 - Final",
    "crumbs": [
      "Formats",
      "Kerchunk",
      "Kerchunk in Practice"
    ]
  },
  {
    "objectID": "kerchunk/kerchunk-in-practice.html#other-examples-of-existing-kerchunk-data",
    "href": "kerchunk/kerchunk-in-practice.html#other-examples-of-existing-kerchunk-data",
    "title": "Kerchunk in Practice",
    "section": "Other examples of existing kerchunk data",
    "text": "Other examples of existing kerchunk data\n\nCase Studies on kerchunk Docs page\nKerchunk Cookbook from Project Pythia",
    "crumbs": [
      "Formats",
      "Kerchunk",
      "Kerchunk in Practice"
    ]
  },
  {
    "objectID": "cookbooks/index.html",
    "href": "cookbooks/index.html",
    "title": "Cloud-Optimized Cookbooks",
    "section": "",
    "text": "Cookbooks should address common questions and present solutions for cloud-optimized access and visualization.\nCookbooks:\n\nZarr Visualization Report (in development)",
    "crumbs": [
      "Cookbooks"
    ]
  },
  {
    "objectID": "geoparquet/geoparquet-example.html",
    "href": "geoparquet/geoparquet-example.html",
    "title": "GeoParquet Example",
    "section": "",
    "text": "This notebook will give an overview of how to read and write GeoParquet files with GeoPandas, putting an emphasis on cloud-native operations where possible.\nThe easiest way to read and write GeoParquet files is to use GeoPandas’ read_parquet and to_parquet functions.",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet Example"
    ]
  },
  {
    "objectID": "geoparquet/geoparquet-example.html#environment",
    "href": "geoparquet/geoparquet-example.html#environment",
    "title": "GeoParquet Example",
    "section": "Environment",
    "text": "Environment\nThe packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda create -f environment.yml\nor\nmamba create -f environment.yml\nThis notebook has been tested to work with the listed Conda environment. If you don’t want to use Conda or Mamba, install the latest versions of geopandas, fsspec, and pyarrow with pip. Note that you’ll also need the GDAL CLI with Parquet driver. If you’re on MacOS, you can install that via brew install gdal.",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet Example"
    ]
  },
  {
    "objectID": "geoparquet/geoparquet-example.html#imports",
    "href": "geoparquet/geoparquet-example.html#imports",
    "title": "GeoParquet Example",
    "section": "Imports",
    "text": "Imports\n\nfrom urllib.request import urlretrieve\n\nimport fsspec\nimport geopandas as gpd\nfrom fsspec.implementations.http import HTTPFileSystem",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet Example"
    ]
  },
  {
    "objectID": "geoparquet/geoparquet-example.html#comparison-with-flatgeobuf",
    "href": "geoparquet/geoparquet-example.html#comparison-with-flatgeobuf",
    "title": "GeoParquet Example",
    "section": "Comparison with FlatGeobuf",
    "text": "Comparison with FlatGeobuf\nIn order to compare reading GeoParquet with FlatGeobuf, we’ll cover reading and writing GeoParquet files on local disk storage. To be consistent with the FlatGeobuf example, we’ll fetch the same US counties FlatGeobuf file (13 MB) and convert it to GeoParquet using ogr2ogr.\n\n# URL to download\nurl = \"https://flatgeobuf.org/test/data/UScounties.fgb\"\n\n# Download, saving to the current directory\nlocal_fgb_path, _ = urlretrieve(url, \"countries.fgb\")\n\n\n!ogr2ogr countries.parquet countries.fgb\n\nLoading this GeoParquet file is really fast! 13% faster than loading the same data via FlatGeobuf (shown in the FlatGeobuf example notebook).\n\n%time gdf = gpd.read_parquet(\"countries.parquet\")\n\nCPU times: user 23.8 ms, sys: 11.8 ms, total: 35.6 ms\nWall time: 34.1 ms\n\n\n\ngdf\n\n\n\n\n\n\n\n\nSTATE_FIPS\nCOUNTY_FIP\nFIPS\nSTATE\nNAME\nLSAD\ngeometry\n\n\n\n\n0\n23\n009\n23009\nME\nHancock\nCounty\nMULTIPOLYGON (((-68.53108 44.33278, -68.53348 ...\n\n\n1\n33\n007\n33007\nNH\nCoos\nCounty\nMULTIPOLYGON (((-71.05975 45.01485, -71.06939 ...\n\n\n2\n50\n009\n50009\nVT\nEssex\nCounty\nMULTIPOLYGON (((-71.49463 44.90874, -71.49392 ...\n\n\n3\n50\n019\n50019\nVT\nOrleans\nCounty\nMULTIPOLYGON (((-72.14193 45.00600, -72.16051 ...\n\n\n4\n23\n007\n23007\nME\nFranklin\nCounty\nMULTIPOLYGON (((-70.83471 45.27514, -70.77984 ...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3216\n15\n003\n15003\nHI\nHonolulu\nCounty\nMULTIPOLYGON (((-171.73761 25.79210, -171.7513...\n\n\n3217\n15\n007\n15007\nHI\nKauai\nCounty\nMULTIPOLYGON (((-160.55535 21.66345, -160.5541...\n\n\n3218\n15\n009\n15009\nHI\nMaui\nCounty\nMULTIPOLYGON (((-157.06121 20.89150, -157.0611...\n\n\n3219\n15\n001\n15001\nHI\nHawaii\nCounty\nMULTIPOLYGON (((-155.08767 19.72887, -155.0909...\n\n\n3220\n15\n005\n15005\nHI\nKalawao\nCounty\nMULTIPOLYGON (((-157.01455 21.18550, -157.0145...\n\n\n\n\n3221 rows × 7 columns",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet Example"
    ]
  },
  {
    "objectID": "geoparquet/geoparquet-example.html#writing-to-local-disk",
    "href": "geoparquet/geoparquet-example.html#writing-to-local-disk",
    "title": "GeoParquet Example",
    "section": "Writing to local disk",
    "text": "Writing to local disk\nWe can use GeoDataFrame.to_parquet to write out this data to GeoParquet files locally. This is about 3x faster than writing the same dataset to FlatGeobuf, but note that FlatGeobuf’s writing is also calculating a spatial index.\n\n%time gdf.to_parquet(\"countries_written.parquet\")\n\nCPU times: user 42.3 ms, sys: 12.6 ms, total: 55 ms\nWall time: 53.9 ms",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet Example"
    ]
  },
  {
    "objectID": "geoparquet/geoparquet-example.html#reading-from-the-cloud",
    "href": "geoparquet/geoparquet-example.html#reading-from-the-cloud",
    "title": "GeoParquet Example",
    "section": "Reading from the cloud",
    "text": "Reading from the cloud\nAs of GeoParquet version 1.0.0-rc.1, spatial indexing has not yet been implemented. Therefore, there is not yet an API in GeoPandas to read data given a specific bounding box.\nWhat is already efficient in GeoParquet is reading only specified columns from a dataset.\n\nurl = \"https://data.source.coop/cholmes/eurocrops/unprojected/geoparquet/FR_2018_EC21.parquet\"\n\nNote that since we’re fetching this data directly from the cloud, we need to pass in an fsspec filesystem object. Otherwise GeoPandas will attempt to load a local file.\n\nfilesystem = HTTPFileSystem()\n\nBy default, calling read_parquet will fetch the entire file and parse it all into a single GeoDataFrame. Since this is a 3GB file, downloading the file takes a long time:\n\n# This cell will take a few minutes to run, because it downloads the entire file\n# %time gdf = gpd.read_parquet(url, filesystem=filesystem)\n\nWe can make this faster by only fetching specific columns. Because GeoParquet stores data in a columnar fashion, when selecting only specific columns we can download a lot less data.\n\n# This cell will take a few minutes to run, because it downloads the entire file for these columns\n# %time gdf = gpd.read_parquet(url, columns=[\"ID_PARCEL\", \"geometry\"], filesystem=filesystem)",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet Example"
    ]
  },
  {
    "objectID": "geoparquet/geoparquet-example.html#working-with-geoparquet-row-groups-advanced",
    "href": "geoparquet/geoparquet-example.html#working-with-geoparquet-row-groups-advanced",
    "title": "GeoParquet Example",
    "section": "Working with GeoParquet row groups (Advanced)",
    "text": "Working with GeoParquet row groups (Advanced)\nAs described in the intro document, GeoParquet is a chunked format, which allows you to access one of the chunks of rows very efficiently. This can allow you to stream a dataset — loading and operating on one chunk at a time — if the dataset is larger than your memory.\nGeoPandas does not yet have built-in support for working with row groups, so this section will use the underlying pyarrow library directly.\n\nimport pyarrow.parquet as pq\nfrom geopandas.io.arrow import _arrow_to_geopandas\n\nFirst, we’ll create a ParquetFile object from the remote URL. All this does is load the metadata from the file, allowing you to inspect the schema and number of columns, rows, and row groups. Because this doesn’t load any actual data, it’s nearly instant to complete.\n\nparquet_file = pq.ParquetFile(url, filesystem=filesystem)\n\nWe can access the column names in the dataset:\n\nparquet_file.schema_arrow.names\n\n['ID_PARCEL',\n 'SURF_PARC',\n 'CODE_CULTU',\n 'CODE_GROUP',\n 'CULTURE_D1',\n 'CULTURE_D2',\n 'EC_org_n',\n 'EC_trans_n',\n 'EC_hcat_n',\n 'EC_hcat_c',\n 'geometry']\n\n\nThis Parquet file includes 9.5 million rows:\n\nparquet_file.metadata.num_rows\n\n9517874\n\n\nAnd 146 row groups. Given that each row group is about the same number of rows, each one contains around 65,000 rows.\n\nparquet_file.num_row_groups\n\n146\n\n\nThen to load one of the row groups by numeric index, we can call ParquetFile.read_row_group.\n\npyarrow_table = parquet_file.read_row_group(0)\n\nNote that this returns a pyarrow.Table, not a geopandas.GeoDataFrame. To convert between the two, we can use _arrow_to_geopandas. This conversion is very fast.\n\ngeopandas_gdf = _arrow_to_geopandas(pyarrow_table, parquet_file.metadata.metadata)\n\nAs expected, this row group contains right around 65,000 rows\n\ngeopandas_gdf.shape\n\n(65536, 11)\n\n\n\ngeopandas_gdf.head()\n\n\n\n\n\n\n\n\nID_PARCEL\nSURF_PARC\nCODE_CULTU\nCODE_GROUP\nCULTURE_D1\nCULTURE_D2\nEC_org_n\nEC_trans_n\nEC_hcat_n\nEC_hcat_c\ngeometry\n\n\n\n\n0\n123563\n6.38\nCZH\n5\nNone\nNone\nColza d’hiver\nWinter rapeseed\nwinter_rapeseed_rape\n3301060401\nMULTIPOLYGON (((3.33896 49.84122, 3.33948 49.8...\n\n\n1\n5527076\n2.30\nPPH\n18\nNone\nNone\nPrairie permanente - herbe prédominante (resso...\nPermanent pasture - predominantly grass (woody...\npasture_meadow_grassland_grass\n3302000000\nMULTIPOLYGON (((-1.44483 49.61280, -1.44467 49...\n\n\n2\n11479241\n6.33\nPPH\n18\nNone\nNone\nPrairie permanente - herbe prédominante (resso...\nPermanent pasture - predominantly grass (woody...\npasture_meadow_grassland_grass\n3302000000\nMULTIPOLYGON (((2.87821 46.53674, 2.87820 46.5...\n\n\n3\n12928442\n5.10\nPPH\n18\nNone\nNone\nPrairie permanente - herbe prédominante (resso...\nPermanent pasture - predominantly grass (woody...\npasture_meadow_grassland_grass\n3302000000\nMULTIPOLYGON (((-0.19026 48.28723, -0.19025 48...\n\n\n4\n318389\n0.92\nPPH\n18\nNone\nNone\nPrairie permanente - herbe prédominante (resso...\nPermanent pasture - predominantly grass (woody...\npasture_meadow_grassland_grass\n3302000000\nMULTIPOLYGON (((5.72084 44.03576, 5.72081 44.0...\n\n\n\n\n\n\n\nAs before, we can speed up the data fetching by requesting only specific columns in the read_row_group call.:\n\npyarrow_table = parquet_file.read_row_group(0, columns=[\"ID_PARCEL\", \"geometry\"])\n\nThen the resulting GeoDataFrame will only have those two columns:\n\n_arrow_to_geopandas(pyarrow_table, parquet_file.metadata.metadata).head()\n\n\n\n\n\n\n\n\nID_PARCEL\ngeometry\n\n\n\n\n0\n123563\nMULTIPOLYGON (((3.33896 49.84122, 3.33948 49.8...\n\n\n1\n5527076\nMULTIPOLYGON (((-1.44483 49.61280, -1.44467 49...\n\n\n2\n11479241\nMULTIPOLYGON (((2.87821 46.53674, 2.87820 46.5...\n\n\n3\n12928442\nMULTIPOLYGON (((-0.19026 48.28723, -0.19025 48...\n\n\n4\n318389\nMULTIPOLYGON (((5.72084 44.03576, 5.72081 44.0...",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet Example"
    ]
  },
  {
    "objectID": "pmtiles/pmtiles-example.html",
    "href": "pmtiles/pmtiles-example.html",
    "title": "PMTiles example",
    "section": "",
    "text": "This notebook will give an overview of how to create and visualize PMTiles archives."
  },
  {
    "objectID": "pmtiles/pmtiles-example.html#environment",
    "href": "pmtiles/pmtiles-example.html#environment",
    "title": "PMTiles example",
    "section": "Environment",
    "text": "Environment\nThe packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda create -f environment.yml \nor\nmamba create -f environment.yml \nAlternatively, you can install pmtiles and mapbox-vector-tile through pip, and tippecanoe through Homebrew (brew install tippecanoe) if on MacOS."
  },
  {
    "objectID": "pmtiles/pmtiles-example.html#creating-pmtiles",
    "href": "pmtiles/pmtiles-example.html#creating-pmtiles",
    "title": "PMTiles example",
    "section": "Creating PMTiles",
    "text": "Creating PMTiles\nFor this example, we’ll use the same file as used in the FlatGeobuf and GeoParquet example notebooks: a 13MB file of US counties.\nWe’ll use Tippecanoe to convert this file into tiles.\nFirst we’ll download the file to our local directory:\n\n!wget https://flatgeobuf.org/test/data/UScounties.fgb\n\n--2023-08-23 15:54:58--  https://flatgeobuf.org/test/data/UScounties.fgb\nResolving flatgeobuf.org (flatgeobuf.org)... 185.199.108.153\nConnecting to flatgeobuf.org (flatgeobuf.org)|185.199.108.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14100008 (13M) [application/octet-stream]\nSaving to: ‘UScounties.fgb’\n\nUScounties.fgb      100%[===================&gt;]  13.45M  7.94MB/s    in 1.7s    \n\n2023-08-23 15:55:02 (7.94 MB/s) - ‘UScounties.fgb’ saved [14100008/14100008]\n\n\n\nTippecanoe has many options to customize its behavior. Here we’ll use the -zg flag to tell Tippecanoe to deduce appropriate minimum and maximum zoom levels for the dataset. The -o counties.pmtiles flag tells Tippecanoe to save the output with that name.\nTippecanoe also works especially well with FlatGeobuf files. When a FlatGeobuf file is used as input, Tippecanoe will reuse the spatial index stored in the FlatGeobuf file instead of creating its own.\n\n!tippecanoe UScounties.fgb -o counties.pmtiles -zg\n\nFor layer 0, using name \"UScountiesfgb\"\ndetected indexed FlatGeobuf: assigning feature IDs by sequence\n3221 features, 5580299 bytes of geometry, 53296 bytes of string pool\nChoosing a maxzoom of -z1 for features typically 141427 feet (43107 meters) apart, and at least 33249 feet (10135 meters) apart\nChoosing a maxzoom of -z7 for resolution of about 3195 feet (973 meters) within features\n  99.9%  7/36/49   \n  100.0%  7/127/42  \n\n\nNow we have a file named counties.pmtiles with our data:\n\n!ls -lh counties.pmtiles\n\n-rw-r--r--@ 1 kyle  staff   2.8M Aug 25 13:09 counties.pmtiles"
  },
  {
    "objectID": "pmtiles/pmtiles-example.html#visualization",
    "href": "pmtiles/pmtiles-example.html#visualization",
    "title": "PMTiles example",
    "section": "Visualization",
    "text": "Visualization\nThe easiest way to interpret this data is to load it into the PMTiles Viewer. Drag the counties.pmtiles file into that website, and you’ll be able to hover over areas"
  },
  {
    "objectID": "pmtiles/pmtiles-example.html#reading-from-python",
    "href": "pmtiles/pmtiles-example.html#reading-from-python",
    "title": "PMTiles example",
    "section": "Reading from Python",
    "text": "Reading from Python\nIt’s possible to open and read a PMTiles file from python using the pmtiles and mapbox-vector-tile libraries. The pmtiles library is used to open the archive and fetch a specific tile, while mapbox-vector-tile is used to decode the MVT vector tile data contained within that tile.\n\nfrom pmtiles.reader import Reader, MmapSource\n\nOpen the file and create a pmtiles Reader object\n\nfile = open(\"counties.pmtiles\")\nreader = Reader(MmapSource(file))\n\nFetch a specific tile. This tile’s coordinates were found from the PMTiles viewer above, and is located over the east coast.\n\nx, y, z = 37, 48, 7\ntile_data = reader.get(z, x, y)\n\ntile_data is now a bytes object, representing the data contained in the PMTiles archive for that specific XYZ tile.\n\ntype(tile_data)\n\nbytes\n\n\n\nlen(tile_data)\n\n11878\n\n\nIn our case, the PMTiles archive contains MVT data, so we can decode the buffer using mapbox_vector_tile. It’s also possible for the archive to contain raster images (e.g. PNG files), in which case a different decoding process would be necessary.\n\nimport mapbox_vector_tile\nimport gzip\n\nWe’ll decode the tile and print the output from mapbox_vector_tile. MVT data are encoded with “quantization”, meaning reduced precision so the data can be compressed better. So the coordinates printed out have a range of 0-4096, where those are the integer steps within the local coordinate reference system within the tile. Refer to the mapbox_vector_tile docs for how to read to GeoJSON.\n\nmapbox_vector_tile.decode(gzip.decompress(tile_data))\n\n{'UScountiesfgb': {'extent': 4096,\n  'version': 2,\n  'features': [{'geometry': {'type': 'Polygon',\n     'coordinates': [[[289, 4176],\n       [290, 4168],\n       [299, 4151],\n       [198, 4102],\n       [172, 4100],\n       [163, 4096],\n       [128, 4080],\n       [130, 4070],\n       [0, 4009],\n       [-71, 3976],\n       [-80, 3970],\n       [-80, 4176],\n       [289, 4176]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '079',\n     'FIPS': '42079',\n     'STATE': 'PA',\n     'NAME': 'Luzerne',\n     'LSAD': 'County'},\n    'id': 2224,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1272, 4176],\n       [1256, 4168],\n       [1247, 4167],\n       [1235, 4163],\n       [1226, 4152],\n       [1206, 4143],\n       [1204, 4139],\n       [1180, 4123],\n       [1175, 4118],\n       [1174, 4113],\n       [1174, 4106],\n       [1171, 4096],\n       [1168, 4090],\n       [1168, 4084],\n       [1171, 4079],\n       [1174, 4076],\n       [1177, 4075],\n       [1187, 4077],\n       [1190, 4074],\n       [1177, 4056],\n       [1154, 4041],\n       [1143, 4037],\n       [1119, 4035],\n       [1108, 4030],\n       [1106, 4020],\n       [1092, 4014],\n       [1081, 4012],\n       [1048, 3996],\n       [1042, 3980],\n       [1014, 3960],\n       [1027, 3941],\n       [976, 3908],\n       [967, 3890],\n       [952, 3877],\n       [928, 3864],\n       [898, 3857],\n       [868, 3837],\n       [807, 3809],\n       [758, 3800],\n       [753, 3795],\n       [721, 3785],\n       [675, 3778],\n       [663, 3807],\n       [648, 3835],\n       [529, 4041],\n       [620, 4096],\n       [643, 4110],\n       [590, 4176],\n       [1272, 4176]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '089',\n     'FIPS': '42089',\n     'STATE': 'PA',\n     'NAME': 'Monroe',\n     'LSAD': 'County'},\n    'id': 2227,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[590, 4176],\n       [643, 4110],\n       [620, 4096],\n       [529, 4041],\n       [598, 3923],\n       [655, 3822],\n       [675, 3778],\n       [668, 3775],\n       [625, 3767],\n       [609, 3766],\n       [596, 3762],\n       [577, 3763],\n       [562, 3758],\n       [553, 3752],\n       [524, 3748],\n       [498, 3737],\n       [494, 3730],\n       [488, 3726],\n       [478, 3725],\n       [399, 3706],\n       [363, 3696],\n       [354, 3692],\n       [342, 3677],\n       [304, 3661],\n       [299, 3656],\n       [289, 3642],\n       [282, 3641],\n       [281, 3635],\n       [262, 3625],\n       [75, 3781],\n       [0, 3867],\n       [-80, 3958],\n       [-80, 3970],\n       [0, 4009],\n       [130, 4070],\n       [128, 4080],\n       [163, 4096],\n       [172, 4100],\n       [198, 4102],\n       [299, 4151],\n       [290, 4168],\n       [289, 4176],\n       [590, 4176]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '025',\n     'FIPS': '42025',\n     'STATE': 'PA',\n     'NAME': 'Carbon',\n     'LSAD': 'County'},\n    'id': 2217,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1529, 4176],\n       [1594, 4096],\n       [1703, 3965],\n       [1691, 3952],\n       [1687, 3954],\n       [1681, 3943],\n       [1671, 3935],\n       [1671, 3928],\n       [1668, 3923],\n       [1651, 3914],\n       [1647, 3908],\n       [1648, 3894],\n       [1645, 3883],\n       [1646, 3880],\n       [1654, 3874],\n       [1652, 3869],\n       [1643, 3865],\n       [1642, 3855],\n       [1637, 3849],\n       [1627, 3845],\n       [1625, 3843],\n       [1622, 3833],\n       [1627, 3829],\n       [1630, 3821],\n       [1617, 3812],\n       [1611, 3804],\n       [1613, 3799],\n       [1606, 3792],\n       [1606, 3786],\n       [1598, 3780],\n       [1592, 3771],\n       [1589, 3771],\n       [1588, 3765],\n       [1583, 3757],\n       [1576, 3756],\n       [1569, 3752],\n       [1567, 3756],\n       [1562, 3755],\n       [1558, 3742],\n       [1546, 3737],\n       [1539, 3731],\n       [1535, 3733],\n       [1532, 3732],\n       [1517, 3719],\n       [1505, 3712],\n       [1507, 3706],\n       [1506, 3698],\n       [1507, 3695],\n       [1494, 3686],\n       [1489, 3680],\n       [1489, 3677],\n       [1485, 3672],\n       [1473, 3663],\n       [1470, 3658],\n       [1467, 3656],\n       [1465, 3651],\n       [1452, 3636],\n       [1451, 3631],\n       [1443, 3624],\n       [1435, 3610],\n       [1426, 3606],\n       [1424, 3602],\n       [1419, 3597],\n       [1410, 3577],\n       [1396, 3578],\n       [1393, 3569],\n       [1378, 3563],\n       [1361, 3545],\n       [1356, 3544],\n       [1351, 3547],\n       [1345, 3543],\n       [1341, 3547],\n       [1337, 3542],\n       [1333, 3542],\n       [1315, 3526],\n       [1310, 3526],\n       [1308, 3524],\n       [1303, 3517],\n       [1301, 3516],\n       [1295, 3518],\n       [1288, 3512],\n       [1282, 3510],\n       [1278, 3505],\n       [1275, 3499],\n       [1267, 3497],\n       [1264, 3494],\n       [1254, 3488],\n       [1247, 3476],\n       [1236, 3472],\n       [1235, 3468],\n       [1226, 3458],\n       [1211, 3451],\n       [1207, 3447],\n       [1208, 3442],\n       [1205, 3440],\n       [1180, 3431],\n       [1168, 3424],\n       [1153, 3420],\n       [1150, 3416],\n       [1150, 3408],\n       [1147, 3405],\n       [1141, 3407],\n       [1129, 3399],\n       [1130, 3393],\n       [1128, 3387],\n       [1124, 3385],\n       [1120, 3378],\n       [1114, 3375],\n       [1111, 3363],\n       [1104, 3361],\n       [1102, 3355],\n       [1096, 3351],\n       [1088, 3349],\n       [1085, 3370],\n       [1072, 3393],\n       [1071, 3398],\n       [1077, 3402],\n       [1088, 3404],\n       [1091, 3412],\n       [1085, 3444],\n       [1073, 3455],\n       [1073, 3460],\n       [1087, 3484],\n       [1099, 3492],\n       [1107, 3505],\n       [1108, 3511],\n       [1107, 3514],\n       [1102, 3518],\n       [1097, 3519],\n       [1088, 3518],\n       [1079, 3521],\n       [1073, 3529],\n       [1068, 3541],\n       [1082, 3584],\n       [1089, 3592],\n       [1100, 3618],\n       [1095, 3629],\n       [1081, 3645],\n       [1079, 3656],\n       [1086, 3664],\n       [1099, 3671],\n       [1105, 3676],\n       [1107, 3681],\n       [1110, 3697],\n       [1115, 3707],\n       [1118, 3708],\n       [1127, 3708],\n       [1148, 3701],\n       [1170, 3699],\n       [1174, 3703],\n       [1186, 3724],\n       [1195, 3729],\n       [1203, 3730],\n       [1207, 3732],\n       [1219, 3749],\n       [1219, 3764],\n       [1225, 3773],\n       [1234, 3780],\n       [1243, 3797],\n       [1243, 3803],\n       [1241, 3807],\n       [1224, 3825],\n       [1223, 3832],\n       [1226, 3840],\n       [1233, 3844],\n       [1254, 3845],\n       [1262, 3841],\n       [1269, 3841],\n       [1272, 3842],\n       [1288, 3865],\n       [1291, 3876],\n       [1290, 3885],\n       [1287, 3891],\n       [1280, 3899],\n       [1270, 3914],\n       [1256, 3931],\n       [1255, 3956],\n       [1250, 3969],\n       [1226, 3989],\n       [1212, 4012],\n       [1211, 4018],\n       [1203, 4035],\n       [1194, 4044],\n       [1190, 4065],\n       [1191, 4070],\n       [1190, 4074],\n       [1187, 4077],\n       [1177, 4075],\n       [1174, 4076],\n       [1171, 4079],\n       [1168, 4084],\n       [1168, 4090],\n       [1171, 4096],\n       [1174, 4106],\n       [1174, 4113],\n       [1175, 4118],\n       [1180, 4123],\n       [1204, 4139],\n       [1206, 4143],\n       [1226, 4152],\n       [1235, 4163],\n       [1247, 4167],\n       [1256, 4168],\n       [1272, 4176],\n       [1529, 4176]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '041',\n     'FIPS': '34041',\n     'STATE': 'NJ',\n     'NAME': 'Warren',\n     'LSAD': 'County'},\n    'id': 2207,\n    'type': 'Feature'},\n   {'geometry': {'type': 'MultiPolygon',\n     'coordinates': [[[[2427, 4176],\n        [2425, 4170],\n        [2437, 4158],\n        [2460, 4120],\n        [2494, 4099],\n        [2518, 4096],\n        [2573, 4090],\n        [2573, 4096],\n        [2574, 4104],\n        [2583, 4096],\n        [2616, 4070],\n        [2606, 4022],\n        [2612, 4020],\n        [2619, 4007],\n        [2615, 3999],\n        [2616, 3992],\n        [2619, 3987],\n        [2630, 3985],\n        [2633, 3981],\n        [2628, 3967],\n        [2630, 3956],\n        [2627, 3941],\n        [2634, 3930],\n        [2632, 3915],\n        [2646, 3904],\n        [2654, 3892],\n        [2656, 3885],\n        [2661, 3874],\n        [2665, 3871],\n        [2665, 3860],\n        [2662, 3856],\n        [2659, 3857],\n        [2656, 3864],\n        [2649, 3864],\n        [2647, 3855],\n        [2648, 3849],\n        [2644, 3833],\n        [2641, 3826],\n        [2645, 3808],\n        [2641, 3793],\n        [2638, 3790],\n        [2632, 3788],\n        [2552, 3846],\n        [2534, 3860],\n        [2536, 3865],\n        [2529, 3868],\n        [2527, 3864],\n        [2472, 3904],\n        [2453, 3906],\n        [2451, 3909],\n        [2443, 3911],\n        [2437, 3917],\n        [2433, 3931],\n        [2425, 3938],\n        [2424, 3942],\n        [2427, 3948],\n        [2424, 3953],\n        [2426, 3956],\n        [2426, 3962],\n        [2430, 3957],\n        [2432, 3957],\n        [2433, 3959],\n        [2426, 3972],\n        [2416, 3973],\n        [2416, 3978],\n        [2408, 3975],\n        [2408, 3980],\n        [2405, 3984],\n        [2407, 3986],\n        [2402, 3989],\n        [2402, 3995],\n        [2401, 3997],\n        [2407, 4002],\n        [2411, 4013],\n        [2415, 4025],\n        [2418, 4047],\n        [2414, 4053],\n        [2415, 4069],\n        [2413, 4071],\n        [2412, 4075],\n        [2413, 4081],\n        [2417, 4079],\n        [2418, 4085],\n        [2412, 4090],\n        [2412, 4096],\n        [2407, 4094],\n        [2399, 4096],\n        [2392, 4094],\n        [2389, 4096],\n        [2389, 4103],\n        [2396, 4111],\n        [2391, 4117],\n        [2391, 4125],\n        [2388, 4128],\n        [2388, 4136],\n        [2387, 4140],\n        [2384, 4140],\n        [2382, 4133],\n        [2370, 4132],\n        [2363, 4139],\n        [2344, 4137],\n        [2341, 4140],\n        [2334, 4142],\n        [2334, 4147],\n        [2329, 4148],\n        [2325, 4146],\n        [2324, 4148],\n        [2318, 4144],\n        [2310, 4144],\n        [2310, 4156],\n        [2306, 4160],\n        [2298, 4162],\n        [2293, 4160],\n        [2292, 4157],\n        [2281, 4157],\n        [2274, 4161],\n        [2265, 4162],\n        [2259, 4157],\n        [2255, 4159],\n        [2251, 4165],\n        [2240, 4168],\n        [2235, 4173],\n        [2234, 4176],\n        [2427, 4176]]],\n      [[[2203, 4176],\n        [2202, 4172],\n        [2196, 4170],\n        [2189, 4174],\n        [2190, 4176],\n        [2203, 4176]]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '031',\n     'FIPS': '34031',\n     'STATE': 'NJ',\n     'NAME': 'Passaic',\n     'LSAD': 'County'},\n    'id': 2230,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2981, 4176],\n       [2977, 4145],\n       [2976, 4129],\n       [2960, 4130],\n       [2885, 4176],\n       [2981, 4176]]]},\n    'properties': {'STATE_FIPS': '36',\n     'COUNTY_FIP': '087',\n     'FIPS': '36087',\n     'STATE': 'NY',\n     'NAME': 'Rockland',\n     'LSAD': 'County'},\n    'id': 2226,\n    'type': 'Feature'},\n   {'geometry': {'type': 'MultiPolygon',\n     'coordinates': [[[[3469, 4176],\n        [3460, 4167],\n        [3451, 4156],\n        [3448, 4138],\n        [3428, 4137],\n        [3426, 4155],\n        [3410, 4166],\n        [3398, 4164],\n        [3341, 4139],\n        [3330, 4126],\n        [3321, 4106],\n        [3317, 4111],\n        [3319, 4122],\n        [3318, 4134],\n        [3325, 4157],\n        [3312, 4176],\n        [3469, 4176]]],\n      [[[3519, 4176],\n        [3517, 4172],\n        [3508, 4166],\n        [3505, 4176],\n        [3519, 4176]]]]},\n    'properties': {'STATE_FIPS': '09',\n     'COUNTY_FIP': '001',\n     'FIPS': '09001',\n     'STATE': 'CT',\n     'NAME': 'Fairfield',\n     'LSAD': 'County'},\n    'id': 1713,\n    'type': 'Feature'},\n   {'geometry': {'type': 'MultiPolygon',\n     'coordinates': [[[[3156, 3920],\n        [3161, 3917],\n        [3162, 3906],\n        [3161, 3904],\n        [3155, 3902],\n        [3149, 3908],\n        [3149, 3912],\n        [3153, 3912],\n        [3153, 3918],\n        [3156, 3920]]],\n      [[[3312, 4176],\n        [3325, 4157],\n        [3318, 4134],\n        [3319, 4122],\n        [3317, 4111],\n        [3321, 4106],\n        [3323, 4096],\n        [3317, 4074],\n        [3314, 4070],\n        [3310, 4072],\n        [3291, 4063],\n        [3283, 4036],\n        [3278, 4029],\n        [3262, 4018],\n        [3227, 4004],\n        [3212, 3990],\n        [3176, 3966],\n        [3140, 3914],\n        [3137, 3905],\n        [3079, 3925],\n        [3078, 3922],\n        [3057, 3931],\n        [3053, 3950],\n        [3048, 3950],\n        [3048, 3952],\n        [3042, 3955],\n        [3036, 3956],\n        [3039, 3961],\n        [3036, 3961],\n        [3033, 3958],\n        [3034, 3955],\n        [3031, 3954],\n        [3026, 3943],\n        [2941, 3976],\n        [2972, 4096],\n        [2981, 4176],\n        [3312, 4176]]]]},\n    'properties': {'STATE_FIPS': '36',\n     'COUNTY_FIP': '119',\n     'FIPS': '36119',\n     'STATE': 'NY',\n     'NAME': 'Westchester',\n     'LSAD': 'County'},\n    'id': 2232,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[-80, 3958],\n       [0, 3867],\n       [75, 3781],\n       [219, 3660],\n       [262, 3625],\n       [250, 3617],\n       [232, 3613],\n       [230, 3605],\n       [205, 3590],\n       [189, 3572],\n       [135, 3544],\n       [120, 3544],\n       [67, 3514],\n       [0, 3481],\n       [-80, 3442],\n       [-80, 3958]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '107',\n     'FIPS': '42107',\n     'STATE': 'PA',\n     'NAME': 'Schuylkill',\n     'LSAD': 'County'},\n    'id': 2216,\n    'type': 'Feature'},\n   {'geometry': {'type': 'MultiPolygon',\n     'coordinates': [[[[-80, 2134],\n        [-80, 2627],\n        [0, 2560],\n        [93, 2480],\n        [69, 2466],\n        [0, 2431],\n        [-8, 2427],\n        [0, 2380],\n        [2, 2369],\n        [2, 2342],\n        [5, 2310],\n        [3, 2293],\n        [0, 2287],\n        [-72, 2157],\n        [-71, 2150],\n        [-77, 2142],\n        [-80, 2134]]],\n      [[[-80, 2117],\n        [-77, 2115],\n        [-74, 2105],\n        [-65, 2097],\n        [-63, 2093],\n        [-64, 2086],\n        [-69, 2090],\n        [-73, 2086],\n        [-79, 2085],\n        [-78, 2081],\n        [-80, 2080],\n        [-80, 2117]]],\n      [[[-80, 1996],\n        [-74, 1988],\n        [-72, 1979],\n        [-77, 1974],\n        [-79, 1970],\n        [-80, 1971],\n        [-80, 1996]]],\n      [[[-80, 2026],\n        [-73, 2020],\n        [-72, 2015],\n        [-76, 2012],\n        [-78, 2006],\n        [-79, 2001],\n        [-77, 2000],\n        [-80, 1996],\n        [-80, 2026]]],\n      [[[-80, 2123], [-78, 2120], [-80, 2117], [-80, 2123]]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '071',\n     'FIPS': '42071',\n     'STATE': 'PA',\n     'NAME': 'Lancaster',\n     'LSAD': 'County'},\n    'id': 2185,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[223, 1051],\n       [235, 1047],\n       [235, 1044],\n       [240, 1045],\n       [249, 1041],\n       [264, 794],\n       [262, 796],\n       [255, 795],\n       [247, 797],\n       [241, 794],\n       [236, 796],\n       [233, 793],\n       [232, 795],\n       [223, 792],\n       [222, 791],\n       [210, 788],\n       [209, 790],\n       [205, 785],\n       [197, 787],\n       [195, 790],\n       [191, 786],\n       [185, 792],\n       [171, 794],\n       [163, 799],\n       [162, 802],\n       [157, 803],\n       [148, 808],\n       [143, 809],\n       [142, 816],\n       [136, 816],\n       [122, 820],\n       [120, 824],\n       [113, 821],\n       [106, 823],\n       [92, 809],\n       [92, 801],\n       [86, 802],\n       [84, 804],\n       [75, 800],\n       [71, 807],\n       [67, 809],\n       [63, 806],\n       [63, 797],\n       [29, 789],\n       [18, 790],\n       [0, 782],\n       [-5, 780],\n       [-10, 781],\n       [-16, 787],\n       [-21, 788],\n       [-27, 784],\n       [-30, 784],\n       [-37, 786],\n       [-43, 793],\n       [-48, 795],\n       [-58, 792],\n       [-70, 794],\n       [-71, 783],\n       [-75, 781],\n       [-80, 780],\n       [-80, 1037],\n       [-75, 1034],\n       [-64, 1021],\n       [-60, 1021],\n       [-56, 1022],\n       [-48, 1033],\n       [-39, 1036],\n       [-21, 1034],\n       [-17, 1032],\n       [-11, 1025],\n       [-5, 1023],\n       [0, 1025],\n       [8, 1030],\n       [14, 1030],\n       [21, 1022],\n       [49, 1017],\n       [63, 1019],\n       [71, 1012],\n       [76, 1010],\n       [81, 1017],\n       [102, 1023],\n       [107, 1021],\n       [111, 1023],\n       [120, 1017],\n       [130, 1024],\n       [134, 1023],\n       [139, 1029],\n       [155, 1035],\n       [166, 1049],\n       [173, 1050],\n       [176, 1047],\n       [186, 1046],\n       [191, 1038],\n       [193, 1037],\n       [198, 1042],\n       [206, 1041],\n       [209, 1046],\n       [213, 1046],\n       [216, 1048],\n       [222, 1046],\n       [223, 1051]]]},\n    'properties': {'STATE_FIPS': '24',\n     'COUNTY_FIP': '029',\n     'FIPS': '24029',\n     'STATE': 'MD',\n     'NAME': 'Kent',\n     'LSAD': 'County'},\n    'id': 2126,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[120, 824],\n       [122, 820],\n       [136, 816],\n       [142, 816],\n       [143, 809],\n       [148, 808],\n       [157, 803],\n       [162, 802],\n       [163, 799],\n       [171, 794],\n       [185, 792],\n       [191, 786],\n       [195, 790],\n       [197, 787],\n       [205, 785],\n       [209, 790],\n       [210, 788],\n       [222, 791],\n       [223, 792],\n       [232, 795],\n       [233, 793],\n       [236, 796],\n       [241, 794],\n       [247, 797],\n       [255, 795],\n       [262, 796],\n       [264, 794],\n       [276, 601],\n       [262, 592],\n       [236, 584],\n       [213, 581],\n       [209, 575],\n       [202, 573],\n       [197, 566],\n       [186, 558],\n       [167, 548],\n       [153, 536],\n       [149, 535],\n       [143, 520],\n       [137, 502],\n       [133, 500],\n       [130, 495],\n       [132, 473],\n       [129, 466],\n       [116, 452],\n       [112, 447],\n       [108, 443],\n       [99, 440],\n       [96, 432],\n       [89, 422],\n       [86, 421],\n       [86, 418],\n       [88, 417],\n       [89, 414],\n       [87, 405],\n       [84, 404],\n       [82, 396],\n       [79, 393],\n       [79, 389],\n       [78, 387],\n       [80, 384],\n       [73, 382],\n       [71, 377],\n       [68, 374],\n       [68, 366],\n       [57, 359],\n       [49, 364],\n       [47, 361],\n       [49, 354],\n       [49, 352],\n       [42, 352],\n       [42, 348],\n       [39, 340],\n       [35, 339],\n       [33, 335],\n       [26, 332],\n       [23, 322],\n       [11, 315],\n       [13, 311],\n       [7, 307],\n       [4, 299],\n       [4, 295],\n       [0, 292],\n       [-5, 289],\n       [-3, 285],\n       [0, 284],\n       [1, 283],\n       [0, 278],\n       [-8, 270],\n       [-1, 257],\n       [-2, 253],\n       [-4, 254],\n       [-7, 250],\n       [-7, 238],\n       [-10, 234],\n       [-20, 231],\n       [-19, 222],\n       [-14, 222],\n       [-14, 220],\n       [-23, 217],\n       [-21, 214],\n       [-17, 213],\n       [-15, 210],\n       [-16, 204],\n       [-17, 202],\n       [-22, 201],\n       [-22, 200],\n       [-18, 190],\n       [-17, 179],\n       [-45, 186],\n       [-50, 186],\n       [-59, 190],\n       [-80, 194],\n       [-80, 780],\n       [-75, 781],\n       [-71, 783],\n       [-70, 794],\n       [-58, 792],\n       [-48, 795],\n       [-43, 793],\n       [-37, 786],\n       [-30, 784],\n       [-27, 784],\n       [-21, 788],\n       [-16, 787],\n       [-10, 781],\n       [-5, 780],\n       [0, 782],\n       [18, 790],\n       [29, 789],\n       [63, 797],\n       [63, 806],\n       [67, 809],\n       [71, 807],\n       [75, 800],\n       [84, 804],\n       [86, 802],\n       [92, 801],\n       [92, 809],\n       [106, 823],\n       [113, 821],\n       [120, 824]]]},\n    'properties': {'STATE_FIPS': '24',\n     'COUNTY_FIP': '035',\n     'FIPS': '24035',\n     'STATE': 'MD',\n     'NAME': \"Queen Anne's\",\n     'LSAD': 'County'},\n    'id': 2137,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[-80, 194],\n       [-59, 190],\n       [-50, 186],\n       [-45, 186],\n       [-17, 179],\n       [-10, 176],\n       [-9, 172],\n       [-16, 172],\n       [-16, 167],\n       [-8, 158],\n       [-9, 156],\n       [-15, 153],\n       [-16, 149],\n       [-11, 145],\n       [-10, 140],\n       [-12, 133],\n       [-16, 130],\n       [-16, 129],\n       [-10, 126],\n       [-4, 117],\n       [-6, 115],\n       [-18, 116],\n       [-20, 115],\n       [-20, 108],\n       [-16, 104],\n       [-3, 110],\n       [0, 107],\n       [0, 104],\n       [-2, 101],\n       [-9, 101],\n       [-10, 98],\n       [-6, 85],\n       [-3, 82],\n       [0, 83],\n       [4, 84],\n       [11, 72],\n       [19, 71],\n       [22, 65],\n       [22, 62],\n       [10, 53],\n       [16, 46],\n       [26, 45],\n       [27, 44],\n       [26, 42],\n       [14, 35],\n       [14, 30],\n       [26, 24],\n       [33, 23],\n       [36, 13],\n       [53, 4],\n       [51, 0],\n       [47, -2],\n       [45, -5],\n       [50, -11],\n       [59, -14],\n       [60, -19],\n       [55, -20],\n       [49, -29],\n       [38, -30],\n       [14, -44],\n       [7, -49],\n       [0, -66],\n       [-15, -80],\n       [-80, -80],\n       [-80, 194]]]},\n    'properties': {'STATE_FIPS': '24',\n     'COUNTY_FIP': '041',\n     'FIPS': '24041',\n     'STATE': 'MD',\n     'NAME': 'Talbot',\n     'LSAD': 'County'},\n    'id': 2156,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[478, 3725],\n       [484, 3712],\n       [485, 3703],\n       [482, 3694],\n       [487, 3684],\n       [486, 3666],\n       [489, 3653],\n       [496, 3641],\n       [502, 3627],\n       [509, 3622],\n       [528, 3625],\n       [533, 3621],\n       [538, 3607],\n       [543, 3602],\n       [552, 3597],\n       [558, 3598],\n       [571, 3604],\n       [576, 3611],\n       [578, 3626],\n       [583, 3630],\n       [593, 3627],\n       [598, 3623],\n       [599, 3618],\n       [600, 3595],\n       [605, 3582],\n       [622, 3548],\n       [633, 3537],\n       [640, 3517],\n       [643, 3515],\n       [653, 3511],\n       [659, 3502],\n       [661, 3494],\n       [661, 3481],\n       [664, 3472],\n       [739, 3508],\n       [739, 3497],\n       [783, 3442],\n       [795, 3430],\n       [801, 3429],\n       [800, 3418],\n       [807, 3411],\n       [806, 3404],\n       [808, 3398],\n       [811, 3398],\n       [813, 3394],\n       [799, 3393],\n       [795, 3359],\n       [777, 3330],\n       [777, 3328],\n       [880, 3245],\n       [769, 3151],\n       [660, 3017],\n       [165, 3428],\n       [67, 3514],\n       [120, 3544],\n       [135, 3544],\n       [189, 3572],\n       [205, 3590],\n       [230, 3605],\n       [232, 3613],\n       [250, 3617],\n       [262, 3625],\n       [281, 3635],\n       [282, 3641],\n       [289, 3642],\n       [299, 3656],\n       [304, 3661],\n       [342, 3677],\n       [354, 3692],\n       [380, 3701],\n       [478, 3725]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '077',\n     'FIPS': '42077',\n     'STATE': 'PA',\n     'NAME': 'Lehigh',\n     'LSAD': 'County'},\n    'id': 2212,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[67, 3514],\n       [165, 3428],\n       [594, 3072],\n       [351, 2680],\n       [283, 2632],\n       [93, 2480],\n       [0, 2560],\n       [-25, 2581],\n       [-80, 2627],\n       [-80, 3442],\n       [0, 3481],\n       [67, 3514]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '011',\n     'FIPS': '42011',\n     'STATE': 'PA',\n     'NAME': 'Berks',\n     'LSAD': 'County'},\n    'id': 2193,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[594, 3072],\n       [660, 3017],\n       [745, 2952],\n       [937, 2799],\n       [1053, 2710],\n       [1176, 2612],\n       [1343, 2482],\n       [1335, 2471],\n       [1280, 2421],\n       [1252, 2383],\n       [1245, 2377],\n       [1239, 2369],\n       [1224, 2351],\n       [1238, 2340],\n       [1228, 2327],\n       [1210, 2313],\n       [1206, 2307],\n       [1108, 2381],\n       [1091, 2358],\n       [1040, 2396],\n       [980, 2322],\n       [987, 2313],\n       [1002, 2299],\n       [1007, 2292],\n       [1018, 2281],\n       [1027, 2273],\n       [1048, 2259],\n       [1056, 2249],\n       [1065, 2242],\n       [1063, 2239],\n       [963, 2176],\n       [952, 2195],\n       [912, 2257],\n       [898, 2250],\n       [846, 2340],\n       [841, 2345],\n       [839, 2344],\n       [831, 2357],\n       [794, 2335],\n       [754, 2404],\n       [728, 2390],\n       [723, 2399],\n       [701, 2386],\n       [700, 2400],\n       [691, 2405],\n       [693, 2416],\n       [690, 2416],\n       [682, 2423],\n       [679, 2432],\n       [682, 2441],\n       [692, 2447],\n       [694, 2452],\n       [693, 2457],\n       [677, 2466],\n       [665, 2461],\n       [648, 2460],\n       [629, 2472],\n       [624, 2479],\n       [621, 2488],\n       [625, 2498],\n       [635, 2512],\n       [636, 2519],\n       [633, 2522],\n       [624, 2524],\n       [620, 2522],\n       [614, 2514],\n       [610, 2502],\n       [607, 2500],\n       [602, 2501],\n       [598, 2504],\n       [597, 2511],\n       [598, 2521],\n       [595, 2538],\n       [591, 2544],\n       [580, 2553],\n       [576, 2558],\n       [566, 2580],\n       [555, 2613],\n       [549, 2619],\n       [544, 2618],\n       [543, 2612],\n       [546, 2604],\n       [546, 2596],\n       [539, 2589],\n       [535, 2589],\n       [530, 2591],\n       [522, 2602],\n       [519, 2612],\n       [510, 2627],\n       [496, 2670],\n       [491, 2673],\n       [487, 2673],\n       [483, 2671],\n       [481, 2669],\n       [481, 2664],\n       [487, 2654],\n       [487, 2652],\n       [483, 2648],\n       [474, 2644],\n       [468, 2648],\n       [463, 2658],\n       [428, 2674],\n       [419, 2680],\n       [411, 2679],\n       [406, 2680],\n       [398, 2686],\n       [388, 2685],\n       [381, 2681],\n       [367, 2670],\n       [360, 2670],\n       [351, 2680],\n       [594, 3072]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '091',\n     'FIPS': '42091',\n     'STATE': 'PA',\n     'NAME': 'Montgomery',\n     'LSAD': 'County'},\n    'id': 2198,\n    'type': 'Feature'},\n   {'geometry': {'type': 'MultiPolygon',\n     'coordinates': [[[[398, 2686],\n        [406, 2680],\n        [411, 2679],\n        [419, 2680],\n        [428, 2674],\n        [463, 2658],\n        [468, 2648],\n        [474, 2644],\n        [483, 2648],\n        [487, 2652],\n        [487, 2654],\n        [481, 2664],\n        [481, 2669],\n        [483, 2671],\n        [487, 2673],\n        [491, 2673],\n        [496, 2670],\n        [510, 2627],\n        [519, 2612],\n        [521, 2604],\n        [525, 2597],\n        [535, 2589],\n        [539, 2589],\n        [546, 2596],\n        [546, 2604],\n        [543, 2612],\n        [544, 2618],\n        [549, 2619],\n        [555, 2613],\n        [566, 2580],\n        [576, 2558],\n        [580, 2553],\n        [591, 2544],\n        [595, 2538],\n        [598, 2521],\n        [597, 2511],\n        [598, 2504],\n        [602, 2501],\n        [607, 2500],\n        [610, 2502],\n        [614, 2514],\n        [620, 2522],\n        [624, 2524],\n        [633, 2522],\n        [636, 2519],\n        [635, 2512],\n        [625, 2498],\n        [621, 2488],\n        [624, 2479],\n        [629, 2472],\n        [648, 2460],\n        [665, 2461],\n        [677, 2466],\n        [693, 2457],\n        [694, 2452],\n        [692, 2447],\n        [682, 2441],\n        [679, 2432],\n        [682, 2423],\n        [690, 2416],\n        [693, 2416],\n        [691, 2405],\n        [700, 2400],\n        [701, 2386],\n        [723, 2399],\n        [728, 2390],\n        [754, 2404],\n        [794, 2335],\n        [831, 2357],\n        [839, 2344],\n        [761, 2299],\n        [776, 2272],\n        [705, 2207],\n        [720, 2208],\n        [724, 2206],\n        [728, 2208],\n        [727, 2204],\n        [731, 2198],\n        [730, 2191],\n        [602, 2113],\n        [609, 2103],\n        [610, 2095],\n        [601, 2090],\n        [599, 2096],\n        [593, 2093],\n        [594, 2091],\n        [593, 2090],\n        [603, 2073],\n        [600, 2071],\n        [598, 2074],\n        [593, 2072],\n        [585, 2086],\n        [575, 2081],\n        [577, 2077],\n        [573, 2074],\n        [573, 2068],\n        [575, 2064],\n        [564, 2058],\n        [570, 2048],\n        [555, 2040],\n        [552, 2046],\n        [548, 2044],\n        [551, 2035],\n        [530, 2023],\n        [532, 2017],\n        [529, 2015],\n        [534, 2007],\n        [520, 1999],\n        [517, 2002],\n        [510, 2000],\n        [514, 1993],\n        [495, 1981],\n        [501, 1972],\n        [502, 1967],\n        [498, 1966],\n        [496, 1962],\n        [497, 1959],\n        [489, 1947],\n        [490, 1945],\n        [493, 1943],\n        [503, 1939],\n        [503, 1937],\n        [500, 1935],\n        [499, 1925],\n        [495, 1918],\n        [495, 1915],\n        [499, 1911],\n        [466, 1904],\n        [431, 1894],\n        [400, 1880],\n        [366, 1861],\n        [344, 1845],\n        [306, 1810],\n        [281, 1779],\n        [258, 1740],\n        [239, 1693],\n        [217, 1692],\n        [0, 1691],\n        [-80, 1691],\n        [-80, 1971],\n        [-79, 1970],\n        [-77, 1974],\n        [-72, 1979],\n        [-72, 1982],\n        [-74, 1988],\n        [-80, 1996],\n        [-77, 2000],\n        [-79, 2001],\n        [-78, 2006],\n        [-76, 2012],\n        [-72, 2015],\n        [-73, 2020],\n        [-80, 2026],\n        [-80, 2080],\n        [-78, 2081],\n        [-79, 2085],\n        [-73, 2086],\n        [-69, 2090],\n        [-64, 2086],\n        [-63, 2093],\n        [-65, 2097],\n        [-74, 2105],\n        [-77, 2115],\n        [-80, 2117],\n        [-78, 2120],\n        [-80, 2123],\n        [-80, 2134],\n        [-77, 2142],\n        [-71, 2150],\n        [-72, 2157],\n        [0, 2287],\n        [3, 2293],\n        [5, 2310],\n        [2, 2342],\n        [2, 2369],\n        [0, 2380],\n        [-8, 2427],\n        [0, 2431],\n        [69, 2466],\n        [93, 2480],\n        [283, 2632],\n        [351, 2680],\n        [360, 2670],\n        [367, 2670],\n        [381, 2681],\n        [388, 2685],\n        [398, 2686]]],\n      [[[512, 1924],\n        [518, 1918],\n        [521, 1913],\n        [502, 1911],\n        [506, 1921],\n        [512, 1924]]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '029',\n     'FIPS': '42029',\n     'STATE': 'PA',\n     'NAME': 'Chester',\n     'LSAD': 'County'},\n    'id': 2187,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[841, 2345],\n       [846, 2340],\n       [898, 2250],\n       [912, 2257],\n       [952, 2195],\n       [963, 2176],\n       [957, 2172],\n       [959, 2167],\n       [964, 2161],\n       [972, 2160],\n       [979, 2153],\n       [986, 2155],\n       [991, 2151],\n       [995, 2154],\n       [1004, 2151],\n       [1004, 2146],\n       [1000, 2142],\n       [1003, 2138],\n       [1002, 2135],\n       [999, 2128],\n       [1001, 2118],\n       [1006, 2112],\n       [1016, 2109],\n       [1016, 2102],\n       [1020, 2102],\n       [1023, 2101],\n       [1020, 2092],\n       [1013, 2093],\n       [1009, 2087],\n       [1011, 2086],\n       [1011, 2082],\n       [1005, 2071],\n       [1006, 2056],\n       [999, 2049],\n       [1001, 2045],\n       [996, 2040],\n       [999, 2038],\n       [993, 2029],\n       [994, 2027],\n       [990, 2021],\n       [992, 2020],\n       [988, 2015],\n       [984, 2014],\n       [983, 2008],\n       [980, 2007],\n       [983, 2002],\n       [980, 2001],\n       [983, 1996],\n       [980, 1996],\n       [983, 1985],\n       [1004, 1985],\n       [1015, 1977],\n       [1023, 1986],\n       [1045, 1983],\n       [1051, 1984],\n       [1051, 1975],\n       [1058, 1964],\n       [1043, 1956],\n       [1023, 1947],\n       [970, 1934],\n       [938, 1932],\n       [914, 1935],\n       [895, 1934],\n       [884, 1933],\n       [868, 1927],\n       [849, 1916],\n       [797, 1869],\n       [761, 1843],\n       [742, 1857],\n       [705, 1878],\n       [664, 1895],\n       [639, 1903],\n       [610, 1909],\n       [580, 1912],\n       [535, 1914],\n       [521, 1913],\n       [518, 1918],\n       [512, 1924],\n       [508, 1922],\n       [502, 1911],\n       [499, 1911],\n       [495, 1915],\n       [495, 1918],\n       [499, 1925],\n       [500, 1935],\n       [503, 1937],\n       [503, 1939],\n       [493, 1943],\n       [490, 1945],\n       [489, 1947],\n       [497, 1959],\n       [496, 1962],\n       [498, 1966],\n       [502, 1967],\n       [501, 1972],\n       [495, 1981],\n       [514, 1993],\n       [510, 2000],\n       [517, 2002],\n       [520, 1999],\n       [534, 2007],\n       [529, 2015],\n       [532, 2017],\n       [530, 2023],\n       [551, 2035],\n       [548, 2044],\n       [552, 2046],\n       [555, 2040],\n       [570, 2048],\n       [564, 2058],\n       [575, 2064],\n       [573, 2068],\n       [573, 2074],\n       [577, 2077],\n       [575, 2081],\n       [585, 2086],\n       [593, 2072],\n       [598, 2074],\n       [600, 2071],\n       [603, 2073],\n       [593, 2090],\n       [594, 2091],\n       [593, 2093],\n       [599, 2096],\n       [601, 2090],\n       [610, 2095],\n       [609, 2103],\n       [602, 2113],\n       [730, 2191],\n       [731, 2198],\n       [727, 2204],\n       [728, 2208],\n       [724, 2206],\n       [720, 2208],\n       [705, 2207],\n       [776, 2272],\n       [761, 2299],\n       [841, 2345]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '045',\n     'FIPS': '42045',\n     'STATE': 'PA',\n     'NAME': 'Delaware',\n     'LSAD': 'County'},\n    'id': 2186,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1190, 4074],\n       [1191, 4070],\n       [1190, 4065],\n       [1194, 4044],\n       [1203, 4035],\n       [1211, 4018],\n       [1212, 4012],\n       [1226, 3989],\n       [1250, 3969],\n       [1255, 3956],\n       [1256, 3931],\n       [1270, 3914],\n       [1280, 3899],\n       [1287, 3891],\n       [1290, 3885],\n       [1291, 3876],\n       [1288, 3865],\n       [1272, 3842],\n       [1269, 3841],\n       [1262, 3841],\n       [1254, 3845],\n       [1233, 3844],\n       [1226, 3840],\n       [1223, 3832],\n       [1224, 3825],\n       [1241, 3807],\n       [1243, 3803],\n       [1243, 3797],\n       [1234, 3780],\n       [1225, 3773],\n       [1219, 3764],\n       [1219, 3749],\n       [1207, 3732],\n       [1203, 3730],\n       [1195, 3729],\n       [1186, 3724],\n       [1174, 3703],\n       [1170, 3699],\n       [1148, 3701],\n       [1127, 3708],\n       [1118, 3708],\n       [1115, 3707],\n       [1110, 3697],\n       [1107, 3681],\n       [1105, 3676],\n       [1099, 3671],\n       [1086, 3664],\n       [1079, 3656],\n       [1081, 3645],\n       [1095, 3629],\n       [1100, 3618],\n       [1089, 3592],\n       [1082, 3584],\n       [1068, 3541],\n       [1073, 3529],\n       [1079, 3521],\n       [1088, 3518],\n       [1097, 3519],\n       [1102, 3518],\n       [1107, 3514],\n       [1108, 3511],\n       [1107, 3505],\n       [1099, 3492],\n       [1087, 3484],\n       [1073, 3460],\n       [1073, 3455],\n       [1085, 3444],\n       [1091, 3412],\n       [1088, 3404],\n       [1077, 3402],\n       [1071, 3398],\n       [1072, 3393],\n       [1079, 3382],\n       [992, 3332],\n       [977, 3322],\n       [938, 3292],\n       [880, 3244],\n       [777, 3328],\n       [777, 3330],\n       [795, 3359],\n       [799, 3393],\n       [813, 3394],\n       [811, 3398],\n       [808, 3398],\n       [806, 3404],\n       [807, 3411],\n       [800, 3418],\n       [801, 3429],\n       [795, 3430],\n       [783, 3442],\n       [739, 3497],\n       [739, 3508],\n       [664, 3472],\n       [661, 3481],\n       [661, 3494],\n       [659, 3502],\n       [653, 3511],\n       [643, 3515],\n       [640, 3517],\n       [633, 3537],\n       [622, 3548],\n       [605, 3582],\n       [600, 3595],\n       [599, 3618],\n       [598, 3623],\n       [593, 3627],\n       [583, 3630],\n       [578, 3626],\n       [576, 3611],\n       [571, 3604],\n       [558, 3598],\n       [552, 3597],\n       [543, 3602],\n       [538, 3607],\n       [533, 3621],\n       [528, 3625],\n       [509, 3622],\n       [502, 3627],\n       [496, 3641],\n       [489, 3653],\n       [486, 3666],\n       [487, 3684],\n       [482, 3694],\n       [485, 3703],\n       [484, 3712],\n       [478, 3725],\n       [488, 3726],\n       [494, 3730],\n       [498, 3737],\n       [524, 3748],\n       [553, 3752],\n       [562, 3758],\n       [577, 3763],\n       [596, 3762],\n       [609, 3766],\n       [625, 3767],\n       [668, 3775],\n       [675, 3778],\n       [721, 3785],\n       [753, 3795],\n       [758, 3800],\n       [807, 3809],\n       [868, 3837],\n       [898, 3857],\n       [928, 3864],\n       [952, 3877],\n       [967, 3890],\n       [976, 3908],\n       [1027, 3941],\n       [1014, 3960],\n       [1042, 3980],\n       [1048, 3996],\n       [1081, 4012],\n       [1092, 4014],\n       [1106, 4020],\n       [1108, 4030],\n       [1119, 4035],\n       [1143, 4037],\n       [1154, 4041],\n       [1177, 4056],\n       [1190, 4074]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '095',\n     'FIPS': '42095',\n     'STATE': 'PA',\n     'NAME': 'Northampton',\n     'LSAD': 'County'},\n    'id': 2208,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1978, 4176],\n       [1938, 4131],\n       [1910, 4096],\n       [1889, 4068],\n       [1888, 4056],\n       [1897, 4040],\n       [1897, 4029],\n       [1889, 4014],\n       [1873, 4001],\n       [1871, 3998],\n       [1870, 3992],\n       [1872, 3987],\n       [1871, 3984],\n       [1855, 3977],\n       [1846, 3969],\n       [1832, 3960],\n       [1827, 3961],\n       [1820, 3959],\n       [1818, 3952],\n       [1814, 3951],\n       [1808, 3954],\n       [1796, 3946],\n       [1792, 3945],\n       [1786, 3940],\n       [1782, 3942],\n       [1785, 3951],\n       [1783, 3958],\n       [1769, 3970],\n       [1761, 3974],\n       [1756, 3982],\n       [1749, 3979],\n       [1734, 3980],\n       [1727, 3976],\n       [1727, 3972],\n       [1722, 3968],\n       [1715, 3968],\n       [1713, 3965],\n       [1711, 3968],\n       [1708, 3965],\n       [1706, 3966],\n       [1703, 3965],\n       [1594, 4096],\n       [1570, 4125],\n       [1529, 4176],\n       [1978, 4176]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '037',\n     'FIPS': '34037',\n     'STATE': 'NJ',\n     'NAME': 'Sussex',\n     'LSAD': 'County'},\n    'id': 2225,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1526, 3726],\n       [1592, 3670],\n       [1594, 3665],\n       [1617, 3642],\n       [1622, 3639],\n       [1633, 3639],\n       [1638, 3638],\n       [1638, 3636],\n       [1644, 3636],\n       [1659, 3638],\n       [1666, 3634],\n       [1704, 3625],\n       [1725, 3617],\n       [1740, 3607],\n       [1740, 3603],\n       [1755, 3595],\n       [1767, 3595],\n       [1768, 3590],\n       [1773, 3586],\n       [1772, 3582],\n       [1774, 3569],\n       [1777, 3567],\n       [1778, 3565],\n       [1776, 3564],\n       [1779, 3558],\n       [1778, 3555],\n       [1772, 3550],\n       [1771, 3541],\n       [1768, 3533],\n       [1776, 3526],\n       [1775, 3519],\n       [1771, 3515],\n       [1769, 3500],\n       [1764, 3494],\n       [1757, 3491],\n       [1762, 3478],\n       [1769, 3474],\n       [1774, 3465],\n       [1768, 3455],\n       [1767, 3450],\n       [1770, 3446],\n       [1769, 3440],\n       [1762, 3434],\n       [1765, 3426],\n       [1774, 3425],\n       [1778, 3426],\n       [1783, 3424],\n       [1786, 3418],\n       [1786, 3413],\n       [1797, 3406],\n       [1786, 3387],\n       [1783, 3389],\n       [1771, 3366],\n       [1775, 3364],\n       [1766, 3344],\n       [1783, 3335],\n       [1766, 3300],\n       [1758, 3295],\n       [1751, 3286],\n       [1740, 3277],\n       [1693, 3182],\n       [1690, 3180],\n       [1685, 3182],\n       [1678, 3190],\n       [1678, 3193],\n       [1674, 3197],\n       [1660, 3203],\n       [1732, 3028],\n       [1644, 3014],\n       [1654, 2953],\n       [1555, 2940],\n       [1575, 2880],\n       [1464, 2866],\n       [1448, 2871],\n       [1447, 2872],\n       [1444, 2901],\n       [1440, 2915],\n       [1433, 2936],\n       [1416, 2977],\n       [1410, 2982],\n       [1390, 2991],\n       [1381, 2999],\n       [1371, 3002],\n       [1360, 2997],\n       [1340, 2991],\n       [1329, 2989],\n       [1324, 2990],\n       [1312, 2995],\n       [1298, 3009],\n       [1284, 3013],\n       [1280, 3017],\n       [1276, 3026],\n       [1274, 3046],\n       [1267, 3075],\n       [1263, 3088],\n       [1267, 3106],\n       [1267, 3122],\n       [1272, 3129],\n       [1275, 3138],\n       [1275, 3147],\n       [1269, 3194],\n       [1271, 3224],\n       [1269, 3244],\n       [1265, 3254],\n       [1251, 3266],\n       [1219, 3304],\n       [1195, 3314],\n       [1166, 3319],\n       [1159, 3318],\n       [1151, 3314],\n       [1135, 3299],\n       [1128, 3296],\n       [1120, 3296],\n       [1110, 3298],\n       [1099, 3303],\n       [1093, 3307],\n       [1083, 3320],\n       [1081, 3326],\n       [1082, 3331],\n       [1087, 3340],\n       [1088, 3349],\n       [1096, 3351],\n       [1102, 3355],\n       [1104, 3361],\n       [1111, 3363],\n       [1114, 3375],\n       [1120, 3378],\n       [1124, 3385],\n       [1128, 3387],\n       [1130, 3393],\n       [1129, 3399],\n       [1141, 3407],\n       [1147, 3405],\n       [1150, 3408],\n       [1150, 3416],\n       [1153, 3420],\n       [1168, 3424],\n       [1180, 3431],\n       [1205, 3440],\n       [1208, 3442],\n       [1207, 3447],\n       [1211, 3451],\n       [1226, 3458],\n       [1235, 3468],\n       [1236, 3472],\n       [1247, 3476],\n       [1254, 3488],\n       [1264, 3494],\n       [1267, 3497],\n       [1275, 3499],\n       [1278, 3505],\n       [1282, 3510],\n       [1288, 3512],\n       [1295, 3518],\n       [1301, 3516],\n       [1303, 3517],\n       [1308, 3524],\n       [1310, 3526],\n       [1315, 3526],\n       [1333, 3542],\n       [1337, 3542],\n       [1341, 3547],\n       [1345, 3543],\n       [1351, 3547],\n       [1356, 3544],\n       [1361, 3545],\n       [1378, 3563],\n       [1393, 3569],\n       [1396, 3578],\n       [1410, 3577],\n       [1419, 3597],\n       [1424, 3602],\n       [1426, 3606],\n       [1435, 3610],\n       [1443, 3624],\n       [1451, 3631],\n       [1452, 3636],\n       [1465, 3651],\n       [1467, 3656],\n       [1470, 3658],\n       [1473, 3663],\n       [1485, 3672],\n       [1489, 3677],\n       [1489, 3680],\n       [1494, 3686],\n       [1507, 3695],\n       [1506, 3698],\n       [1507, 3706],\n       [1505, 3712],\n       [1517, 3719],\n       [1526, 3726]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '019',\n     'FIPS': '34019',\n     'STATE': 'NJ',\n     'NAME': 'Hunterdon',\n     'LSAD': 'County'},\n    'id': 2209,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2012, 3669],\n       [2018, 3663],\n       [2019, 3655],\n       [2017, 3652],\n       [2023, 3646],\n       [2022, 3622],\n       [2026, 3623],\n       [2028, 3621],\n       [2035, 3622],\n       [2043, 3616],\n       [2046, 3617],\n       [2052, 3611],\n       [2048, 3598],\n       [2048, 3595],\n       [2051, 3594],\n       [2052, 3591],\n       [2058, 3587],\n       [2057, 3581],\n       [2054, 3578],\n       [2056, 3575],\n       [2061, 3574],\n       [2068, 3568],\n       [2069, 3561],\n       [2071, 3561],\n       [2073, 3559],\n       [2073, 3547],\n       [2070, 3544],\n       [2067, 3545],\n       [2066, 3541],\n       [2069, 3539],\n       [2069, 3537],\n       [2062, 3535],\n       [2058, 3537],\n       [2055, 3529],\n       [2053, 3527],\n       [2052, 3521],\n       [2056, 3501],\n       [2051, 3498],\n       [2050, 3495],\n       [2043, 3494],\n       [2044, 3491],\n       [2050, 3490],\n       [2051, 3486],\n       [2056, 3482],\n       [2054, 3479],\n       [2056, 3469],\n       [2059, 3469],\n       [2055, 3465],\n       [2060, 3464],\n       [2061, 3461],\n       [2070, 3462],\n       [2071, 3463],\n       [2075, 3460],\n       [2082, 3466],\n       [2086, 3475],\n       [2085, 3477],\n       [2088, 3480],\n       [2091, 3478],\n       [2095, 3479],\n       [2094, 3483],\n       [2100, 3484],\n       [2102, 3482],\n       [2106, 3484],\n       [2106, 3487],\n       [2112, 3488],\n       [2114, 3485],\n       [2117, 3487],\n       [2116, 3491],\n       [2128, 3493],\n       [2133, 3498],\n       [2137, 3496],\n       [2149, 3503],\n       [2150, 3506],\n       [2164, 3469],\n       [2175, 3475],\n       [2185, 3486],\n       [2196, 3491],\n       [2198, 3487],\n       [2202, 3485],\n       [2208, 3477],\n       [2211, 3478],\n       [2214, 3483],\n       [2220, 3485],\n       [2224, 3491],\n       [2231, 3495],\n       [2234, 3493],\n       [2236, 3487],\n       [2235, 3481],\n       [2235, 3473],\n       [2229, 3462],\n       [2229, 3455],\n       [2226, 3447],\n       [2220, 3446],\n       [2219, 3444],\n       [2217, 3429],\n       [2213, 3418],\n       [2206, 3414],\n       [2205, 3405],\n       [2199, 3397],\n       [2193, 3396],\n       [2184, 3385],\n       [2173, 3383],\n       [2169, 3381],\n       [2164, 3373],\n       [2159, 3373],\n       [2152, 3365],\n       [2143, 3362],\n       [2137, 3362],\n       [2126, 3354],\n       [2118, 3345],\n       [2108, 3340],\n       [2104, 3341],\n       [2097, 3338],\n       [2094, 3340],\n       [2091, 3337],\n       [2091, 3334],\n       [2086, 3339],\n       [2076, 3331],\n       [2070, 3329],\n       [2071, 3326],\n       [2063, 3315],\n       [2059, 3307],\n       [2056, 3305],\n       [2057, 3292],\n       [2054, 3288],\n       [2059, 3286],\n       [2065, 3280],\n       [2068, 3273],\n       [2072, 3255],\n       [2083, 3244],\n       [2100, 3230],\n       [2119, 3197],\n       [2122, 3195],\n       [2141, 3190],\n       [2143, 3178],\n       [2151, 3171],\n       [2153, 3166],\n       [2152, 3156],\n       [2140, 3148],\n       [2125, 3143],\n       [2119, 3139],\n       [2102, 3115],\n       [2082, 3099],\n       [2075, 3091],\n       [2064, 3080],\n       [2055, 3068],\n       [2041, 3056],\n       [2013, 3040],\n       [1971, 3018],\n       [1968, 3014],\n       [1969, 2989],\n       [1966, 2981],\n       [1966, 2971],\n       [1960, 2967],\n       [1955, 2956],\n       [1944, 2945],\n       [1928, 2936],\n       [1919, 2933],\n       [1922, 2940],\n       [1921, 2944],\n       [1915, 2952],\n       [1870, 2965],\n       [1839, 2952],\n       [1791, 2943],\n       [1770, 2935],\n       [1660, 3203],\n       [1674, 3197],\n       [1678, 3193],\n       [1678, 3190],\n       [1685, 3182],\n       [1692, 3180],\n       [1740, 3277],\n       [1751, 3286],\n       [1758, 3295],\n       [1766, 3300],\n       [1783, 3335],\n       [1766, 3344],\n       [1775, 3364],\n       [1771, 3366],\n       [1783, 3389],\n       [1786, 3387],\n       [1797, 3406],\n       [1786, 3413],\n       [1786, 3418],\n       [1783, 3424],\n       [1778, 3426],\n       [1774, 3425],\n       [1765, 3426],\n       [1762, 3434],\n       [1769, 3440],\n       [1770, 3446],\n       [1767, 3450],\n       [1768, 3455],\n       [1774, 3465],\n       [1769, 3474],\n       [1762, 3478],\n       [1757, 3491],\n       [1764, 3494],\n       [1769, 3500],\n       [1771, 3515],\n       [1775, 3519],\n       [1776, 3526],\n       [1768, 3533],\n       [1771, 3541],\n       [1772, 3550],\n       [1778, 3555],\n       [1779, 3558],\n       [1776, 3564],\n       [1778, 3565],\n       [1777, 3567],\n       [1774, 3569],\n       [1772, 3582],\n       [1773, 3586],\n       [1768, 3590],\n       [1767, 3595],\n       [1848, 3621],\n       [2012, 3669]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '035',\n     'FIPS': '34035',\n     'STATE': 'NJ',\n     'NAME': 'Somerset',\n     'LSAD': 'County'},\n    'id': 2211,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1079, 3382],\n       [1085, 3370],\n       [1088, 3347],\n       [1087, 3340],\n       [1082, 3331],\n       [1082, 3324],\n       [1085, 3316],\n       [1099, 3303],\n       [1110, 3298],\n       [1128, 3296],\n       [1135, 3299],\n       [1151, 3314],\n       [1159, 3318],\n       [1166, 3319],\n       [1195, 3314],\n       [1219, 3304],\n       [1251, 3266],\n       [1265, 3254],\n       [1269, 3244],\n       [1271, 3224],\n       [1269, 3194],\n       [1275, 3147],\n       [1275, 3138],\n       [1272, 3129],\n       [1267, 3122],\n       [1267, 3106],\n       [1263, 3088],\n       [1267, 3075],\n       [1274, 3046],\n       [1276, 3026],\n       [1280, 3017],\n       [1284, 3013],\n       [1298, 3009],\n       [1312, 2995],\n       [1324, 2990],\n       [1329, 2989],\n       [1340, 2991],\n       [1360, 2997],\n       [1371, 3002],\n       [1381, 2999],\n       [1390, 2991],\n       [1410, 2982],\n       [1416, 2977],\n       [1433, 2936],\n       [1440, 2915],\n       [1444, 2901],\n       [1447, 2872],\n       [1453, 2864],\n       [1499, 2823],\n       [1506, 2821],\n       [1516, 2820],\n       [1523, 2816],\n       [1539, 2802],\n       [1557, 2782],\n       [1569, 2762],\n       [1595, 2697],\n       [1604, 2688],\n       [1622, 2679],\n       [1628, 2674],\n       [1663, 2656],\n       [1698, 2630],\n       [1714, 2598],\n       [1723, 2572],\n       [1727, 2568],\n       [1738, 2564],\n       [1748, 2558],\n       [1770, 2525],\n       [1771, 2512],\n       [1767, 2499],\n       [1743, 2477],\n       [1735, 2475],\n       [1722, 2476],\n       [1716, 2475],\n       [1701, 2465],\n       [1683, 2449],\n       [1673, 2449],\n       [1656, 2453],\n       [1633, 2462],\n       [1624, 2461],\n       [1619, 2455],\n       [1615, 2448],\n       [1609, 2432],\n       [1605, 2417],\n       [1601, 2411],\n       [1593, 2405],\n       [1577, 2397],\n       [1572, 2388],\n       [1570, 2381],\n       [1564, 2376],\n       [1540, 2369],\n       [1529, 2364],\n       [1498, 2353],\n       [1481, 2355],\n       [1474, 2354],\n       [1446, 2340],\n       [1403, 2312],\n       [1394, 2322],\n       [1389, 2323],\n       [1388, 2329],\n       [1394, 2334],\n       [1394, 2345],\n       [1405, 2356],\n       [1408, 2365],\n       [1420, 2368],\n       [1426, 2380],\n       [1426, 2390],\n       [1429, 2397],\n       [1429, 2402],\n       [1421, 2406],\n       [1422, 2416],\n       [1416, 2425],\n       [1418, 2428],\n       [1419, 2438],\n       [1418, 2443],\n       [1414, 2444],\n       [1409, 2442],\n       [1404, 2448],\n       [1395, 2447],\n       [1390, 2450],\n       [1390, 2454],\n       [1384, 2454],\n       [1379, 2460],\n       [1374, 2460],\n       [1377, 2469],\n       [1375, 2471],\n       [1368, 2463],\n       [1184, 2606],\n       [1065, 2701],\n       [937, 2799],\n       [745, 2952],\n       [660, 3017],\n       [769, 3151],\n       [872, 3239],\n       [880, 3244],\n       [938, 3292],\n       [977, 3322],\n       [992, 3332],\n       [1079, 3382]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '017',\n     'FIPS': '42017',\n     'STATE': 'PA',\n     'NAME': 'Bucks',\n     'LSAD': 'County'},\n    'id': 2197,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1343, 2482],\n       [1368, 2463],\n       [1375, 2471],\n       [1377, 2469],\n       [1374, 2460],\n       [1379, 2460],\n       [1384, 2454],\n       [1390, 2454],\n       [1390, 2450],\n       [1395, 2447],\n       [1404, 2448],\n       [1409, 2442],\n       [1414, 2444],\n       [1418, 2443],\n       [1419, 2438],\n       [1418, 2428],\n       [1416, 2425],\n       [1422, 2416],\n       [1421, 2406],\n       [1429, 2402],\n       [1429, 2397],\n       [1426, 2390],\n       [1426, 2380],\n       [1420, 2368],\n       [1408, 2365],\n       [1405, 2356],\n       [1394, 2345],\n       [1394, 2334],\n       [1388, 2329],\n       [1389, 2323],\n       [1394, 2322],\n       [1403, 2312],\n       [1389, 2301],\n       [1349, 2260],\n       [1308, 2244],\n       [1297, 2236],\n       [1291, 2228],\n       [1279, 2205],\n       [1260, 2183],\n       [1236, 2172],\n       [1208, 2163],\n       [1180, 2146],\n       [1171, 2133],\n       [1168, 2119],\n       [1167, 2094],\n       [1169, 2081],\n       [1179, 2052],\n       [1175, 2030],\n       [1158, 2004],\n       [1154, 2000],\n       [1146, 1997],\n       [1099, 1995],\n       [1090, 1993],\n       [1058, 1964],\n       [1051, 1975],\n       [1051, 1984],\n       [1045, 1983],\n       [1023, 1986],\n       [1015, 1977],\n       [1004, 1985],\n       [983, 1985],\n       [980, 1996],\n       [983, 1996],\n       [980, 2001],\n       [983, 2002],\n       [980, 2007],\n       [983, 2008],\n       [984, 2014],\n       [988, 2015],\n       [992, 2020],\n       [990, 2021],\n       [994, 2027],\n       [993, 2029],\n       [999, 2038],\n       [996, 2040],\n       [1001, 2045],\n       [999, 2049],\n       [1006, 2056],\n       [1005, 2071],\n       [1011, 2082],\n       [1011, 2086],\n       [1009, 2087],\n       [1013, 2093],\n       [1020, 2092],\n       [1023, 2101],\n       [1020, 2102],\n       [1016, 2102],\n       [1016, 2109],\n       [1006, 2112],\n       [1001, 2118],\n       [999, 2128],\n       [1002, 2135],\n       [1003, 2138],\n       [1000, 2142],\n       [1004, 2146],\n       [1004, 2151],\n       [995, 2154],\n       [991, 2151],\n       [986, 2155],\n       [979, 2153],\n       [972, 2160],\n       [964, 2161],\n       [957, 2172],\n       [982, 2187],\n       [993, 2196],\n       [1063, 2239],\n       [1065, 2242],\n       [1056, 2249],\n       [1048, 2259],\n       [1027, 2273],\n       [1018, 2281],\n       [1007, 2292],\n       [1002, 2299],\n       [987, 2313],\n       [980, 2322],\n       [1040, 2396],\n       [1091, 2358],\n       [1108, 2381],\n       [1206, 2307],\n       [1210, 2313],\n       [1228, 2327],\n       [1238, 2340],\n       [1224, 2351],\n       [1239, 2369],\n       [1245, 2377],\n       [1252, 2383],\n       [1280, 2421],\n       [1335, 2471],\n       [1343, 2482]]]},\n    'properties': {'STATE_FIPS': '42',\n     'COUNTY_FIP': '101',\n     'FIPS': '42101',\n     'STATE': 'PA',\n     'NAME': 'Philadelphia',\n     'LSAD': 'County'},\n    'id': 2177,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1732, 3028],\n       [1770, 2935],\n       [1791, 2943],\n       [1839, 2952],\n       [1870, 2965],\n       [1915, 2952],\n       [1921, 2944],\n       [1922, 2940],\n       [1916, 2919],\n       [1912, 2916],\n       [1904, 2878],\n       [1904, 2869],\n       [1914, 2857],\n       [1912, 2847],\n       [1915, 2845],\n       [1915, 2840],\n       [1916, 2838],\n       [1925, 2839],\n       [1936, 2833],\n       [1943, 2832],\n       [1947, 2821],\n       [1953, 2819],\n       [1957, 2820],\n       [1969, 2816],\n       [1972, 2803],\n       [1976, 2799],\n       [1983, 2797],\n       [1991, 2790],\n       [1995, 2791],\n       [1999, 2785],\n       [2007, 2783],\n       [2010, 2776],\n       [2016, 2773],\n       [2021, 2773],\n       [2027, 2775],\n       [2028, 2777],\n       [2032, 2775],\n       [2036, 2769],\n       [2040, 2771],\n       [2045, 2776],\n       [2052, 2780],\n       [2061, 2772],\n       [2061, 2765],\n       [2072, 2766],\n       [2077, 2759],\n       [2086, 2754],\n       [2088, 2754],\n       [2091, 2751],\n       [2095, 2754],\n       [2119, 2741],\n       [2116, 2702],\n       [2112, 2702],\n       [2112, 2700],\n       [2119, 2689],\n       [2121, 2683],\n       [2069, 2652],\n       [2031, 2632],\n       [2025, 2624],\n       [2023, 2615],\n       [2011, 2600],\n       [2002, 2581],\n       [1991, 2573],\n       [1985, 2576],\n       [1968, 2569],\n       [1957, 2562],\n       [1954, 2562],\n       [1952, 2574],\n       [1948, 2575],\n       [1927, 2567],\n       [1966, 2483],\n       [1945, 2480],\n       [1937, 2485],\n       [1938, 2487],\n       [1933, 2485],\n       [1926, 2487],\n       [1924, 2489],\n       [1920, 2487],\n       [1916, 2488],\n       [1914, 2490],\n       [1915, 2495],\n       [1914, 2497],\n       [1913, 2503],\n       [1908, 2504],\n       [1908, 2503],\n       [1904, 2503],\n       [1902, 2506],\n       [1899, 2503],\n       [1897, 2508],\n       [1893, 2509],\n       [1892, 2513],\n       [1887, 2515],\n       [1885, 2513],\n       [1880, 2517],\n       [1878, 2516],\n       [1874, 2521],\n       [1868, 2521],\n       [1867, 2524],\n       [1862, 2528],\n       [1859, 2528],\n       [1857, 2534],\n       [1853, 2533],\n       [1852, 2536],\n       [1849, 2537],\n       [1847, 2537],\n       [1846, 2534],\n       [1845, 2537],\n       [1841, 2538],\n       [1838, 2535],\n       [1835, 2536],\n       [1840, 2543],\n       [1838, 2548],\n       [1840, 2550],\n       [1832, 2550],\n       [1831, 2554],\n       [1824, 2559],\n       [1823, 2553],\n       [1822, 2557],\n       [1816, 2556],\n       [1813, 2561],\n       [1808, 2560],\n       [1810, 2567],\n       [1806, 2567],\n       [1799, 2570],\n       [1796, 2566],\n       [1790, 2566],\n       [1786, 2563],\n       [1788, 2560],\n       [1783, 2554],\n       [1786, 2549],\n       [1790, 2547],\n       [1787, 2539],\n       [1788, 2537],\n       [1796, 2541],\n       [1793, 2527],\n       [1783, 2517],\n       [1782, 2510],\n       [1774, 2501],\n       [1767, 2500],\n       [1771, 2512],\n       [1770, 2525],\n       [1748, 2558],\n       [1738, 2564],\n       [1727, 2568],\n       [1723, 2572],\n       [1714, 2598],\n       [1698, 2630],\n       [1663, 2656],\n       [1628, 2674],\n       [1622, 2679],\n       [1604, 2688],\n       [1595, 2697],\n       [1569, 2762],\n       [1557, 2782],\n       [1539, 2802],\n       [1523, 2816],\n       [1516, 2820],\n       [1506, 2821],\n       [1499, 2823],\n       [1486, 2834],\n       [1453, 2864],\n       [1448, 2871],\n       [1464, 2866],\n       [1575, 2880],\n       [1555, 2940],\n       [1654, 2953],\n       [1644, 3014],\n       [1732, 3028]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '021',\n     'FIPS': '34021',\n     'STATE': 'NJ',\n     'NAME': 'Mercer',\n     'LSAD': 'County'},\n    'id': 2196,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1799, 2570],\n       [1806, 2567],\n       [1810, 2567],\n       [1808, 2560],\n       [1813, 2561],\n       [1816, 2556],\n       [1822, 2557],\n       [1823, 2553],\n       [1824, 2559],\n       [1831, 2554],\n       [1832, 2550],\n       [1840, 2550],\n       [1838, 2548],\n       [1840, 2543],\n       [1835, 2536],\n       [1838, 2535],\n       [1841, 2538],\n       [1845, 2537],\n       [1846, 2534],\n       [1847, 2537],\n       [1849, 2537],\n       [1852, 2536],\n       [1853, 2533],\n       [1857, 2534],\n       [1859, 2528],\n       [1862, 2528],\n       [1867, 2524],\n       [1868, 2521],\n       [1874, 2521],\n       [1878, 2516],\n       [1880, 2517],\n       [1885, 2513],\n       [1887, 2515],\n       [1892, 2513],\n       [1893, 2509],\n       [1897, 2508],\n       [1899, 2503],\n       [1902, 2506],\n       [1904, 2503],\n       [1908, 2503],\n       [1908, 2504],\n       [1913, 2503],\n       [1914, 2497],\n       [1915, 2495],\n       [1914, 2490],\n       [1916, 2488],\n       [1920, 2487],\n       [1924, 2489],\n       [1926, 2487],\n       [1933, 2485],\n       [1938, 2487],\n       [1937, 2485],\n       [1945, 2480],\n       [1966, 2483],\n       [2052, 2289],\n       [2084, 2203],\n       [2167, 1994],\n       [2254, 1789],\n       [2253, 1454],\n       [2250, 1450],\n       [2244, 1432],\n       [2242, 1417],\n       [2238, 1409],\n       [2235, 1408],\n       [2232, 1404],\n       [2226, 1404],\n       [2224, 1406],\n       [2221, 1405],\n       [2216, 1405],\n       [2212, 1402],\n       [2214, 1393],\n       [2219, 1390],\n       [2214, 1380],\n       [2209, 1380],\n       [2201, 1373],\n       [2199, 1373],\n       [2191, 1377],\n       [2187, 1376],\n       [2184, 1374],\n       [2184, 1369],\n       [2190, 1359],\n       [2190, 1353],\n       [2188, 1350],\n       [2182, 1350],\n       [2175, 1357],\n       [2172, 1363],\n       [2176, 1378],\n       [2172, 1383],\n       [2169, 1384],\n       [2164, 1382],\n       [2162, 1378],\n       [2162, 1367],\n       [2158, 1361],\n       [2154, 1362],\n       [2152, 1366],\n       [2150, 1376],\n       [2148, 1379],\n       [2144, 1379],\n       [2139, 1377],\n       [2134, 1371],\n       [2130, 1369],\n       [2126, 1372],\n       [2120, 1382],\n       [2113, 1383],\n       [2108, 1379],\n       [2103, 1379],\n       [2101, 1381],\n       [2099, 1391],\n       [2092, 1395],\n       [2088, 1403],\n       [2080, 1402],\n       [2076, 1404],\n       [2072, 1412],\n       [2067, 1411],\n       [2059, 1403],\n       [2054, 1403],\n       [2051, 1405],\n       [2045, 1411],\n       [2037, 1413],\n       [2034, 1417],\n       [2031, 1425],\n       [2033, 1434],\n       [2022, 1442],\n       [2015, 1457],\n       [2000, 1455],\n       [1992, 1459],\n       [1983, 1458],\n       [1975, 1468],\n       [1969, 1471],\n       [1966, 1481],\n       [1964, 1484],\n       [1954, 1488],\n       [1936, 1502],\n       [1929, 1506],\n       [1926, 1505],\n       [1916, 1512],\n       [1914, 1512],\n       [1911, 1509],\n       [1910, 1503],\n       [1906, 1499],\n       [1902, 1501],\n       [1902, 1507],\n       [1900, 1513],\n       [1896, 1513],\n       [1893, 1505],\n       [1890, 1504],\n       [1889, 1505],\n       [1888, 1510],\n       [1878, 1509],\n       [1878, 1514],\n       [1874, 1515],\n       [1871, 1511],\n       [1868, 1512],\n       [1868, 1518],\n       [1866, 1520],\n       [1861, 1521],\n       [1863, 1530],\n       [1861, 1538],\n       [1865, 1546],\n       [1862, 1550],\n       [1863, 1554],\n       [1863, 1559],\n       [1860, 1560],\n       [1862, 1563],\n       [1861, 1566],\n       [1863, 1569],\n       [1860, 1591],\n       [1854, 1601],\n       [1854, 1606],\n       [1851, 1615],\n       [1846, 1623],\n       [1842, 1625],\n       [1837, 1634],\n       [1718, 1733],\n       [1716, 1731],\n       [1713, 1735],\n       [1706, 1738],\n       [1702, 1745],\n       [1696, 1747],\n       [1693, 1758],\n       [1684, 1761],\n       [1680, 1769],\n       [1681, 1771],\n       [1678, 1775],\n       [1676, 1776],\n       [1669, 1787],\n       [1659, 1794],\n       [1646, 1809],\n       [1632, 1813],\n       [1628, 1816],\n       [1603, 1822],\n       [1591, 1821],\n       [1587, 1819],\n       [1584, 1819],\n       [1579, 1815],\n       [1560, 1811],\n       [1545, 1805],\n       [1526, 1806],\n       [1517, 1810],\n       [1513, 1819],\n       [1506, 1823],\n       [1472, 1980],\n       [1464, 2006],\n       [1451, 2015],\n       [1447, 2016],\n       [1443, 2024],\n       [1439, 2027],\n       [1435, 2032],\n       [1435, 2041],\n       [1430, 2055],\n       [1421, 2065],\n       [1418, 2069],\n       [1402, 2070],\n       [1392, 2080],\n       [1389, 2081],\n       [1387, 2086],\n       [1388, 2089],\n       [1393, 2089],\n       [1393, 2094],\n       [1397, 2095],\n       [1402, 2102],\n       [1400, 2105],\n       [1392, 2106],\n       [1385, 2105],\n       [1381, 2107],\n       [1376, 2106],\n       [1368, 2106],\n       [1364, 2109],\n       [1359, 2108],\n       [1352, 2115],\n       [1345, 2116],\n       [1344, 2118],\n       [1342, 2125],\n       [1346, 2132],\n       [1346, 2137],\n       [1340, 2142],\n       [1340, 2149],\n       [1338, 2150],\n       [1337, 2161],\n       [1342, 2163],\n       [1342, 2167],\n       [1340, 2170],\n       [1343, 2172],\n       [1343, 2175],\n       [1351, 2181],\n       [1345, 2182],\n       [1341, 2189],\n       [1344, 2192],\n       [1340, 2191],\n       [1338, 2199],\n       [1336, 2201],\n       [1335, 2198],\n       [1328, 2202],\n       [1325, 2207],\n       [1324, 2204],\n       [1321, 2204],\n       [1314, 2212],\n       [1305, 2210],\n       [1303, 2207],\n       [1300, 2209],\n       [1296, 2208],\n       [1293, 2203],\n       [1278, 2204],\n       [1291, 2228],\n       [1297, 2236],\n       [1308, 2244],\n       [1349, 2260],\n       [1389, 2301],\n       [1402, 2312],\n       [1446, 2340],\n       [1474, 2354],\n       [1481, 2355],\n       [1498, 2353],\n       [1529, 2364],\n       [1540, 2369],\n       [1564, 2376],\n       [1568, 2379],\n       [1574, 2393],\n       [1577, 2397],\n       [1593, 2405],\n       [1601, 2411],\n       [1605, 2417],\n       [1609, 2432],\n       [1615, 2448],\n       [1619, 2455],\n       [1624, 2461],\n       [1633, 2462],\n       [1656, 2453],\n       [1678, 2448],\n       [1683, 2449],\n       [1701, 2465],\n       [1716, 2475],\n       [1722, 2476],\n       [1735, 2475],\n       [1743, 2477],\n       [1767, 2500],\n       [1774, 2501],\n       [1782, 2510],\n       [1783, 2517],\n       [1793, 2527],\n       [1796, 2541],\n       [1788, 2537],\n       [1787, 2539],\n       [1790, 2547],\n       [1786, 2549],\n       [1783, 2554],\n       [1788, 2560],\n       [1786, 2563],\n       [1790, 2566],\n       [1796, 2566],\n       [1799, 2570]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '005',\n     'FIPS': '34005',\n     'STATE': 'NJ',\n     'NAME': 'Burlington',\n     'LSAD': 'County'},\n    'id': 2178,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[217, 1692],\n       [217, 1571],\n       [219, 1532],\n       [249, 1041],\n       [240, 1045],\n       [235, 1044],\n       [235, 1047],\n       [223, 1051],\n       [222, 1046],\n       [216, 1048],\n       [213, 1046],\n       [209, 1046],\n       [206, 1041],\n       [198, 1042],\n       [193, 1037],\n       [191, 1038],\n       [186, 1046],\n       [176, 1047],\n       [173, 1050],\n       [166, 1049],\n       [155, 1035],\n       [139, 1029],\n       [134, 1023],\n       [130, 1024],\n       [120, 1017],\n       [111, 1023],\n       [107, 1021],\n       [102, 1023],\n       [81, 1017],\n       [76, 1010],\n       [71, 1012],\n       [63, 1019],\n       [49, 1017],\n       [21, 1022],\n       [14, 1030],\n       [8, 1030],\n       [0, 1025],\n       [-5, 1023],\n       [-11, 1025],\n       [-17, 1032],\n       [-21, 1034],\n       [-39, 1036],\n       [-48, 1033],\n       [-56, 1022],\n       [-60, 1021],\n       [-64, 1021],\n       [-75, 1034],\n       [-80, 1037],\n       [-80, 1144],\n       [-66, 1150],\n       [-57, 1168],\n       [-57, 1174],\n       [-76, 1194],\n       [-80, 1194],\n       [-80, 1260],\n       [-71, 1292],\n       [-56, 1329],\n       [-43, 1345],\n       [-43, 1367],\n       [-48, 1381],\n       [-80, 1391],\n       [-80, 1691],\n       [0, 1691],\n       [217, 1692]]]},\n    'properties': {'STATE_FIPS': '24',\n     'COUNTY_FIP': '015',\n     'FIPS': '24015',\n     'STATE': 'MD',\n     'NAME': 'Cecil',\n     'LSAD': 'County'},\n    'id': 2129,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1272, 1811],\n       [1283, 1808],\n       [1285, 1809],\n       [1287, 1804],\n       [1294, 1799],\n       [1292, 1795],\n       [1294, 1786],\n       [1300, 1777],\n       [1303, 1772],\n       [1302, 1767],\n       [1308, 1764],\n       [1313, 1759],\n       [1315, 1754],\n       [1333, 1738],\n       [1334, 1735],\n       [1332, 1727],\n       [1335, 1722],\n       [1334, 1714],\n       [1331, 1708],\n       [1373, 1681],\n       [1374, 1670],\n       [1381, 1667],\n       [1384, 1663],\n       [1397, 1657],\n       [1407, 1655],\n       [1413, 1656],\n       [1417, 1655],\n       [1425, 1656],\n       [1434, 1649],\n       [1447, 1647],\n       [1461, 1637],\n       [1463, 1634],\n       [1466, 1633],\n       [1467, 1629],\n       [1472, 1626],\n       [1472, 1621],\n       [1476, 1618],\n       [1480, 1611],\n       [1484, 1611],\n       [1486, 1600],\n       [1498, 1585],\n       [1496, 1582],\n       [1498, 1574],\n       [1497, 1569],\n       [1499, 1567],\n       [1496, 1561],\n       [1498, 1559],\n       [1501, 1539],\n       [1507, 1522],\n       [1522, 1508],\n       [1521, 1502],\n       [1523, 1495],\n       [1536, 1486],\n       [1537, 1482],\n       [1544, 1477],\n       [1387, 1300],\n       [1075, 1588],\n       [1044, 1578],\n       [1039, 1581],\n       [1026, 1578],\n       [1004, 1584],\n       [1000, 1592],\n       [999, 1606],\n       [996, 1610],\n       [991, 1611],\n       [984, 1616],\n       [972, 1617],\n       [963, 1613],\n       [961, 1615],\n       [948, 1612],\n       [938, 1621],\n       [918, 1631],\n       [914, 1630],\n       [912, 1627],\n       [907, 1629],\n       [909, 1632],\n       [907, 1634],\n       [898, 1634],\n       [895, 1631],\n       [890, 1631],\n       [889, 1639],\n       [891, 1643],\n       [884, 1644],\n       [883, 1648],\n       [878, 1651],\n       [876, 1658],\n       [870, 1660],\n       [868, 1659],\n       [859, 1664],\n       [856, 1671],\n       [840, 1675],\n       [839, 1682],\n       [836, 1681],\n       [831, 1684],\n       [832, 1689],\n       [831, 1692],\n       [828, 1692],\n       [823, 1697],\n       [822, 1696],\n       [817, 1700],\n       [817, 1704],\n       [821, 1717],\n       [816, 1725],\n       [819, 1727],\n       [815, 1727],\n       [817, 1729],\n       [815, 1732],\n       [817, 1735],\n       [812, 1742],\n       [803, 1739],\n       [801, 1742],\n       [802, 1747],\n       [800, 1746],\n       [800, 1749],\n       [796, 1749],\n       [796, 1745],\n       [793, 1747],\n       [794, 1749],\n       [789, 1753],\n       [791, 1756],\n       [794, 1754],\n       [797, 1756],\n       [797, 1759],\n       [794, 1760],\n       [798, 1765],\n       [788, 1767],\n       [787, 1762],\n       [785, 1759],\n       [783, 1762],\n       [784, 1766],\n       [778, 1766],\n       [774, 1771],\n       [776, 1782],\n       [779, 1783],\n       [782, 1779],\n       [785, 1781],\n       [790, 1790],\n       [788, 1791],\n       [785, 1788],\n       [783, 1789],\n       [781, 1794],\n       [782, 1799],\n       [779, 1807],\n       [777, 1807],\n       [775, 1803],\n       [771, 1804],\n       [774, 1810],\n       [772, 1816],\n       [769, 1817],\n       [764, 1812],\n       [757, 1813],\n       [748, 1809],\n       [734, 1808],\n       [723, 1803],\n       [728, 1808],\n       [775, 1833],\n       [761, 1843],\n       [797, 1869],\n       [849, 1916],\n       [868, 1927],\n       [884, 1933],\n       [895, 1934],\n       [914, 1935],\n       [938, 1932],\n       [970, 1934],\n       [1011, 1943],\n       [1043, 1956],\n       [1059, 1965],\n       [1090, 1993],\n       [1099, 1995],\n       [1146, 1997],\n       [1154, 2000],\n       [1161, 2008],\n       [1177, 1998],\n       [1180, 1993],\n       [1173, 1989],\n       [1174, 1987],\n       [1190, 1978],\n       [1184, 1976],\n       [1187, 1971],\n       [1188, 1965],\n       [1192, 1959],\n       [1195, 1960],\n       [1193, 1968],\n       [1201, 1959],\n       [1196, 1959],\n       [1201, 1952],\n       [1208, 1953],\n       [1205, 1956],\n       [1209, 1960],\n       [1211, 1960],\n       [1213, 1957],\n       [1211, 1955],\n       [1213, 1943],\n       [1216, 1942],\n       [1215, 1947],\n       [1228, 1938],\n       [1229, 1931],\n       [1235, 1930],\n       [1238, 1924],\n       [1241, 1924],\n       [1244, 1919],\n       [1239, 1912],\n       [1241, 1910],\n       [1242, 1906],\n       [1245, 1903],\n       [1242, 1898],\n       [1245, 1891],\n       [1241, 1887],\n       [1241, 1884],\n       [1237, 1879],\n       [1240, 1870],\n       [1236, 1868],\n       [1238, 1866],\n       [1242, 1865],\n       [1243, 1861],\n       [1248, 1855],\n       [1247, 1853],\n       [1256, 1849],\n       [1259, 1845],\n       [1255, 1839],\n       [1255, 1836],\n       [1257, 1835],\n       [1259, 1827],\n       [1249, 1820],\n       [1246, 1823],\n       [1241, 1813],\n       [1239, 1801],\n       [1252, 1804],\n       [1257, 1790],\n       [1264, 1795],\n       [1261, 1802],\n       [1269, 1806],\n       [1268, 1807],\n       [1272, 1811]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '015',\n     'FIPS': '34015',\n     'STATE': 'NJ',\n     'NAME': 'Gloucester',\n     'LSAD': 'County'},\n    'id': 2180,\n    'type': 'Feature'},\n   {'geometry': {'type': 'MultiPolygon',\n     'coordinates': [[[[535, 1914],\n        [580, 1912],\n        [610, 1909],\n        [639, 1903],\n        [691, 1885],\n        [742, 1857],\n        [775, 1833],\n        [728, 1808],\n        [712, 1791],\n        [714, 1789],\n        [707, 1781],\n        [696, 1775],\n        [691, 1766],\n        [691, 1761],\n        [682, 1733],\n        [675, 1718],\n        [670, 1697],\n        [670, 1679],\n        [668, 1677],\n        [659, 1680],\n        [654, 1678],\n        [643, 1653],\n        [627, 1645],\n        [624, 1640],\n        [623, 1624],\n        [594, 1636],\n        [547, 1568],\n        [510, 1558],\n        [472, 1501],\n        [470, 1491],\n        [472, 1486],\n        [471, 1473],\n        [474, 1456],\n        [485, 1442],\n        [509, 1425],\n        [511, 1421],\n        [545, 1390],\n        [542, 1383],\n        [536, 1349],\n        [535, 1324],\n        [549, 1311],\n        [540, 1288],\n        [526, 1290],\n        [509, 1265],\n        [502, 1233],\n        [507, 1198],\n        [520, 1180],\n        [522, 1174],\n        [534, 1164],\n        [533, 1157],\n        [556, 1141],\n        [581, 1115],\n        [585, 1101],\n        [603, 1068],\n        [619, 1019],\n        [614, 1017],\n        [613, 1010],\n        [606, 1006],\n        [603, 1008],\n        [597, 1006],\n        [595, 1002],\n        [597, 998],\n        [595, 996],\n        [591, 998],\n        [587, 996],\n        [587, 992],\n        [585, 991],\n        [580, 993],\n        [579, 997],\n        [577, 998],\n        [573, 992],\n        [570, 991],\n        [567, 992],\n        [567, 995],\n        [565, 997],\n        [562, 997],\n        [558, 993],\n        [559, 989],\n        [562, 989],\n        [563, 981],\n        [558, 981],\n        [554, 988],\n        [549, 980],\n        [550, 977],\n        [553, 976],\n        [552, 969],\n        [555, 967],\n        [555, 962],\n        [551, 958],\n        [543, 956],\n        [537, 950],\n        [535, 945],\n        [537, 940],\n        [528, 921],\n        [519, 916],\n        [514, 912],\n        [510, 914],\n        [510, 916],\n        [504, 917],\n        [500, 911],\n        [496, 914],\n        [489, 911],\n        [486, 916],\n        [482, 911],\n        [474, 912],\n        [471, 911],\n        [468, 914],\n        [463, 914],\n        [458, 911],\n        [454, 903],\n        [451, 904],\n        [445, 898],\n        [430, 897],\n        [430, 894],\n        [422, 888],\n        [417, 880],\n        [403, 880],\n        [396, 878],\n        [391, 879],\n        [383, 878],\n        [375, 881],\n        [368, 890],\n        [324, 894],\n        [258, 889],\n        [219, 1532],\n        [217, 1571],\n        [217, 1692],\n        [239, 1693],\n        [250, 1722],\n        [269, 1760],\n        [293, 1794],\n        [321, 1825],\n        [344, 1845],\n        [366, 1861],\n        [400, 1880],\n        [431, 1894],\n        [466, 1904],\n        [501, 1911],\n        [535, 1914]]],\n      [[[551, 1518],\n        [550, 1507],\n        [554, 1484],\n        [553, 1479],\n        [556, 1472],\n        [547, 1471],\n        [539, 1487],\n        [533, 1506],\n        [534, 1512],\n        [551, 1518]]],\n      [[[521, 1459],\n        [541, 1443],\n        [543, 1430],\n        [526, 1439],\n        [523, 1445],\n        [521, 1459]]]]},\n    'properties': {'STATE_FIPS': '10',\n     'COUNTY_FIP': '003',\n     'FIPS': '10003',\n     'STATE': 'DE',\n     'NAME': 'New Castle',\n     'LSAD': 'County'},\n    'id': 2130,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[276, 601],\n       [314, 0],\n       [314, -1],\n       [319, -80],\n       [-15, -80],\n       [0, -66],\n       [7, -49],\n       [14, -44],\n       [38, -30],\n       [49, -29],\n       [55, -20],\n       [60, -19],\n       [59, -14],\n       [50, -11],\n       [45, -5],\n       [47, -2],\n       [51, 0],\n       [53, 4],\n       [36, 13],\n       [33, 23],\n       [26, 24],\n       [14, 30],\n       [14, 35],\n       [26, 42],\n       [27, 44],\n       [26, 45],\n       [16, 46],\n       [10, 53],\n       [22, 62],\n       [22, 65],\n       [19, 71],\n       [11, 72],\n       [4, 84],\n       [0, 83],\n       [-3, 82],\n       [-6, 85],\n       [-10, 98],\n       [-9, 101],\n       [-2, 101],\n       [0, 104],\n       [0, 107],\n       [-3, 110],\n       [-16, 104],\n       [-20, 108],\n       [-20, 115],\n       [-18, 116],\n       [-6, 115],\n       [-4, 117],\n       [-10, 126],\n       [-16, 129],\n       [-16, 130],\n       [-12, 133],\n       [-10, 140],\n       [-11, 145],\n       [-16, 149],\n       [-15, 153],\n       [-9, 156],\n       [-8, 158],\n       [-16, 167],\n       [-16, 172],\n       [-9, 172],\n       [-10, 176],\n       [-17, 179],\n       [-18, 190],\n       [-22, 200],\n       [-22, 201],\n       [-17, 202],\n       [-16, 204],\n       [-15, 210],\n       [-17, 213],\n       [-21, 214],\n       [-23, 217],\n       [-14, 220],\n       [-14, 222],\n       [-19, 222],\n       [-20, 231],\n       [-10, 234],\n       [-7, 238],\n       [-7, 250],\n       [-4, 254],\n       [-2, 253],\n       [-1, 257],\n       [-8, 270],\n       [0, 278],\n       [1, 283],\n       [0, 284],\n       [-3, 285],\n       [-5, 289],\n       [0, 292],\n       [4, 295],\n       [4, 299],\n       [7, 307],\n       [13, 311],\n       [11, 315],\n       [23, 322],\n       [26, 332],\n       [33, 335],\n       [35, 339],\n       [39, 340],\n       [42, 348],\n       [42, 352],\n       [49, 352],\n       [49, 354],\n       [47, 361],\n       [49, 364],\n       [57, 359],\n       [68, 366],\n       [68, 374],\n       [71, 377],\n       [73, 382],\n       [80, 384],\n       [78, 387],\n       [79, 389],\n       [79, 393],\n       [82, 396],\n       [84, 404],\n       [87, 405],\n       [89, 414],\n       [88, 417],\n       [86, 418],\n       [86, 421],\n       [89, 422],\n       [96, 432],\n       [99, 440],\n       [108, 443],\n       [112, 447],\n       [116, 452],\n       [129, 466],\n       [132, 473],\n       [130, 495],\n       [133, 500],\n       [137, 502],\n       [143, 520],\n       [149, 535],\n       [153, 536],\n       [167, 548],\n       [186, 558],\n       [197, 566],\n       [202, 573],\n       [209, 575],\n       [213, 581],\n       [236, 584],\n       [262, 592],\n       [276, 601]]]},\n    'properties': {'STATE_FIPS': '24',\n     'COUNTY_FIP': '011',\n     'FIPS': '24011',\n     'STATE': 'MD',\n     'NAME': 'Caroline',\n     'LSAD': 'County'},\n    'id': 2136,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[619, 1019],\n       [629, 1007],\n       [646, 998],\n       [649, 993],\n       [646, 983],\n       [651, 976],\n       [682, 953],\n       [695, 949],\n       [726, 921],\n       [729, 913],\n       [731, 889],\n       [742, 867],\n       [771, 829],\n       [778, 810],\n       [774, 789],\n       [774, 752],\n       [787, 738],\n       [793, 716],\n       [790, 685],\n       [783, 682],\n       [771, 660],\n       [767, 625],\n       [781, 498],\n       [780, 457],\n       [789, 443],\n       [788, 441],\n       [800, 428],\n       [812, 424],\n       [862, 379],\n       [869, 369],\n       [902, 310],\n       [907, 296],\n       [912, 271],\n       [910, 240],\n       [911, 231],\n       [908, 233],\n       [909, 239],\n       [904, 244],\n       [900, 245],\n       [897, 245],\n       [893, 238],\n       [887, 237],\n       [884, 239],\n       [881, 240],\n       [877, 233],\n       [872, 231],\n       [865, 234],\n       [867, 241],\n       [864, 248],\n       [863, 248],\n       [855, 234],\n       [852, 234],\n       [841, 253],\n       [832, 246],\n       [829, 253],\n       [830, 256],\n       [823, 260],\n       [817, 258],\n       [810, 260],\n       [808, 249],\n       [799, 247],\n       [796, 251],\n       [793, 250],\n       [792, 244],\n       [785, 238],\n       [781, 233],\n       [785, 221],\n       [783, 218],\n       [779, 215],\n       [783, 211],\n       [786, 213],\n       [781, 200],\n       [784, 195],\n       [783, 193],\n       [780, 192],\n       [778, 187],\n       [781, 180],\n       [779, 178],\n       [776, 177],\n       [775, 181],\n       [770, 182],\n       [770, 176],\n       [759, 179],\n       [757, 171],\n       [753, 174],\n       [751, 171],\n       [741, 169],\n       [732, 165],\n       [724, 169],\n       [716, 169],\n       [695, 162],\n       [688, 166],\n       [670, 156],\n       [659, 152],\n       [654, 143],\n       [655, 131],\n       [651, 125],\n       [645, 122],\n       [642, 113],\n       [640, 111],\n       [635, 109],\n       [632, 110],\n       [625, 105],\n       [607, 62],\n       [584, 51],\n       [585, 47],\n       [580, 39],\n       [577, 38],\n       [573, 39],\n       [566, 35],\n       [557, 24],\n       [313, 14],\n       [258, 889],\n       [324, 894],\n       [368, 890],\n       [375, 881],\n       [383, 878],\n       [391, 879],\n       [396, 878],\n       [403, 880],\n       [417, 880],\n       [422, 888],\n       [430, 894],\n       [430, 897],\n       [445, 898],\n       [451, 904],\n       [454, 903],\n       [458, 911],\n       [463, 914],\n       [468, 914],\n       [471, 911],\n       [474, 912],\n       [482, 911],\n       [486, 916],\n       [489, 911],\n       [496, 914],\n       [500, 911],\n       [504, 917],\n       [510, 916],\n       [510, 914],\n       [514, 912],\n       [519, 916],\n       [528, 921],\n       [537, 940],\n       [535, 945],\n       [537, 950],\n       [543, 956],\n       [551, 958],\n       [555, 962],\n       [555, 967],\n       [552, 969],\n       [553, 976],\n       [550, 977],\n       [549, 980],\n       [554, 988],\n       [558, 981],\n       [563, 981],\n       [562, 989],\n       [559, 989],\n       [558, 993],\n       [562, 997],\n       [565, 997],\n       [567, 995],\n       [567, 992],\n       [570, 991],\n       [573, 992],\n       [577, 998],\n       [579, 997],\n       [580, 993],\n       [585, 991],\n       [587, 992],\n       [587, 996],\n       [591, 998],\n       [595, 996],\n       [597, 998],\n       [595, 1002],\n       [597, 1006],\n       [603, 1008],\n       [606, 1006],\n       [613, 1010],\n       [614, 1017],\n       [619, 1019]]]},\n    'properties': {'STATE_FIPS': '10',\n     'COUNTY_FIP': '001',\n     'FIPS': '10001',\n     'STATE': 'DE',\n     'NAME': 'Kent',\n     'LSAD': 'County'},\n    'id': 2138,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1272, 1811],\n       [1268, 1807],\n       [1269, 1806],\n       [1261, 1802],\n       [1264, 1795],\n       [1257, 1790],\n       [1252, 1804],\n       [1239, 1801],\n       [1241, 1813],\n       [1246, 1823],\n       [1249, 1820],\n       [1259, 1827],\n       [1257, 1835],\n       [1255, 1836],\n       [1255, 1839],\n       [1259, 1845],\n       [1256, 1849],\n       [1247, 1853],\n       [1248, 1855],\n       [1243, 1861],\n       [1242, 1865],\n       [1238, 1866],\n       [1236, 1868],\n       [1240, 1870],\n       [1237, 1879],\n       [1241, 1884],\n       [1241, 1887],\n       [1245, 1891],\n       [1242, 1898],\n       [1245, 1903],\n       [1242, 1906],\n       [1241, 1910],\n       [1239, 1912],\n       [1244, 1919],\n       [1241, 1924],\n       [1238, 1924],\n       [1235, 1930],\n       [1229, 1931],\n       [1228, 1938],\n       [1215, 1947],\n       [1216, 1942],\n       [1213, 1943],\n       [1211, 1955],\n       [1213, 1957],\n       [1211, 1960],\n       [1209, 1960],\n       [1205, 1956],\n       [1208, 1953],\n       [1201, 1952],\n       [1196, 1959],\n       [1201, 1959],\n       [1193, 1968],\n       [1195, 1960],\n       [1192, 1959],\n       [1188, 1965],\n       [1187, 1971],\n       [1184, 1976],\n       [1190, 1978],\n       [1174, 1987],\n       [1173, 1989],\n       [1180, 1993],\n       [1177, 1998],\n       [1161, 2008],\n       [1175, 2030],\n       [1179, 2052],\n       [1169, 2081],\n       [1167, 2094],\n       [1168, 2119],\n       [1171, 2133],\n       [1180, 2146],\n       [1208, 2163],\n       [1236, 2172],\n       [1260, 2183],\n       [1278, 2204],\n       [1293, 2203],\n       [1296, 2208],\n       [1300, 2209],\n       [1303, 2207],\n       [1305, 2210],\n       [1314, 2212],\n       [1321, 2204],\n       [1324, 2204],\n       [1325, 2207],\n       [1328, 2202],\n       [1335, 2198],\n       [1336, 2201],\n       [1338, 2199],\n       [1340, 2191],\n       [1344, 2192],\n       [1341, 2189],\n       [1345, 2182],\n       [1351, 2181],\n       [1343, 2175],\n       [1343, 2172],\n       [1340, 2170],\n       [1342, 2167],\n       [1342, 2163],\n       [1337, 2161],\n       [1338, 2150],\n       [1340, 2149],\n       [1340, 2142],\n       [1346, 2137],\n       [1346, 2132],\n       [1342, 2125],\n       [1344, 2118],\n       [1345, 2116],\n       [1352, 2115],\n       [1359, 2108],\n       [1364, 2109],\n       [1368, 2106],\n       [1376, 2106],\n       [1381, 2107],\n       [1385, 2105],\n       [1392, 2106],\n       [1400, 2105],\n       [1402, 2102],\n       [1397, 2095],\n       [1393, 2094],\n       [1393, 2089],\n       [1388, 2089],\n       [1387, 2086],\n       [1389, 2081],\n       [1392, 2080],\n       [1402, 2070],\n       [1418, 2069],\n       [1421, 2065],\n       [1430, 2055],\n       [1435, 2041],\n       [1435, 2032],\n       [1439, 2027],\n       [1443, 2024],\n       [1447, 2016],\n       [1451, 2015],\n       [1464, 2006],\n       [1472, 1980],\n       [1506, 1823],\n       [1513, 1819],\n       [1517, 1810],\n       [1526, 1806],\n       [1545, 1805],\n       [1560, 1811],\n       [1579, 1815],\n       [1584, 1819],\n       [1587, 1819],\n       [1591, 1821],\n       [1603, 1822],\n       [1628, 1816],\n       [1632, 1813],\n       [1646, 1809],\n       [1659, 1794],\n       [1669, 1787],\n       [1676, 1776],\n       [1678, 1775],\n       [1681, 1771],\n       [1680, 1769],\n       [1684, 1761],\n       [1693, 1758],\n       [1696, 1747],\n       [1702, 1745],\n       [1706, 1738],\n       [1713, 1735],\n       [1716, 1731],\n       [1718, 1733],\n       [1749, 1707],\n       [1544, 1477],\n       [1537, 1482],\n       [1536, 1486],\n       [1523, 1495],\n       [1521, 1502],\n       [1522, 1508],\n       [1507, 1522],\n       [1501, 1539],\n       [1498, 1559],\n       [1496, 1561],\n       [1499, 1567],\n       [1497, 1569],\n       [1498, 1574],\n       [1496, 1582],\n       [1498, 1585],\n       [1486, 1600],\n       [1484, 1611],\n       [1480, 1611],\n       [1476, 1618],\n       [1472, 1621],\n       [1472, 1626],\n       [1467, 1629],\n       [1466, 1633],\n       [1463, 1634],\n       [1461, 1637],\n       [1447, 1647],\n       [1434, 1649],\n       [1425, 1656],\n       [1417, 1655],\n       [1413, 1656],\n       [1407, 1655],\n       [1397, 1657],\n       [1384, 1663],\n       [1381, 1667],\n       [1374, 1670],\n       [1373, 1681],\n       [1331, 1708],\n       [1334, 1714],\n       [1335, 1722],\n       [1332, 1727],\n       [1334, 1735],\n       [1333, 1738],\n       [1315, 1754],\n       [1313, 1759],\n       [1308, 1764],\n       [1302, 1767],\n       [1303, 1772],\n       [1300, 1777],\n       [1294, 1786],\n       [1292, 1795],\n       [1294, 1799],\n       [1287, 1804],\n       [1285, 1809],\n       [1283, 1808],\n       [1272, 1811]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '007',\n     'FIPS': '34007',\n     'STATE': 'NJ',\n     'NAME': 'Camden',\n     'LSAD': 'County'},\n    'id': 2179,\n    'type': 'Feature'},\n   {'geometry': {'type': 'MultiPolygon',\n     'coordinates': [[[[763, 1054],\n        [751, 1058],\n        [736, 1068],\n        [721, 1088],\n        [688, 1157],\n        [672, 1156],\n        [661, 1161],\n        [629, 1184],\n        [625, 1195],\n        [584, 1198],\n        [575, 1217],\n        [573, 1234],\n        [575, 1266],\n        [596, 1269],\n        [598, 1273],\n        [594, 1275],\n        [593, 1283],\n        [593, 1293],\n        [598, 1323],\n        [598, 1332],\n        [597, 1339],\n        [591, 1342],\n        [588, 1349],\n        [590, 1354],\n        [609, 1377],\n        [616, 1391],\n        [620, 1400],\n        [619, 1420],\n        [615, 1425],\n        [609, 1430],\n        [600, 1431],\n        [583, 1448],\n        [571, 1455],\n        [559, 1465],\n        [553, 1479],\n        [554, 1484],\n        [550, 1507],\n        [551, 1518],\n        [555, 1527],\n        [568, 1538],\n        [576, 1548],\n        [586, 1551],\n        [616, 1591],\n        [620, 1602],\n        [624, 1623],\n        [624, 1640],\n        [627, 1645],\n        [643, 1653],\n        [654, 1678],\n        [659, 1680],\n        [668, 1677],\n        [670, 1679],\n        [670, 1697],\n        [675, 1718],\n        [682, 1733],\n        [691, 1761],\n        [691, 1766],\n        [696, 1775],\n        [707, 1781],\n        [714, 1789],\n        [712, 1791],\n        [723, 1803],\n        [734, 1808],\n        [748, 1809],\n        [757, 1813],\n        [764, 1812],\n        [769, 1817],\n        [772, 1816],\n        [774, 1810],\n        [771, 1804],\n        [775, 1803],\n        [777, 1807],\n        [779, 1807],\n        [782, 1799],\n        [781, 1794],\n        [783, 1789],\n        [785, 1788],\n        [788, 1791],\n        [790, 1790],\n        [785, 1781],\n        [782, 1779],\n        [779, 1783],\n        [776, 1782],\n        [774, 1771],\n        [778, 1766],\n        [784, 1766],\n        [783, 1762],\n        [785, 1759],\n        [787, 1762],\n        [788, 1767],\n        [798, 1765],\n        [794, 1760],\n        [797, 1759],\n        [797, 1756],\n        [794, 1754],\n        [791, 1756],\n        [789, 1753],\n        [794, 1749],\n        [793, 1747],\n        [796, 1745],\n        [796, 1749],\n        [800, 1749],\n        [800, 1746],\n        [802, 1747],\n        [801, 1742],\n        [803, 1739],\n        [812, 1742],\n        [817, 1735],\n        [815, 1732],\n        [817, 1729],\n        [815, 1727],\n        [819, 1727],\n        [816, 1725],\n        [821, 1717],\n        [817, 1704],\n        [817, 1700],\n        [822, 1696],\n        [823, 1697],\n        [828, 1692],\n        [831, 1692],\n        [832, 1689],\n        [831, 1684],\n        [836, 1681],\n        [839, 1682],\n        [840, 1675],\n        [856, 1671],\n        [859, 1664],\n        [868, 1659],\n        [870, 1660],\n        [876, 1658],\n        [878, 1651],\n        [883, 1648],\n        [884, 1644],\n        [891, 1643],\n        [889, 1639],\n        [890, 1631],\n        [895, 1631],\n        [898, 1634],\n        [907, 1634],\n        [909, 1632],\n        [907, 1629],\n        [912, 1627],\n        [914, 1630],\n        [918, 1631],\n        [938, 1621],\n        [948, 1612],\n        [961, 1615],\n        [963, 1613],\n        [972, 1617],\n        [984, 1616],\n        [991, 1611],\n        [996, 1610],\n        [999, 1606],\n        [1000, 1592],\n        [1004, 1584],\n        [1026, 1578],\n        [1039, 1581],\n        [1044, 1578],\n        [1075, 1588],\n        [1275, 1402],\n        [1269, 1387],\n        [1268, 1378],\n        [1264, 1373],\n        [1257, 1357],\n        [1262, 1348],\n        [1260, 1340],\n        [1260, 1334],\n        [1262, 1334],\n        [1258, 1324],\n        [1260, 1318],\n        [1256, 1309],\n        [1256, 1303],\n        [1253, 1299],\n        [1252, 1291],\n        [1253, 1289],\n        [1252, 1283],\n        [1253, 1282],\n        [1249, 1275],\n        [1251, 1267],\n        [1254, 1264],\n        [1257, 1257],\n        [1256, 1251],\n        [1259, 1245],\n        [1257, 1239],\n        [1254, 1237],\n        [1253, 1229],\n        [1251, 1229],\n        [1252, 1227],\n        [1250, 1224],\n        [1251, 1222],\n        [1248, 1219],\n        [1247, 1215],\n        [1249, 1212],\n        [1247, 1211],\n        [1246, 1206],\n        [1248, 1202],\n        [1243, 1192],\n        [1178, 1249],\n        [1016, 1384],\n        [1003, 1367],\n        [902, 1257],\n        [891, 1251],\n        [867, 1255],\n        [855, 1230],\n        [854, 1219],\n        [851, 1216],\n        [845, 1216],\n        [844, 1212],\n        [836, 1205],\n        [836, 1201],\n        [829, 1191],\n        [820, 1188],\n        [813, 1192],\n        [813, 1194],\n        [802, 1193],\n        [783, 1201],\n        [771, 1194],\n        [761, 1182],\n        [762, 1179],\n        [768, 1178],\n        [766, 1169],\n        [769, 1165],\n        [765, 1159],\n        [760, 1160],\n        [759, 1156],\n        [762, 1153],\n        [768, 1157],\n        [769, 1153],\n        [766, 1147],\n        [764, 1147],\n        [760, 1149],\n        [755, 1145],\n        [757, 1140],\n        [762, 1138],\n        [765, 1142],\n        [768, 1141],\n        [768, 1135],\n        [771, 1131],\n        [770, 1129],\n        [766, 1128],\n        [765, 1126],\n        [766, 1122],\n        [761, 1121],\n        [762, 1114],\n        [756, 1113],\n        [755, 1105],\n        [760, 1100],\n        [762, 1093],\n        [764, 1092],\n        [766, 1094],\n        [767, 1100],\n        [775, 1098],\n        [775, 1094],\n        [773, 1091],\n        [767, 1086],\n        [767, 1084],\n        [772, 1080],\n        [772, 1076],\n        [769, 1074],\n        [763, 1074],\n        [758, 1076],\n        [757, 1071],\n        [761, 1066],\n        [758, 1062],\n        [758, 1060],\n        [763, 1054]]],\n      [[[763, 1054],\n        [765, 1055],\n        [767, 1060],\n        [770, 1060],\n        [772, 1050],\n        [763, 1054]]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '033',\n     'FIPS': '34033',\n     'STATE': 'NJ',\n     'NAME': 'Salem',\n     'LSAD': 'County'},\n    'id': 2131,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1275, 1402],\n       [1575, 1130],\n       [1573, 1127],\n       [1574, 1123],\n       [1579, 1112],\n       [1580, 1106],\n       [1577, 1095],\n       [1572, 1086],\n       [1573, 1080],\n       [1572, 1075],\n       [1580, 1055],\n       [1581, 1041],\n       [1579, 1031],\n       [1569, 1012],\n       [1570, 1000],\n       [1564, 981],\n       [1564, 974],\n       [1562, 968],\n       [1563, 948],\n       [1568, 940],\n       [1563, 941],\n       [1557, 929],\n       [1549, 919],\n       [1537, 911],\n       [1515, 773],\n       [1490, 757],\n       [1479, 754],\n       [1476, 743],\n       [1474, 742],\n       [1476, 740],\n       [1474, 739],\n       [1473, 735],\n       [1475, 732],\n       [1474, 730],\n       [1476, 727],\n       [1473, 727],\n       [1473, 724],\n       [1477, 722],\n       [1476, 719],\n       [1479, 717],\n       [1474, 710],\n       [1473, 701],\n       [1475, 698],\n       [1483, 700],\n       [1484, 697],\n       [1478, 691],\n       [1478, 686],\n       [1480, 685],\n       [1481, 687],\n       [1491, 685],\n       [1488, 679],\n       [1490, 673],\n       [1493, 672],\n       [1492, 669],\n       [1493, 667],\n       [1489, 665],\n       [1420, 689],\n       [1400, 693],\n       [1368, 691],\n       [1327, 695],\n       [1323, 697],\n       [1325, 706],\n       [1331, 712],\n       [1331, 716],\n       [1327, 726],\n       [1313, 736],\n       [1305, 737],\n       [1296, 729],\n       [1289, 733],\n       [1274, 733],\n       [1240, 723],\n       [1218, 729],\n       [1209, 729],\n       [1198, 722],\n       [1180, 688],\n       [1166, 669],\n       [1163, 670],\n       [1124, 711],\n       [1125, 739],\n       [1117, 772],\n       [1107, 788],\n       [1099, 795],\n       [1086, 793],\n       [1079, 813],\n       [1066, 825],\n       [1056, 825],\n       [1013, 847],\n       [1010, 853],\n       [1012, 859],\n       [1009, 869],\n       [999, 895],\n       [970, 903],\n       [954, 894],\n       [950, 881],\n       [945, 876],\n       [920, 899],\n       [906, 915],\n       [889, 957],\n       [888, 969],\n       [879, 981],\n       [867, 987],\n       [848, 985],\n       [834, 973],\n       [797, 1005],\n       [791, 1015],\n       [790, 1030],\n       [784, 1045],\n       [772, 1050],\n       [770, 1060],\n       [767, 1060],\n       [765, 1055],\n       [763, 1054],\n       [758, 1060],\n       [758, 1062],\n       [761, 1066],\n       [757, 1071],\n       [757, 1075],\n       [758, 1076],\n       [763, 1074],\n       [769, 1074],\n       [772, 1076],\n       [772, 1080],\n       [767, 1084],\n       [767, 1086],\n       [773, 1091],\n       [775, 1094],\n       [775, 1098],\n       [767, 1100],\n       [766, 1094],\n       [764, 1092],\n       [762, 1093],\n       [760, 1100],\n       [755, 1103],\n       [754, 1109],\n       [756, 1113],\n       [762, 1114],\n       [761, 1121],\n       [766, 1122],\n       [765, 1126],\n       [766, 1128],\n       [770, 1129],\n       [771, 1131],\n       [768, 1135],\n       [768, 1141],\n       [765, 1142],\n       [762, 1138],\n       [757, 1140],\n       [755, 1145],\n       [760, 1149],\n       [764, 1147],\n       [766, 1147],\n       [769, 1153],\n       [768, 1157],\n       [762, 1153],\n       [759, 1156],\n       [760, 1160],\n       [765, 1159],\n       [769, 1165],\n       [766, 1169],\n       [768, 1178],\n       [762, 1179],\n       [761, 1182],\n       [771, 1194],\n       [783, 1201],\n       [802, 1193],\n       [813, 1194],\n       [813, 1192],\n       [820, 1188],\n       [829, 1191],\n       [836, 1201],\n       [836, 1205],\n       [844, 1212],\n       [845, 1216],\n       [851, 1216],\n       [854, 1219],\n       [855, 1230],\n       [867, 1255],\n       [891, 1251],\n       [902, 1257],\n       [1003, 1367],\n       [1016, 1384],\n       [1178, 1249],\n       [1243, 1192],\n       [1248, 1202],\n       [1246, 1206],\n       [1247, 1211],\n       [1249, 1212],\n       [1247, 1215],\n       [1248, 1219],\n       [1251, 1222],\n       [1250, 1224],\n       [1252, 1227],\n       [1251, 1229],\n       [1253, 1229],\n       [1254, 1237],\n       [1257, 1239],\n       [1259, 1245],\n       [1256, 1251],\n       [1257, 1257],\n       [1254, 1264],\n       [1251, 1267],\n       [1249, 1275],\n       [1253, 1282],\n       [1252, 1283],\n       [1253, 1289],\n       [1252, 1291],\n       [1253, 1299],\n       [1256, 1303],\n       [1256, 1309],\n       [1260, 1318],\n       [1258, 1324],\n       [1262, 1334],\n       [1260, 1334],\n       [1260, 1340],\n       [1262, 1348],\n       [1257, 1357],\n       [1264, 1373],\n       [1268, 1378],\n       [1269, 1387],\n       [1275, 1402]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '011',\n     'FIPS': '34011',\n     'STATE': 'NJ',\n     'NAME': 'Cumberland',\n     'LSAD': 'County'},\n    'id': 2133,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1563, 941],\n       [1568, 940],\n       [1568, 932],\n       [1576, 930],\n       [1582, 926],\n       [1602, 924],\n       [1611, 916],\n       [1620, 905],\n       [1627, 909],\n       [1629, 907],\n       [1643, 907],\n       [1646, 904],\n       [1651, 909],\n       [1656, 911],\n       [1659, 908],\n       [1664, 908],\n       [1666, 905],\n       [1671, 909],\n       [1676, 908],\n       [1679, 909],\n       [1689, 902],\n       [1687, 894],\n       [1688, 894],\n       [1696, 897],\n       [1699, 894],\n       [1706, 893],\n       [1709, 889],\n       [1720, 894],\n       [1725, 891],\n       [1726, 886],\n       [1735, 885],\n       [1744, 888],\n       [1748, 893],\n       [1758, 892],\n       [1764, 888],\n       [1773, 893],\n       [1775, 892],\n       [1773, 886],\n       [1778, 881],\n       [1782, 880],\n       [1785, 886],\n       [1788, 886],\n       [1789, 880],\n       [1791, 878],\n       [1794, 884],\n       [1798, 885],\n       [1802, 878],\n       [1805, 875],\n       [1810, 879],\n       [1816, 878],\n       [1820, 883],\n       [1824, 885],\n       [1828, 883],\n       [1831, 884],\n       [1835, 890],\n       [1837, 890],\n       [1840, 887],\n       [1840, 883],\n       [1838, 877],\n       [1840, 875],\n       [1843, 876],\n       [1845, 881],\n       [1845, 889],\n       [1849, 891],\n       [1864, 890],\n       [1862, 879],\n       [1863, 877],\n       [1870, 874],\n       [1875, 875],\n       [1879, 882],\n       [1885, 887],\n       [1889, 888],\n       [1900, 886],\n       [1913, 881],\n       [1918, 882],\n       [1960, 913],\n       [1967, 914],\n       [1994, 894],\n       [2011, 893],\n       [2023, 900],\n       [2031, 899],\n       [2033, 896],\n       [2019, 883],\n       [2016, 871],\n       [2005, 855],\n       [1976, 841],\n       [1951, 818],\n       [1927, 791],\n       [1895, 747],\n       [1880, 730],\n       [1873, 705],\n       [1844, 669],\n       [1781, 557],\n       [1780, 551],\n       [1782, 547],\n       [1796, 534],\n       [1794, 525],\n       [1746, 472],\n       [1688, 375],\n       [1676, 332],\n       [1667, 317],\n       [1645, 306],\n       [1628, 293],\n       [1583, 247],\n       [1563, 220],\n       [1562, 222],\n       [1554, 226],\n       [1537, 223],\n       [1501, 205],\n       [1481, 199],\n       [1462, 198],\n       [1419, 203],\n       [1413, 207],\n       [1406, 220],\n       [1430, 334],\n       [1439, 361],\n       [1455, 398],\n       [1506, 496],\n       [1514, 517],\n       [1522, 544],\n       [1531, 601],\n       [1530, 630],\n       [1503, 660],\n       [1489, 665],\n       [1493, 667],\n       [1492, 669],\n       [1493, 672],\n       [1490, 673],\n       [1488, 679],\n       [1491, 685],\n       [1481, 687],\n       [1480, 685],\n       [1478, 686],\n       [1478, 691],\n       [1484, 697],\n       [1482, 701],\n       [1475, 698],\n       [1473, 701],\n       [1474, 710],\n       [1479, 717],\n       [1476, 719],\n       [1477, 722],\n       [1473, 724],\n       [1473, 727],\n       [1476, 727],\n       [1474, 730],\n       [1475, 732],\n       [1473, 735],\n       [1474, 739],\n       [1476, 740],\n       [1474, 742],\n       [1476, 743],\n       [1479, 754],\n       [1490, 757],\n       [1515, 773],\n       [1537, 911],\n       [1549, 919],\n       [1557, 929],\n       [1563, 941]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '009',\n     'FIPS': '34009',\n     'STATE': 'NJ',\n     'NAME': 'Cape May',\n     'LSAD': 'County'},\n    'id': 2134,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2885, 4176],\n       [2960, 4130],\n       [2976, 4129],\n       [2972, 4096],\n       [2941, 3976],\n       [2925, 3922],\n       [2889, 3842],\n       [2875, 3801],\n       [2844, 3745],\n       [2831, 3752],\n       [2832, 3760],\n       [2803, 3780],\n       [2803, 3784],\n       [2806, 3789],\n       [2802, 3794],\n       [2797, 3795],\n       [2792, 3789],\n       [2792, 3785],\n       [2788, 3785],\n       [2787, 3781],\n       [2785, 3780],\n       [2786, 3778],\n       [2781, 3781],\n       [2780, 3774],\n       [2775, 3774],\n       [2769, 3765],\n       [2742, 3763],\n       [2733, 3760],\n       [2727, 3755],\n       [2722, 3747],\n       [2718, 3744],\n       [2710, 3742],\n       [2708, 3736],\n       [2692, 3711],\n       [2690, 3703],\n       [2693, 3687],\n       [2690, 3677],\n       [2686, 3679],\n       [2683, 3677],\n       [2677, 3687],\n       [2679, 3692],\n       [2675, 3703],\n       [2672, 3702],\n       [2669, 3705],\n       [2662, 3706],\n       [2657, 3703],\n       [2653, 3694],\n       [2650, 3692],\n       [2640, 3696],\n       [2636, 3695],\n       [2633, 3699],\n       [2632, 3707],\n       [2607, 3723],\n       [2619, 3744],\n       [2620, 3749],\n       [2618, 3760],\n       [2623, 3784],\n       [2638, 3790],\n       [2641, 3793],\n       [2643, 3799],\n       [2645, 3808],\n       [2641, 3826],\n       [2644, 3833],\n       [2648, 3849],\n       [2647, 3855],\n       [2649, 3864],\n       [2656, 3864],\n       [2659, 3857],\n       [2661, 3856],\n       [2665, 3860],\n       [2666, 3864],\n       [2665, 3871],\n       [2661, 3874],\n       [2656, 3885],\n       [2654, 3892],\n       [2646, 3904],\n       [2632, 3915],\n       [2634, 3930],\n       [2627, 3941],\n       [2630, 3956],\n       [2628, 3967],\n       [2633, 3981],\n       [2630, 3985],\n       [2619, 3987],\n       [2616, 3992],\n       [2615, 3999],\n       [2619, 4007],\n       [2612, 4020],\n       [2606, 4022],\n       [2616, 4070],\n       [2583, 4096],\n       [2574, 4104],\n       [2573, 4096],\n       [2573, 4090],\n       [2518, 4096],\n       [2494, 4099],\n       [2460, 4120],\n       [2437, 4158],\n       [2425, 4170],\n       [2427, 4176],\n       [2885, 4176]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '003',\n     'FIPS': '34003',\n     'STATE': 'NJ',\n     'NAME': 'Bergen',\n     'LSAD': 'County'},\n    'id': 2229,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2234, 4176],\n       [2235, 4173],\n       [2240, 4168],\n       [2251, 4165],\n       [2255, 4159],\n       [2259, 4157],\n       [2265, 4162],\n       [2274, 4161],\n       [2281, 4157],\n       [2292, 4157],\n       [2293, 4160],\n       [2298, 4162],\n       [2306, 4160],\n       [2310, 4156],\n       [2310, 4144],\n       [2318, 4144],\n       [2324, 4148],\n       [2325, 4146],\n       [2329, 4148],\n       [2334, 4147],\n       [2334, 4142],\n       [2341, 4140],\n       [2344, 4137],\n       [2363, 4139],\n       [2370, 4132],\n       [2382, 4133],\n       [2384, 4140],\n       [2387, 4140],\n       [2388, 4136],\n       [2388, 4128],\n       [2391, 4125],\n       [2391, 4117],\n       [2396, 4111],\n       [2389, 4103],\n       [2389, 4096],\n       [2392, 4094],\n       [2399, 4096],\n       [2407, 4094],\n       [2412, 4096],\n       [2412, 4090],\n       [2418, 4085],\n       [2417, 4079],\n       [2413, 4081],\n       [2412, 4075],\n       [2413, 4071],\n       [2415, 4069],\n       [2414, 4053],\n       [2418, 4047],\n       [2415, 4025],\n       [2407, 4002],\n       [2401, 3997],\n       [2402, 3995],\n       [2402, 3989],\n       [2407, 3986],\n       [2405, 3984],\n       [2408, 3980],\n       [2408, 3975],\n       [2416, 3978],\n       [2416, 3973],\n       [2426, 3972],\n       [2433, 3959],\n       [2432, 3957],\n       [2430, 3957],\n       [2426, 3962],\n       [2426, 3956],\n       [2424, 3953],\n       [2427, 3948],\n       [2424, 3942],\n       [2425, 3938],\n       [2427, 3936],\n       [2413, 3937],\n       [2411, 3934],\n       [2407, 3933],\n       [2408, 3938],\n       [2404, 3940],\n       [2411, 3942],\n       [2411, 3944],\n       [2406, 3945],\n       [2400, 3941],\n       [2397, 3942],\n       [2396, 3944],\n       [2397, 3945],\n       [2396, 3948],\n       [2397, 3953],\n       [2390, 3954],\n       [2380, 3950],\n       [2381, 3956],\n       [2376, 3952],\n       [2370, 3951],\n       [2367, 3952],\n       [2364, 3949],\n       [2358, 3948],\n       [2358, 3957],\n       [2355, 3957],\n       [2353, 3955],\n       [2351, 3956],\n       [2352, 3959],\n       [2347, 3958],\n       [2345, 3956],\n       [2348, 3956],\n       [2348, 3953],\n       [2344, 3953],\n       [2342, 3959],\n       [2340, 3955],\n       [2335, 3952],\n       [2330, 3939],\n       [2335, 3937],\n       [2335, 3935],\n       [2331, 3935],\n       [2328, 3931],\n       [2333, 3929],\n       [2328, 3927],\n       [2327, 3923],\n       [2329, 3918],\n       [2326, 3902],\n       [2330, 3902],\n       [2330, 3900],\n       [2334, 3898],\n       [2330, 3894],\n       [2335, 3891],\n       [2330, 3888],\n       [2332, 3885],\n       [2329, 3884],\n       [2331, 3878],\n       [2337, 3883],\n       [2339, 3878],\n       [2347, 3877],\n       [2353, 3873],\n       [2356, 3876],\n       [2359, 3874],\n       [2358, 3871],\n       [2354, 3869],\n       [2357, 3861],\n       [2349, 3861],\n       [2347, 3858],\n       [2348, 3854],\n       [2346, 3851],\n       [2345, 3847],\n       [2341, 3844],\n       [2343, 3835],\n       [2347, 3834],\n       [2345, 3830],\n       [2342, 3829],\n       [2341, 3816],\n       [2343, 3811],\n       [2334, 3803],\n       [2338, 3792],\n       [2335, 3785],\n       [2327, 3774],\n       [2320, 3776],\n       [2313, 3774],\n       [2311, 3763],\n       [2303, 3757],\n       [2298, 3749],\n       [2291, 3748],\n       [2291, 3742],\n       [2288, 3739],\n       [2287, 3711],\n       [2279, 3701],\n       [2282, 3700],\n       [2284, 3694],\n       [2280, 3687],\n       [2283, 3686],\n       [2282, 3680],\n       [2278, 3678],\n       [2274, 3678],\n       [2272, 3675],\n       [2275, 3674],\n       [2277, 3669],\n       [2287, 3675],\n       [2286, 3669],\n       [2291, 3668],\n       [2295, 3665],\n       [2301, 3665],\n       [2301, 3662],\n       [2298, 3655],\n       [2287, 3652],\n       [2286, 3650],\n       [2291, 3646],\n       [2288, 3642],\n       [2279, 3637],\n       [2282, 3626],\n       [2280, 3622],\n       [2276, 3622],\n       [2272, 3623],\n       [2268, 3621],\n       [2272, 3615],\n       [2262, 3604],\n       [2258, 3607],\n       [2253, 3606],\n       [2253, 3592],\n       [2246, 3590],\n       [2236, 3592],\n       [2228, 3583],\n       [2227, 3577],\n       [2222, 3577],\n       [2217, 3568],\n       [2200, 3560],\n       [2197, 3555],\n       [2194, 3545],\n       [2189, 3545],\n       [2184, 3542],\n       [2181, 3537],\n       [2169, 3529],\n       [2164, 3529],\n       [2162, 3524],\n       [2164, 3518],\n       [2162, 3514],\n       [2163, 3512],\n       [2160, 3507],\n       [2154, 3508],\n       [2150, 3506],\n       [2149, 3503],\n       [2137, 3496],\n       [2133, 3498],\n       [2128, 3493],\n       [2116, 3491],\n       [2117, 3487],\n       [2114, 3485],\n       [2112, 3488],\n       [2106, 3487],\n       [2106, 3484],\n       [2102, 3482],\n       [2100, 3484],\n       [2094, 3483],\n       [2095, 3479],\n       [2091, 3478],\n       [2088, 3480],\n       [2085, 3477],\n       [2086, 3475],\n       [2082, 3466],\n       [2075, 3460],\n       [2071, 3463],\n       [2070, 3462],\n       [2061, 3461],\n       [2060, 3464],\n       [2055, 3465],\n       [2059, 3469],\n       [2056, 3469],\n       [2054, 3479],\n       [2056, 3482],\n       [2051, 3486],\n       [2050, 3490],\n       [2044, 3491],\n       [2043, 3494],\n       [2050, 3495],\n       [2051, 3498],\n       [2056, 3501],\n       [2052, 3521],\n       [2053, 3527],\n       [2055, 3529],\n       [2058, 3537],\n       [2062, 3535],\n       [2069, 3537],\n       [2069, 3539],\n       [2066, 3541],\n       [2067, 3545],\n       [2070, 3544],\n       [2073, 3547],\n       [2073, 3559],\n       [2071, 3561],\n       [2069, 3561],\n       [2068, 3568],\n       [2061, 3574],\n       [2056, 3575],\n       [2054, 3578],\n       [2057, 3581],\n       [2058, 3587],\n       [2052, 3591],\n       [2051, 3594],\n       [2048, 3595],\n       [2048, 3598],\n       [2052, 3611],\n       [2046, 3617],\n       [2043, 3616],\n       [2035, 3622],\n       [2028, 3621],\n       [2026, 3623],\n       [2022, 3622],\n       [2023, 3646],\n       [2017, 3652],\n       [2019, 3655],\n       [2018, 3663],\n       [2012, 3669],\n       [1848, 3621],\n       [1765, 3594],\n       [1755, 3595],\n       [1740, 3603],\n       [1740, 3607],\n       [1725, 3617],\n       [1704, 3625],\n       [1666, 3634],\n       [1659, 3638],\n       [1644, 3636],\n       [1638, 3636],\n       [1638, 3638],\n       [1633, 3639],\n       [1622, 3639],\n       [1617, 3642],\n       [1594, 3665],\n       [1592, 3670],\n       [1526, 3726],\n       [1532, 3732],\n       [1535, 3733],\n       [1539, 3731],\n       [1546, 3737],\n       [1558, 3742],\n       [1562, 3755],\n       [1567, 3756],\n       [1569, 3752],\n       [1576, 3756],\n       [1583, 3757],\n       [1588, 3765],\n       [1589, 3771],\n       [1592, 3771],\n       [1598, 3780],\n       [1606, 3786],\n       [1606, 3792],\n       [1613, 3799],\n       [1611, 3804],\n       [1617, 3812],\n       [1630, 3821],\n       [1627, 3829],\n       [1622, 3833],\n       [1625, 3843],\n       [1627, 3845],\n       [1637, 3849],\n       [1642, 3855],\n       [1643, 3865],\n       [1652, 3869],\n       [1654, 3874],\n       [1646, 3880],\n       [1645, 3883],\n       [1648, 3894],\n       [1647, 3908],\n       [1651, 3914],\n       [1668, 3923],\n       [1671, 3928],\n       [1671, 3935],\n       [1681, 3943],\n       [1687, 3954],\n       [1691, 3952],\n       [1706, 3966],\n       [1708, 3965],\n       [1711, 3968],\n       [1713, 3965],\n       [1715, 3968],\n       [1722, 3968],\n       [1727, 3972],\n       [1727, 3976],\n       [1734, 3980],\n       [1749, 3979],\n       [1756, 3982],\n       [1761, 3974],\n       [1769, 3970],\n       [1783, 3958],\n       [1785, 3951],\n       [1782, 3942],\n       [1786, 3940],\n       [1792, 3945],\n       [1796, 3946],\n       [1808, 3954],\n       [1814, 3951],\n       [1818, 3952],\n       [1820, 3959],\n       [1827, 3961],\n       [1832, 3960],\n       [1846, 3969],\n       [1855, 3977],\n       [1871, 3984],\n       [1872, 3987],\n       [1870, 3992],\n       [1871, 3998],\n       [1873, 4001],\n       [1889, 4014],\n       [1897, 4029],\n       [1897, 4040],\n       [1888, 4056],\n       [1889, 4068],\n       [1910, 4096],\n       [1938, 4131],\n       [1978, 4176],\n       [2190, 4176],\n       [2189, 4174],\n       [2196, 4170],\n       [2202, 4172],\n       [2203, 4176],\n       [2234, 4176]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '027',\n     'FIPS': '34027',\n     'STATE': 'NJ',\n     'NAME': 'Morris',\n     'LSAD': 'County'},\n    'id': 2228,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2352, 3959],\n       [2351, 3956],\n       [2353, 3955],\n       [2355, 3957],\n       [2358, 3957],\n       [2358, 3948],\n       [2364, 3949],\n       [2367, 3952],\n       [2370, 3951],\n       [2376, 3952],\n       [2381, 3956],\n       [2380, 3950],\n       [2390, 3954],\n       [2397, 3953],\n       [2396, 3948],\n       [2397, 3945],\n       [2396, 3944],\n       [2397, 3942],\n       [2400, 3941],\n       [2406, 3945],\n       [2411, 3944],\n       [2411, 3942],\n       [2404, 3940],\n       [2408, 3938],\n       [2407, 3933],\n       [2411, 3934],\n       [2413, 3937],\n       [2427, 3936],\n       [2433, 3931],\n       [2437, 3917],\n       [2443, 3911],\n       [2451, 3909],\n       [2453, 3906],\n       [2472, 3904],\n       [2527, 3864],\n       [2529, 3868],\n       [2536, 3865],\n       [2534, 3860],\n       [2632, 3788],\n       [2623, 3784],\n       [2618, 3760],\n       [2620, 3749],\n       [2619, 3744],\n       [2607, 3723],\n       [2606, 3710],\n       [2596, 3695],\n       [2592, 3684],\n       [2584, 3671],\n       [2580, 3648],\n       [2586, 3626],\n       [2589, 3623],\n       [2596, 3622],\n       [2606, 3623],\n       [2610, 3626],\n       [2614, 3633],\n       [2621, 3637],\n       [2643, 3638],\n       [2650, 3633],\n       [2651, 3629],\n       [2645, 3598],\n       [2647, 3588],\n       [2657, 3563],\n       [2623, 3508],\n       [2620, 3507],\n       [2588, 3532],\n       [2587, 3536],\n       [2585, 3537],\n       [2577, 3538],\n       [2575, 3535],\n       [2528, 3532],\n       [2516, 3540],\n       [2511, 3558],\n       [2498, 3571],\n       [2448, 3583],\n       [2406, 3600],\n       [2398, 3598],\n       [2393, 3595],\n       [2392, 3591],\n       [2386, 3584],\n       [2382, 3583],\n       [2368, 3587],\n       [2361, 3594],\n       [2354, 3594],\n       [2348, 3590],\n       [2323, 3606],\n       [2304, 3624],\n       [2280, 3633],\n       [2279, 3637],\n       [2288, 3642],\n       [2291, 3646],\n       [2286, 3650],\n       [2287, 3652],\n       [2298, 3655],\n       [2301, 3662],\n       [2301, 3665],\n       [2295, 3665],\n       [2291, 3668],\n       [2286, 3669],\n       [2287, 3675],\n       [2277, 3669],\n       [2275, 3674],\n       [2272, 3675],\n       [2274, 3678],\n       [2278, 3678],\n       [2282, 3680],\n       [2283, 3686],\n       [2280, 3687],\n       [2284, 3694],\n       [2282, 3700],\n       [2279, 3701],\n       [2287, 3711],\n       [2288, 3739],\n       [2291, 3742],\n       [2291, 3748],\n       [2298, 3749],\n       [2303, 3757],\n       [2311, 3763],\n       [2313, 3774],\n       [2320, 3776],\n       [2327, 3774],\n       [2335, 3785],\n       [2338, 3792],\n       [2334, 3803],\n       [2343, 3811],\n       [2341, 3816],\n       [2342, 3829],\n       [2345, 3830],\n       [2347, 3834],\n       [2343, 3835],\n       [2341, 3844],\n       [2345, 3847],\n       [2346, 3851],\n       [2348, 3854],\n       [2347, 3858],\n       [2349, 3861],\n       [2357, 3861],\n       [2354, 3869],\n       [2358, 3871],\n       [2359, 3874],\n       [2356, 3876],\n       [2353, 3873],\n       [2347, 3877],\n       [2339, 3878],\n       [2337, 3883],\n       [2331, 3878],\n       [2329, 3884],\n       [2332, 3885],\n       [2330, 3888],\n       [2335, 3891],\n       [2330, 3894],\n       [2334, 3898],\n       [2330, 3900],\n       [2330, 3902],\n       [2326, 3902],\n       [2329, 3918],\n       [2327, 3923],\n       [2328, 3927],\n       [2333, 3929],\n       [2328, 3931],\n       [2331, 3935],\n       [2335, 3935],\n       [2335, 3937],\n       [2330, 3939],\n       [2331, 3942],\n       [2335, 3952],\n       [2340, 3955],\n       [2342, 3959],\n       [2344, 3953],\n       [2348, 3953],\n       [2348, 3956],\n       [2345, 3956],\n       [2347, 3958],\n       [2352, 3959]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '013',\n     'FIPS': '34013',\n     'STATE': 'NJ',\n     'NAME': 'Essex',\n     'LSAD': 'County'},\n    'id': 2206,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2380, 3382],\n       [2380, 3379],\n       [2378, 3378],\n       [2380, 3376],\n       [2377, 3366],\n       [2385, 3361],\n       [2397, 3350],\n       [2443, 3360],\n       [2446, 3370],\n       [2467, 3365],\n       [2478, 3370],\n       [2484, 3363],\n       [2498, 3366],\n       [2504, 3368],\n       [2508, 3373],\n       [2512, 3369],\n       [2513, 3362],\n       [2521, 3355],\n       [2517, 3320],\n       [2509, 3290],\n       [2504, 3283],\n       [2484, 3287],\n       [2460, 3269],\n       [2456, 3255],\n       [2463, 3214],\n       [2431, 3172],\n       [2425, 3151],\n       [2432, 3120],\n       [2440, 3106],\n       [2477, 3093],\n       [2494, 3084],\n       [2494, 3083],\n       [2497, 3080],\n       [2490, 3067],\n       [2483, 3068],\n       [2481, 3065],\n       [2479, 3067],\n       [2461, 3042],\n       [2461, 3006],\n       [2458, 3003],\n       [2460, 2996],\n       [2455, 2990],\n       [2271, 2799],\n       [2248, 2769],\n       [2249, 2752],\n       [2150, 2698],\n       [2146, 2700],\n       [2116, 2702],\n       [2119, 2741],\n       [2095, 2754],\n       [2091, 2751],\n       [2088, 2754],\n       [2086, 2754],\n       [2077, 2759],\n       [2072, 2766],\n       [2061, 2765],\n       [2061, 2772],\n       [2052, 2780],\n       [2045, 2776],\n       [2040, 2771],\n       [2036, 2769],\n       [2032, 2775],\n       [2028, 2777],\n       [2027, 2775],\n       [2021, 2773],\n       [2016, 2773],\n       [2010, 2776],\n       [2007, 2783],\n       [1999, 2785],\n       [1995, 2791],\n       [1991, 2790],\n       [1983, 2797],\n       [1976, 2799],\n       [1972, 2803],\n       [1969, 2816],\n       [1957, 2820],\n       [1953, 2819],\n       [1947, 2821],\n       [1943, 2832],\n       [1936, 2833],\n       [1925, 2839],\n       [1916, 2838],\n       [1915, 2840],\n       [1915, 2845],\n       [1912, 2847],\n       [1914, 2857],\n       [1904, 2869],\n       [1904, 2878],\n       [1912, 2916],\n       [1916, 2919],\n       [1919, 2933],\n       [1928, 2936],\n       [1944, 2945],\n       [1955, 2956],\n       [1960, 2967],\n       [1966, 2971],\n       [1966, 2981],\n       [1969, 2989],\n       [1969, 3016],\n       [1983, 3025],\n       [2013, 3040],\n       [2041, 3056],\n       [2055, 3068],\n       [2064, 3080],\n       [2075, 3091],\n       [2082, 3099],\n       [2102, 3115],\n       [2119, 3139],\n       [2125, 3143],\n       [2140, 3148],\n       [2152, 3156],\n       [2153, 3166],\n       [2151, 3171],\n       [2143, 3178],\n       [2141, 3190],\n       [2122, 3195],\n       [2119, 3197],\n       [2100, 3230],\n       [2083, 3244],\n       [2072, 3255],\n       [2068, 3273],\n       [2065, 3280],\n       [2059, 3286],\n       [2054, 3288],\n       [2057, 3292],\n       [2056, 3305],\n       [2059, 3307],\n       [2063, 3315],\n       [2071, 3326],\n       [2070, 3329],\n       [2076, 3331],\n       [2086, 3339],\n       [2091, 3334],\n       [2091, 3337],\n       [2094, 3340],\n       [2097, 3338],\n       [2104, 3341],\n       [2108, 3340],\n       [2118, 3345],\n       [2126, 3354],\n       [2137, 3362],\n       [2143, 3362],\n       [2147, 3364],\n       [2154, 3355],\n       [2156, 3357],\n       [2203, 3362],\n       [2279, 3370],\n       [2380, 3382]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '023',\n     'FIPS': '34023',\n     'STATE': 'NJ',\n     'NAME': 'Middlesex',\n     'LSAD': 'County'},\n    'id': 2194,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2280, 3633],\n       [2304, 3624],\n       [2323, 3606],\n       [2348, 3590],\n       [2354, 3594],\n       [2361, 3594],\n       [2368, 3587],\n       [2382, 3583],\n       [2386, 3584],\n       [2392, 3591],\n       [2393, 3595],\n       [2398, 3598],\n       [2406, 3600],\n       [2448, 3583],\n       [2498, 3571],\n       [2511, 3558],\n       [2516, 3540],\n       [2528, 3532],\n       [2575, 3535],\n       [2577, 3538],\n       [2585, 3537],\n       [2587, 3536],\n       [2588, 3532],\n       [2620, 3507],\n       [2623, 3508],\n       [2587, 3450],\n       [2558, 3454],\n       [2551, 3454],\n       [2527, 3425],\n       [2521, 3355],\n       [2513, 3362],\n       [2512, 3369],\n       [2508, 3373],\n       [2504, 3368],\n       [2498, 3366],\n       [2484, 3363],\n       [2478, 3370],\n       [2467, 3365],\n       [2446, 3370],\n       [2443, 3360],\n       [2397, 3350],\n       [2385, 3361],\n       [2377, 3366],\n       [2380, 3376],\n       [2378, 3378],\n       [2380, 3379],\n       [2380, 3382],\n       [2279, 3370],\n       [2203, 3362],\n       [2156, 3357],\n       [2154, 3355],\n       [2147, 3364],\n       [2152, 3365],\n       [2159, 3373],\n       [2164, 3373],\n       [2169, 3381],\n       [2173, 3383],\n       [2184, 3385],\n       [2193, 3396],\n       [2199, 3397],\n       [2203, 3402],\n       [2206, 3414],\n       [2213, 3418],\n       [2217, 3429],\n       [2219, 3444],\n       [2220, 3446],\n       [2226, 3447],\n       [2229, 3455],\n       [2229, 3462],\n       [2235, 3473],\n       [2235, 3481],\n       [2236, 3487],\n       [2234, 3493],\n       [2231, 3495],\n       [2224, 3491],\n       [2220, 3485],\n       [2214, 3483],\n       [2211, 3478],\n       [2208, 3477],\n       [2202, 3485],\n       [2198, 3487],\n       [2196, 3491],\n       [2185, 3486],\n       [2175, 3475],\n       [2164, 3469],\n       [2150, 3506],\n       [2154, 3508],\n       [2160, 3507],\n       [2163, 3512],\n       [2162, 3514],\n       [2164, 3518],\n       [2162, 3524],\n       [2164, 3529],\n       [2169, 3529],\n       [2181, 3537],\n       [2184, 3542],\n       [2189, 3545],\n       [2194, 3545],\n       [2197, 3555],\n       [2200, 3560],\n       [2217, 3568],\n       [2222, 3577],\n       [2227, 3577],\n       [2228, 3583],\n       [2236, 3592],\n       [2246, 3590],\n       [2253, 3592],\n       [2253, 3606],\n       [2258, 3607],\n       [2262, 3604],\n       [2272, 3615],\n       [2268, 3621],\n       [2272, 3623],\n       [2276, 3622],\n       [2280, 3622],\n       [2282, 3626],\n       [2280, 3633]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '039',\n     'FIPS': '34039',\n     'STATE': 'NJ',\n     'NAME': 'Union',\n     'LSAD': 'County'},\n    'id': 2204,\n    'type': 'Feature'},\n   {'geometry': {'type': 'MultiPolygon',\n     'coordinates': [[[[2941, 3976],\n        [3026, 3943],\n        [3031, 3954],\n        [3034, 3955],\n        [3033, 3958],\n        [3036, 3961],\n        [3039, 3961],\n        [3036, 3956],\n        [3042, 3955],\n        [3048, 3952],\n        [3048, 3950],\n        [3053, 3950],\n        [3057, 3931],\n        [3078, 3922],\n        [3079, 3925],\n        [3137, 3905],\n        [3135, 3901],\n        [3134, 3882],\n        [3129, 3862],\n        [3130, 3854],\n        [3135, 3849],\n        [3139, 3841],\n        [3140, 3824],\n        [3136, 3820],\n        [3130, 3830],\n        [3126, 3839],\n        [3128, 3848],\n        [3124, 3857],\n        [3122, 3856],\n        [3117, 3850],\n        [3114, 3842],\n        [3103, 3845],\n        [3094, 3839],\n        [3090, 3817],\n        [3091, 3809],\n        [3096, 3798],\n        [3106, 3785],\n        [3117, 3779],\n        [3133, 3751],\n        [3131, 3749],\n        [3122, 3749],\n        [3097, 3759],\n        [3088, 3760],\n        [3074, 3755],\n        [3038, 3749],\n        [3022, 3744],\n        [3011, 3732],\n        [3010, 3724],\n        [2998, 3722],\n        [2982, 3725],\n        [2981, 3730],\n        [2949, 3742],\n        [2940, 3748],\n        [2935, 3754],\n        [2928, 3754],\n        [2921, 3765],\n        [2920, 3772],\n        [2919, 3818],\n        [2926, 3837],\n        [2938, 3859],\n        [2952, 3877],\n        [2957, 3891],\n        [2957, 3896],\n        [2953, 3901],\n        [2946, 3897],\n        [2945, 3895],\n        [2938, 3897],\n        [2936, 3900],\n        [2919, 3907],\n        [2925, 3922],\n        [2941, 3976]]],\n      [[[3156, 3866],\n        [3162, 3860],\n        [3164, 3854],\n        [3162, 3836],\n        [3157, 3835],\n        [3152, 3842],\n        [3152, 3864],\n        [3156, 3866]]]]},\n    'properties': {'STATE_FIPS': '36',\n     'COUNTY_FIP': '005',\n     'FIPS': '36005',\n     'STATE': 'NY',\n     'NAME': 'Bronx',\n     'LSAD': 'County'},\n    'id': 2199,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2797, 3795],\n       [2802, 3794],\n       [2806, 3789],\n       [2803, 3784],\n       [2803, 3780],\n       [2832, 3760],\n       [2831, 3752],\n       [2844, 3745],\n       [2802, 3666],\n       [2786, 3575],\n       [2766, 3578],\n       [2747, 3549],\n       [2722, 3481],\n       [2694, 3468],\n       [2685, 3461],\n       [2613, 3446],\n       [2587, 3450],\n       [2657, 3563],\n       [2647, 3588],\n       [2645, 3598],\n       [2651, 3629],\n       [2650, 3633],\n       [2643, 3638],\n       [2621, 3637],\n       [2614, 3633],\n       [2610, 3626],\n       [2606, 3623],\n       [2596, 3622],\n       [2589, 3623],\n       [2583, 3633],\n       [2580, 3648],\n       [2582, 3665],\n       [2586, 3675],\n       [2592, 3684],\n       [2596, 3695],\n       [2606, 3710],\n       [2607, 3723],\n       [2632, 3707],\n       [2633, 3699],\n       [2636, 3695],\n       [2640, 3696],\n       [2650, 3692],\n       [2653, 3694],\n       [2657, 3703],\n       [2662, 3706],\n       [2669, 3705],\n       [2672, 3702],\n       [2675, 3703],\n       [2679, 3692],\n       [2677, 3687],\n       [2683, 3677],\n       [2686, 3679],\n       [2690, 3677],\n       [2693, 3687],\n       [2690, 3703],\n       [2692, 3711],\n       [2708, 3736],\n       [2710, 3742],\n       [2718, 3744],\n       [2722, 3747],\n       [2727, 3755],\n       [2733, 3760],\n       [2742, 3763],\n       [2769, 3765],\n       [2775, 3774],\n       [2780, 3774],\n       [2781, 3781],\n       [2786, 3778],\n       [2785, 3780],\n       [2787, 3781],\n       [2788, 3785],\n       [2792, 3785],\n       [2792, 3789],\n       [2797, 3795]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '017',\n     'FIPS': '34017',\n     'STATE': 'NJ',\n     'NAME': 'Hudson',\n     'LSAD': 'County'},\n    'id': 2205,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2711, 3458],\n       [2720, 3444],\n       [2723, 3411],\n       [2734, 3388],\n       [2744, 3372],\n       [2735, 3353],\n       [2722, 3335],\n       [2689, 3301],\n       [2659, 3263],\n       [2658, 3265],\n       [2644, 3255],\n       [2622, 3231],\n       [2617, 3238],\n       [2611, 3241],\n       [2605, 3240],\n       [2587, 3225],\n       [2562, 3211],\n       [2556, 3213],\n       [2531, 3196],\n       [2515, 3192],\n       [2502, 3179],\n       [2482, 3176],\n       [2462, 3166],\n       [2457, 3167],\n       [2451, 3178],\n       [2449, 3189],\n       [2454, 3200],\n       [2468, 3214],\n       [2470, 3233],\n       [2461, 3257],\n       [2488, 3279],\n       [2506, 3279],\n       [2515, 3290],\n       [2525, 3345],\n       [2535, 3360],\n       [2537, 3369],\n       [2536, 3396],\n       [2529, 3398],\n       [2528, 3403],\n       [2530, 3426],\n       [2546, 3446],\n       [2559, 3453],\n       [2568, 3452],\n       [2574, 3446],\n       [2599, 3440],\n       [2647, 3446],\n       [2696, 3458],\n       [2711, 3458]]]},\n    'properties': {'STATE_FIPS': '36',\n     'COUNTY_FIP': '085',\n     'FIPS': '36085',\n     'STATE': 'NY',\n     'NAME': 'Richmond',\n     'LSAD': 'County'},\n    'id': 2210,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2891, 3633],\n       [2906, 3626],\n       [2913, 3615],\n       [2925, 3611],\n       [2933, 3596],\n       [2934, 3590],\n       [2932, 3587],\n       [2937, 3577],\n       [2936, 3574],\n       [2950, 3564],\n       [2949, 3562],\n       [2952, 3559],\n       [2950, 3557],\n       [2961, 3549],\n       [2959, 3546],\n       [2966, 3541],\n       [2966, 3533],\n       [2972, 3523],\n       [2976, 3529],\n       [2978, 3525],\n       [2982, 3528],\n       [2983, 3527],\n       [3005, 3546],\n       [3013, 3548],\n       [3017, 3522],\n       [3020, 3523],\n       [3025, 3502],\n       [3029, 3503],\n       [3032, 3488],\n       [3028, 3487],\n       [3029, 3480],\n       [3021, 3477],\n       [3031, 3460],\n       [3033, 3447],\n       [3043, 3449],\n       [3064, 3420],\n       [3063, 3379],\n       [3040, 3344],\n       [2998, 3317],\n       [2911, 3300],\n       [2902, 3305],\n       [2894, 3314],\n       [2834, 3308],\n       [2819, 3309],\n       [2804, 3316],\n       [2803, 3323],\n       [2819, 3347],\n       [2817, 3357],\n       [2806, 3367],\n       [2774, 3374],\n       [2764, 3390],\n       [2760, 3413],\n       [2766, 3436],\n       [2775, 3454],\n       [2795, 3478],\n       [2792, 3516],\n       [2788, 3521],\n       [2793, 3518],\n       [2798, 3520],\n       [2799, 3522],\n       [2816, 3536],\n       [2825, 3557],\n       [2831, 3566],\n       [2852, 3568],\n       [2862, 3574],\n       [2866, 3570],\n       [2866, 3575],\n       [2869, 3589],\n       [2878, 3603],\n       [2876, 3630],\n       [2878, 3628],\n       [2889, 3633],\n       [2891, 3633]]]},\n    'properties': {'STATE_FIPS': '36',\n     'COUNTY_FIP': '047',\n     'FIPS': '36047',\n     'STATE': 'NY',\n     'NAME': 'Kings',\n     'LSAD': 'County'},\n    'id': 2203,\n    'type': 'Feature'},\n   {'geometry': {'type': 'MultiPolygon',\n     'coordinates': [[[[2919, 3907],\n        [2936, 3900],\n        [2938, 3897],\n        [2945, 3895],\n        [2946, 3897],\n        [2953, 3901],\n        [2957, 3896],\n        [2957, 3891],\n        [2952, 3877],\n        [2938, 3859],\n        [2926, 3837],\n        [2919, 3813],\n        [2921, 3765],\n        [2928, 3754],\n        [2935, 3754],\n        [2940, 3748],\n        [2949, 3742],\n        [2952, 3732],\n        [2945, 3727],\n        [2941, 3721],\n        [2927, 3705],\n        [2921, 3708],\n        [2918, 3706],\n        [2916, 3709],\n        [2915, 3705],\n        [2911, 3701],\n        [2912, 3697],\n        [2916, 3695],\n        [2915, 3693],\n        [2903, 3681],\n        [2887, 3656],\n        [2882, 3645],\n        [2878, 3640],\n        [2876, 3630],\n        [2878, 3603],\n        [2869, 3589],\n        [2866, 3575],\n        [2866, 3570],\n        [2862, 3574],\n        [2852, 3568],\n        [2830, 3565],\n        [2816, 3536],\n        [2799, 3522],\n        [2798, 3520],\n        [2793, 3518],\n        [2787, 3523],\n        [2786, 3532],\n        [2790, 3545],\n        [2795, 3548],\n        [2797, 3561],\n        [2793, 3571],\n        [2786, 3575],\n        [2802, 3666],\n        [2875, 3801],\n        [2889, 3842],\n        [2919, 3907]]],\n      [[[2763, 3558],\n        [2766, 3555],\n        [2757, 3535],\n        [2753, 3539],\n        [2763, 3558]]]]},\n    'properties': {'STATE_FIPS': '36',\n     'COUNTY_FIP': '061',\n     'FIPS': '36061',\n     'STATE': 'NY',\n     'NAME': 'New York',\n     'LSAD': 'County'},\n    'id': 2200,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[3088, 3760],\n       [3097, 3759],\n       [3122, 3749],\n       [3131, 3749],\n       [3133, 3751],\n       [3140, 3740],\n       [3148, 3740],\n       [3159, 3751],\n       [3190, 3709],\n       [3190, 3712],\n       [3228, 3682],\n       [3255, 3660],\n       [3257, 3655],\n       [3258, 3633],\n       [3247, 3611],\n       [3228, 3606],\n       [3214, 3600],\n       [3219, 3578],\n       [3222, 3558],\n       [3220, 3551],\n       [3221, 3519],\n       [3217, 3493],\n       [3222, 3468],\n       [3217, 3463],\n       [3199, 3457],\n       [3198, 3446],\n       [3201, 3434],\n       [3197, 3432],\n       [3198, 3437],\n       [3198, 3438],\n       [3194, 3438],\n       [3193, 3431],\n       [3162, 3415],\n       [3163, 3394],\n       [3171, 3387],\n       [3178, 3386],\n       [3178, 3384],\n       [3189, 3389],\n       [3190, 3387],\n       [3192, 3388],\n       [3203, 3370],\n       [3203, 3354],\n       [3204, 3352],\n       [3195, 3352],\n       [3189, 3349],\n       [3179, 3339],\n       [3181, 3347],\n       [3149, 3348],\n       [3103, 3336],\n       [3063, 3322],\n       [2998, 3290],\n       [2908, 3256],\n       [2909, 3278],\n       [2920, 3289],\n       [2915, 3298],\n       [2911, 3300],\n       [2998, 3317],\n       [3040, 3344],\n       [3063, 3379],\n       [3064, 3420],\n       [3043, 3449],\n       [3033, 3447],\n       [3031, 3460],\n       [3021, 3477],\n       [3029, 3480],\n       [3028, 3487],\n       [3032, 3488],\n       [3029, 3503],\n       [3025, 3502],\n       [3020, 3523],\n       [3017, 3522],\n       [3013, 3548],\n       [3005, 3546],\n       [2983, 3527],\n       [2982, 3528],\n       [2978, 3525],\n       [2976, 3529],\n       [2972, 3523],\n       [2966, 3533],\n       [2966, 3541],\n       [2959, 3546],\n       [2961, 3549],\n       [2950, 3557],\n       [2952, 3559],\n       [2949, 3562],\n       [2950, 3564],\n       [2936, 3574],\n       [2937, 3577],\n       [2932, 3587],\n       [2934, 3590],\n       [2933, 3596],\n       [2925, 3611],\n       [2913, 3615],\n       [2906, 3626],\n       [2891, 3633],\n       [2889, 3633],\n       [2878, 3628],\n       [2876, 3630],\n       [2878, 3640],\n       [2882, 3645],\n       [2887, 3656],\n       [2903, 3681],\n       [2915, 3693],\n       [2916, 3695],\n       [2912, 3697],\n       [2911, 3701],\n       [2915, 3705],\n       [2916, 3709],\n       [2918, 3706],\n       [2921, 3708],\n       [2927, 3705],\n       [2941, 3721],\n       [2945, 3727],\n       [2952, 3732],\n       [2949, 3742],\n       [2981, 3730],\n       [2982, 3725],\n       [2998, 3722],\n       [3010, 3724],\n       [3011, 3732],\n       [3022, 3744],\n       [3038, 3749],\n       [3074, 3755],\n       [3088, 3760]]]},\n    'properties': {'STATE_FIPS': '36',\n     'COUNTY_FIP': '081',\n     'FIPS': '36081',\n     'STATE': 'NY',\n     'NAME': 'Queens',\n     'LSAD': 'County'},\n    'id': 2202,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2801, 3129],\n       [2811, 3126],\n       [2828, 3114],\n       [2853, 3059],\n       [2855, 2998],\n       [2863, 2928],\n       [2863, 2883],\n       [2855, 2790],\n       [2848, 2752],\n       [2831, 2672],\n       [2778, 2453],\n       [2772, 2416],\n       [2775, 2412],\n       [2765, 2418],\n       [2763, 2417],\n       [2757, 2420],\n       [2749, 2418],\n       [2740, 2412],\n       [2738, 2405],\n       [2733, 2403],\n       [2718, 2389],\n       [2700, 2402],\n       [2684, 2424],\n       [2685, 2431],\n       [2683, 2435],\n       [2685, 2438],\n       [2681, 2448],\n       [2682, 2464],\n       [2680, 2465],\n       [2677, 2463],\n       [2670, 2472],\n       [2671, 2475],\n       [2665, 2480],\n       [2603, 2407],\n       [2597, 2409],\n       [2596, 2413],\n       [2592, 2416],\n       [2591, 2419],\n       [2588, 2423],\n       [2571, 2424],\n       [2561, 2430],\n       [2556, 2429],\n       [2551, 2431],\n       [2546, 2430],\n       [2539, 2433],\n       [2533, 2432],\n       [2525, 2434],\n       [2513, 2429],\n       [2503, 2429],\n       [2495, 2427],\n       [2483, 2429],\n       [2480, 2432],\n       [2473, 2433],\n       [2465, 2445],\n       [2467, 2453],\n       [2469, 2454],\n       [2470, 2466],\n       [2458, 2494],\n       [2453, 2500],\n       [2447, 2503],\n       [2443, 2503],\n       [2445, 2509],\n       [2443, 2518],\n       [2446, 2524],\n       [2442, 2535],\n       [2438, 2540],\n       [2229, 2548],\n       [2138, 2464],\n       [2054, 2391],\n       [2016, 2370],\n       [1927, 2567],\n       [1948, 2575],\n       [1952, 2574],\n       [1954, 2562],\n       [1957, 2562],\n       [1968, 2569],\n       [1985, 2576],\n       [1991, 2573],\n       [2002, 2581],\n       [2011, 2600],\n       [2023, 2615],\n       [2025, 2624],\n       [2031, 2632],\n       [2069, 2652],\n       [2120, 2682],\n       [2119, 2689],\n       [2114, 2695],\n       [2112, 2702],\n       [2146, 2700],\n       [2150, 2698],\n       [2249, 2752],\n       [2248, 2769],\n       [2271, 2799],\n       [2460, 2996],\n       [2458, 3003],\n       [2461, 3006],\n       [2461, 3042],\n       [2479, 3067],\n       [2481, 3065],\n       [2483, 3068],\n       [2490, 3067],\n       [2497, 3080],\n       [2494, 3083],\n       [2494, 3084],\n       [2516, 3073],\n       [2521, 3060],\n       [2521, 3056],\n       [2518, 3054],\n       [2520, 3050],\n       [2527, 3057],\n       [2539, 3061],\n       [2548, 3073],\n       [2567, 3089],\n       [2567, 3076],\n       [2564, 3074],\n       [2566, 3071],\n       [2574, 3079],\n       [2578, 3075],\n       [2580, 3072],\n       [2584, 3075],\n       [2592, 3071],\n       [2599, 3073],\n       [2614, 3079],\n       [2620, 3086],\n       [2624, 3088],\n       [2651, 3070],\n       [2693, 3056],\n       [2711, 3047],\n       [2736, 3026],\n       [2752, 3018],\n       [2812, 3003],\n       [2824, 3003],\n       [2828, 3019],\n       [2834, 3064],\n       [2813, 3106],\n       [2796, 3120],\n       [2796, 3124],\n       [2801, 3129]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '025',\n     'FIPS': '34025',\n     'STATE': 'NJ',\n     'NAME': 'Monmouth',\n     'LSAD': 'County'},\n    'id': 2195,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[2229, 2548],\n       [2438, 2540],\n       [2442, 2535],\n       [2446, 2524],\n       [2443, 2518],\n       [2445, 2509],\n       [2443, 2503],\n       [2447, 2503],\n       [2453, 2500],\n       [2458, 2494],\n       [2470, 2466],\n       [2469, 2454],\n       [2467, 2453],\n       [2465, 2445],\n       [2473, 2433],\n       [2480, 2432],\n       [2483, 2429],\n       [2495, 2427],\n       [2503, 2429],\n       [2513, 2429],\n       [2525, 2434],\n       [2533, 2432],\n       [2539, 2433],\n       [2546, 2430],\n       [2551, 2431],\n       [2556, 2429],\n       [2561, 2430],\n       [2571, 2424],\n       [2588, 2423],\n       [2591, 2419],\n       [2592, 2416],\n       [2596, 2413],\n       [2597, 2409],\n       [2603, 2407],\n       [2665, 2480],\n       [2671, 2475],\n       [2670, 2472],\n       [2677, 2463],\n       [2680, 2465],\n       [2682, 2464],\n       [2681, 2448],\n       [2685, 2438],\n       [2683, 2435],\n       [2685, 2431],\n       [2684, 2424],\n       [2700, 2402],\n       [2718, 2389],\n       [2733, 2403],\n       [2738, 2405],\n       [2740, 2412],\n       [2749, 2418],\n       [2757, 2420],\n       [2763, 2417],\n       [2765, 2418],\n       [2776, 2411],\n       [2773, 2409],\n       [2764, 2374],\n       [2728, 2180],\n       [2709, 2050],\n       [2689, 1840],\n       [2680, 1779],\n       [2681, 1770],\n       [2678, 1763],\n       [2656, 1727],\n       [2615, 1630],\n       [2544, 1509],\n       [2471, 1376],\n       [2459, 1363],\n       [2418, 1299],\n       [2396, 1277],\n       [2381, 1273],\n       [2378, 1286],\n       [2369, 1285],\n       [2367, 1272],\n       [2353, 1282],\n       [2346, 1284],\n       [2330, 1285],\n       [2306, 1283],\n       [2296, 1276],\n       [2267, 1270],\n       [2234, 1349],\n       [2224, 1363],\n       [2216, 1366],\n       [2215, 1368],\n       [2216, 1378],\n       [2214, 1380],\n       [2219, 1390],\n       [2214, 1393],\n       [2212, 1402],\n       [2216, 1405],\n       [2221, 1405],\n       [2224, 1406],\n       [2226, 1404],\n       [2232, 1404],\n       [2235, 1408],\n       [2238, 1409],\n       [2242, 1417],\n       [2244, 1432],\n       [2250, 1450],\n       [2253, 1454],\n       [2254, 1789],\n       [2167, 1994],\n       [2084, 2203],\n       [2052, 2289],\n       [2016, 2370],\n       [2054, 2391],\n       [2138, 2464],\n       [2229, 2548]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '029',\n     'FIPS': '34029',\n     'STATE': 'NJ',\n     'NAME': 'Ocean',\n     'LSAD': 'County'},\n    'id': 2176,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[3554, 3986],\n       [3563, 3979],\n       [3567, 3971],\n       [3568, 3972],\n       [3574, 3953],\n       [3579, 3944],\n       [3583, 3941],\n       [3585, 3898],\n       [3597, 3881],\n       [3604, 3879],\n       [3602, 3864],\n       [3612, 3843],\n       [3615, 3828],\n       [3661, 3502],\n       [3660, 3492],\n       [3660, 3476],\n       [3661, 3473],\n       [3658, 3473],\n       [3661, 3384],\n       [3622, 3372],\n       [3572, 3360],\n       [3440, 3315],\n       [3442, 3337],\n       [3428, 3340],\n       [3388, 3342],\n       [3336, 3332],\n       [3179, 3335],\n       [3179, 3339],\n       [3189, 3349],\n       [3195, 3352],\n       [3204, 3352],\n       [3203, 3354],\n       [3203, 3370],\n       [3192, 3388],\n       [3190, 3387],\n       [3189, 3389],\n       [3178, 3384],\n       [3178, 3386],\n       [3171, 3387],\n       [3163, 3394],\n       [3162, 3415],\n       [3193, 3431],\n       [3194, 3438],\n       [3198, 3438],\n       [3198, 3437],\n       [3197, 3432],\n       [3201, 3434],\n       [3198, 3446],\n       [3199, 3457],\n       [3217, 3463],\n       [3222, 3468],\n       [3217, 3493],\n       [3221, 3519],\n       [3220, 3551],\n       [3222, 3558],\n       [3219, 3578],\n       [3214, 3600],\n       [3228, 3606],\n       [3247, 3611],\n       [3258, 3633],\n       [3256, 3658],\n       [3190, 3712],\n       [3190, 3709],\n       [3159, 3751],\n       [3180, 3790],\n       [3179, 3801],\n       [3217, 3851],\n       [3220, 3859],\n       [3214, 3875],\n       [3216, 3877],\n       [3239, 3884],\n       [3294, 3859],\n       [3323, 3872],\n       [3325, 3900],\n       [3344, 3928],\n       [3355, 3938],\n       [3365, 3941],\n       [3379, 3938],\n       [3402, 3947],\n       [3411, 3956],\n       [3448, 3972],\n       [3454, 3972],\n       [3480, 3959],\n       [3522, 3970],\n       [3528, 3967],\n       [3550, 3977],\n       [3554, 3986]]]},\n    'properties': {'STATE_FIPS': '36',\n     'COUNTY_FIP': '059',\n     'FIPS': '36059',\n     'STATE': 'NY',\n     'NAME': 'Nassau',\n     'LSAD': 'County'},\n    'id': 2201,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[1749, 1707],\n       [1837, 1634],\n       [1842, 1625],\n       [1846, 1623],\n       [1851, 1615],\n       [1854, 1606],\n       [1854, 1601],\n       [1860, 1591],\n       [1863, 1569],\n       [1861, 1566],\n       [1862, 1563],\n       [1860, 1560],\n       [1863, 1559],\n       [1863, 1554],\n       [1862, 1550],\n       [1865, 1546],\n       [1861, 1538],\n       [1863, 1530],\n       [1861, 1521],\n       [1866, 1520],\n       [1868, 1518],\n       [1868, 1512],\n       [1871, 1511],\n       [1874, 1515],\n       [1878, 1514],\n       [1878, 1509],\n       [1888, 1510],\n       [1889, 1505],\n       [1890, 1504],\n       [1893, 1505],\n       [1896, 1513],\n       [1900, 1513],\n       [1902, 1507],\n       [1902, 1501],\n       [1906, 1499],\n       [1910, 1503],\n       [1911, 1509],\n       [1914, 1512],\n       [1916, 1512],\n       [1926, 1505],\n       [1929, 1506],\n       [1936, 1502],\n       [1954, 1488],\n       [1964, 1484],\n       [1966, 1481],\n       [1969, 1471],\n       [1975, 1468],\n       [1983, 1458],\n       [1992, 1459],\n       [2000, 1455],\n       [2015, 1457],\n       [2022, 1442],\n       [2033, 1434],\n       [2031, 1425],\n       [2034, 1417],\n       [2037, 1413],\n       [2045, 1411],\n       [2051, 1405],\n       [2054, 1403],\n       [2059, 1403],\n       [2067, 1411],\n       [2072, 1412],\n       [2076, 1404],\n       [2080, 1402],\n       [2088, 1403],\n       [2092, 1395],\n       [2099, 1391],\n       [2101, 1381],\n       [2103, 1379],\n       [2108, 1379],\n       [2113, 1383],\n       [2120, 1382],\n       [2126, 1372],\n       [2130, 1369],\n       [2134, 1371],\n       [2139, 1377],\n       [2144, 1379],\n       [2148, 1379],\n       [2150, 1376],\n       [2152, 1366],\n       [2154, 1362],\n       [2158, 1361],\n       [2162, 1367],\n       [2162, 1378],\n       [2164, 1382],\n       [2169, 1384],\n       [2172, 1383],\n       [2176, 1378],\n       [2172, 1363],\n       [2178, 1354],\n       [2182, 1350],\n       [2185, 1349],\n       [2190, 1353],\n       [2190, 1359],\n       [2184, 1369],\n       [2184, 1374],\n       [2187, 1376],\n       [2191, 1377],\n       [2199, 1373],\n       [2201, 1373],\n       [2209, 1380],\n       [2214, 1380],\n       [2216, 1378],\n       [2215, 1368],\n       [2216, 1366],\n       [2224, 1363],\n       [2234, 1349],\n       [2267, 1270],\n       [2296, 1276],\n       [2306, 1283],\n       [2330, 1285],\n       [2346, 1284],\n       [2353, 1282],\n       [2367, 1272],\n       [2365, 1261],\n       [2373, 1242],\n       [2378, 1240],\n       [2382, 1233],\n       [2378, 1218],\n       [2334, 1144],\n       [2287, 1088],\n       [2229, 1041],\n       [2227, 1018],\n       [2221, 1010],\n       [2152, 980],\n       [2062, 921],\n       [2033, 896],\n       [2029, 900],\n       [2023, 900],\n       [2011, 893],\n       [1994, 894],\n       [1967, 914],\n       [1960, 913],\n       [1918, 882],\n       [1913, 881],\n       [1900, 886],\n       [1889, 888],\n       [1885, 887],\n       [1879, 882],\n       [1875, 875],\n       [1870, 874],\n       [1863, 877],\n       [1862, 879],\n       [1864, 890],\n       [1849, 891],\n       [1845, 889],\n       [1845, 881],\n       [1843, 876],\n       [1840, 875],\n       [1838, 877],\n       [1840, 883],\n       [1840, 887],\n       [1837, 890],\n       [1835, 890],\n       [1831, 884],\n       [1828, 883],\n       [1824, 885],\n       [1820, 883],\n       [1816, 878],\n       [1810, 879],\n       [1805, 875],\n       [1802, 878],\n       [1798, 885],\n       [1794, 884],\n       [1791, 878],\n       [1789, 880],\n       [1788, 886],\n       [1785, 886],\n       [1782, 880],\n       [1778, 881],\n       [1773, 886],\n       [1775, 892],\n       [1773, 893],\n       [1764, 888],\n       [1758, 892],\n       [1748, 893],\n       [1744, 888],\n       [1735, 885],\n       [1726, 886],\n       [1725, 891],\n       [1720, 894],\n       [1709, 889],\n       [1706, 893],\n       [1699, 894],\n       [1696, 897],\n       [1688, 894],\n       [1687, 894],\n       [1689, 902],\n       [1679, 909],\n       [1676, 908],\n       [1671, 909],\n       [1666, 905],\n       [1664, 908],\n       [1659, 908],\n       [1656, 911],\n       [1651, 909],\n       [1646, 904],\n       [1643, 907],\n       [1629, 907],\n       [1627, 909],\n       [1620, 905],\n       [1611, 916],\n       [1602, 924],\n       [1582, 926],\n       [1576, 930],\n       [1568, 932],\n       [1568, 940],\n       [1563, 948],\n       [1562, 968],\n       [1564, 974],\n       [1564, 981],\n       [1570, 1000],\n       [1569, 1012],\n       [1579, 1031],\n       [1581, 1041],\n       [1580, 1055],\n       [1572, 1075],\n       [1573, 1080],\n       [1572, 1086],\n       [1577, 1095],\n       [1580, 1106],\n       [1579, 1112],\n       [1574, 1123],\n       [1573, 1127],\n       [1575, 1130],\n       [1387, 1300],\n       [1749, 1707]]]},\n    'properties': {'STATE_FIPS': '34',\n     'COUNTY_FIP': '001',\n     'FIPS': '34001',\n     'STATE': 'NJ',\n     'NAME': 'Atlantic',\n     'LSAD': 'County'},\n    'id': 2132,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[812, 261],\n       [817, 258],\n       [823, 260],\n       [830, 256],\n       [829, 253],\n       [832, 246],\n       [841, 253],\n       [852, 234],\n       [855, 234],\n       [863, 248],\n       [865, 247],\n       [867, 241],\n       [865, 234],\n       [872, 231],\n       [877, 233],\n       [881, 240],\n       [884, 239],\n       [887, 237],\n       [893, 238],\n       [897, 245],\n       [904, 244],\n       [909, 239],\n       [908, 233],\n       [925, 218],\n       [911, 191],\n       [922, 169],\n       [982, 102],\n       [1027, 41],\n       [1067, 0],\n       [1088, -29],\n       [1134, -61],\n       [1170, -75],\n       [1200, -74],\n       [1224, -63],\n       [1229, -53],\n       [1224, -36],\n       [1229, -35],\n       [1235, -47],\n       [1242, -80],\n       [319, -80],\n       [314, 0],\n       [313, 14],\n       [557, 24],\n       [566, 35],\n       [573, 39],\n       [577, 38],\n       [580, 39],\n       [585, 47],\n       [584, 51],\n       [607, 62],\n       [625, 105],\n       [632, 110],\n       [635, 109],\n       [640, 111],\n       [642, 113],\n       [645, 122],\n       [651, 125],\n       [655, 131],\n       [654, 143],\n       [659, 152],\n       [670, 156],\n       [688, 166],\n       [695, 162],\n       [716, 169],\n       [724, 169],\n       [732, 165],\n       [741, 169],\n       [751, 171],\n       [753, 174],\n       [757, 171],\n       [759, 179],\n       [770, 176],\n       [770, 182],\n       [775, 181],\n       [776, 177],\n       [779, 178],\n       [781, 180],\n       [778, 187],\n       [780, 192],\n       [783, 193],\n       [784, 195],\n       [781, 200],\n       [786, 213],\n       [783, 211],\n       [779, 215],\n       [783, 218],\n       [785, 221],\n       [781, 233],\n       [785, 238],\n       [792, 244],\n       [793, 250],\n       [796, 251],\n       [799, 247],\n       [808, 249],\n       [810, 260],\n       [812, 261]]]},\n    'properties': {'STATE_FIPS': '10',\n     'COUNTY_FIP': '005',\n     'FIPS': '10005',\n     'STATE': 'DE',\n     'NAME': 'Sussex',\n     'LSAD': 'County'},\n    'id': 2175,\n    'type': 'Feature'},\n   {'geometry': {'type': 'Polygon',\n     'coordinates': [[[4106, 4092],\n       [4117, 4081],\n       [4159, 4083],\n       [4176, 4076],\n       [4176, 3486],\n       [4096, 3462],\n       [3974, 3424],\n       [3896, 3406],\n       [3832, 3405],\n       [3827, 3408],\n       [3813, 3434],\n       [3766, 3424],\n       [3707, 3399],\n       [3661, 3384],\n       [3658, 3473],\n       [3661, 3473],\n       [3660, 3476],\n       [3660, 3492],\n       [3661, 3502],\n       [3615, 3828],\n       [3612, 3843],\n       [3602, 3864],\n       [3604, 3879],\n       [3597, 3881],\n       [3585, 3898],\n       [3583, 3941],\n       [3579, 3944],\n       [3574, 3953],\n       [3568, 3972],\n       [3567, 3971],\n       [3563, 3979],\n       [3554, 3986],\n       [3562, 4023],\n       [3571, 4031],\n       [3581, 4023],\n       [3603, 4015],\n       [3642, 4009],\n       [3652, 3999],\n       [3654, 3983],\n       [3687, 3981],\n       [3691, 3990],\n       [3690, 4023],\n       [3694, 4046],\n       [3696, 4048],\n       [3706, 4049],\n       [3733, 4014],\n       [3745, 4003],\n       [3764, 3993],\n       [3775, 3991],\n       [3777, 3995],\n       [3795, 3999],\n       [3848, 3989],\n       [3944, 3952],\n       [4061, 3998],\n       [4065, 4010],\n       [4067, 4050],\n       [4053, 4061],\n       [4046, 4074],\n       [4078, 4071],\n       [4096, 4085],\n       [4106, 4092]]]},\n    'properties': {'STATE_FIPS': '36',\n     'COUNTY_FIP': '103',\n     'FIPS': '36103',\n     'STATE': 'NY',\n     'NAME': 'Suffolk',\n     'LSAD': 'County'},\n    'id': 2231,\n    'type': 'Feature'}],\n  'type': 'FeatureCollection'}}"
  },
  {
    "objectID": "transition-from-rmarkdown.html",
    "href": "transition-from-rmarkdown.html",
    "title": "Transition from RMarkdown",
    "section": "",
    "text": "You may already have workflows in RMarkdown and are interested in transitioning to Quarto. There’s no hurry to migrate to Quarto. Keep using Rmarkdown and when you’re ready the migration will be fine.\nHere are some notes as we migrate RMarkdown sites and books.\nTODO: translating R code chunks"
  },
  {
    "objectID": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "href": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Bookdown to Quarto",
    "text": "Bookdown to Quarto\nConverting a Bookdown book to Quarto is slightly more involved than converting a website. A book has chapters whose order must be defined, and likely has citations and cross-refs. Still, conversion is not that hard.\nWe got some practice converting from Bookdown to Quarto by helping Gavin Fay convert his lab’s fantastic onboarding documentation, the Faylab Lab Manual. Here’s the GitHub view before and after.\nOur best first reference material for this was Nick Tierney’s Notes on Changing from Rmarkdown/Bookdown to Quarto. Nick shares some scripts in that post to automate some changes. In our case, the book was small enough that we made all changes manually. Quarto documentation was indispensable.\n\nExperimenting in a low-risk environment\nWe forked a copy of the Faylab Lab manual to the Openscapes organization, and worked in a branch so we could make changes relatively risk-free. We could always fork a new copy of the original if we “broke” something. (Caution: the default when making a pull request from a fork is to push changes to the original upstream repo, not your fork and it does this without warning if you have write-access to that repo.) With local previews it’s easy to test / play with settings to see what they do. We tended to make a change, Preview, then compare the look and functionality of the book to the original. It was helpful to comment out some elements of the configuration file _output.yml after their counterparts had been added to the Quarto configuration file _quarto.yml, or to confirm they were no longer needed, before making the drastic move of deleting them.\n\n\nThe conversion\nHere are the main steps to convert the Faylab Lab manual from Bookdown to Quarto.\nCreate new empty file called _quarto.yml and add book metadata there. The screenshots below\nSet project type as book.\nMove metadata out of index.qmd and into _quarto.yml. Title, author, and publication date were in index.qmd with date set using date: \"Last updated:r Sys.Date()\". Now these are in _quarto.yml with date set using date: last-modified. Note that having R code would require you to adjust code chunk options in the Quarto style (#|). This tripped us up a bit; see GitHub Actions.\nMove chapters listing out of _bookdown.yml and into _quarto.yml.\nAdd page footer to _quarto.yml.\nHere’s what ours looked like when we finished the steps above (_quarto.yml).\n\n\n\n\n\n\n\n\n\n_quarto.yml contents\n\n\n\n\n\n\n\nFaylab Lab Manual\n\n\n\n\n\nChange insertion of images from html style to Quarto style. (Note Quarto calls them “figures”, not “images”.) The following snippet will insert the GitHub octocat logo in a page:\n![](https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png){fig-align=\"left\" width=\"35px\"}\nChange all filename extensions .Rmd -&gt; .qmd (you could Preview after this change and see that the book looks the same). Note that Quarto works with .Rmd files just as well as it does .qmd, so this change is not urgent. In fact, if you have a lot of R code in your .Rmds (unlike the Faylab Lab Manual), there will be additional tinkering needed to make the code chunks happy.\n\n\nCitations\nThe Faylab Lab Manual cited two papers, presenting us with an opportunity to see how easy it is to add references to a Quarto book. Briefly, in the Visual Editor, Insert &gt; Citation &gt; DOI. Pasting the DOI or its full URL, we can insert the citation. This automatically creates a references.bib file and adds the full citations at the bottom of the chapter page (watch demo). In July 2022, we had to manually add a ## References heading, but this may not be necessary in future Quarto updates.\n\n\n\n\n\n\n\n\n\nInsert citation via its DOI using RStudio Visual Editor\n\n\n\n\n\n\n\n\n\n\nPublishing notes\nIf the book’s output is strictly html, there’s no need to specify output-dir in _quarto.yml. The output directory default is _book/, which is what we’d like. If we wanted other types of output like like PDF or EPUB, etc. those single file outputs are also written to the output-dir (Quarto docs).\nIf you currently have a docs/ folder, delete it.\nUpdate .gitignore to ignore _book/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_book/\nOnce all is settled, delete _output.yml.\nOnce the Openscapes fork was fully reviewed, we made a pull request from that to the main branch of the book’s repo. Once that was merged, we set up GitHub Actions to render the book. (TODO: instructions for GitHub Actions)\n\n\nGitHub Actions\nThis book was mostly prose and screenshots without any R code. This made the conversion from RMarkdown to Quarto likely more straightforward than if you also needed to adjust code chunk options in the quarto style (#|). Our initial GitHub Action to render the converted Faylab Lab Manual failed because we had a piece of R code - even though the code was commented out! This was resolved when we deleted the line."
  },
  {
    "objectID": "transition-from-rmarkdown.html#distill-to-quarto",
    "href": "transition-from-rmarkdown.html#distill-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Distill to quarto",
    "text": "Distill to quarto\nWe transitioned our events site from distill to quarto in May 2022 (github view before and after). We followed excellent notes and examples from Nick Tierney and Danielle Navarro.\nAfter we had changed all the files, the Build tab in the RStudio IDE still showed “Build website” rather then “Render Website” and “Preview Website”, and would error when we pushed them (because that button was expecting a distill site, not a quarto site). To fix this, we updated the .Rproj file. Clicking on the .Rproj file in the RStudio IDE will open a dialog box where you can click things you want (you can also open these in a text editor or from the GitHub website to see the actual text). To fix this situation with the Build tab: Project Options &gt; Build Tools &gt; Project Build Tools &gt; None.\nLooking at files /posts/_metadata.yml and _quarto.yml helps see where things are defined. For example, to make event post citations appear, we added citation: true to /posts/_metadata.yml and in _quarto.yml under the website key we set site-url: https://openscapes.github.io/events. We deleted footer.html used with distill because footer is now defined in quarto.yml.\n\nPublishing notes\n\nBackground: Our distill site had been set up to output to a docs folder, and had GitHub Settings &gt; Pages set to look there rather gh-pages branch. (Julie note: this was a new-to-me capability when we set up the events distill site in Spring 2021 so I had forgotten that was an option). We’ve inititally kept this same set-up for now with our events page in _quarto.yml: output-dir: docs. However, this is sub-optimal; better to not have to commit and push these files but to instead have a GitHub Action generate them upon a commit. So the following is what we did -\n\nDon’t specify output-dir in _quarto.yml. The output directory default is _site/, which is what we’d like.\nIf you currently have a docs/ folder (like we did as we were experimenting), delete it.\nUpdate .gitignore to ignore _site/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_site/\nPush these changes, merge into main.\nOn GitHub.com, in your repo, set up GitHub publishing\nFollow instructions from the explore and setup chapter."
  },
  {
    "objectID": "transition-from-rmarkdown.html#troubleshooting",
    "href": "transition-from-rmarkdown.html#troubleshooting",
    "title": "Transition from RMarkdown",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nGitHub Action fails, says you need RMarkdown but you don’t have R code!\nAnd you changed all .Rmds to .qmds!\nYou likely have a few setup code chunks from RMarkdown, that look like this:\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nYou can find them by opening each of your files and having a look, or use GitHub’s search for the keyword knitr"
  },
  {
    "objectID": "zarr/intro.html",
    "href": "zarr/intro.html",
    "title": "Zarr",
    "section": "",
    "text": "Zarr, despite its name, is not a scary format. It’s designed for data that is too big for users’ machines, but Zarr makes data small and organizes it in a way where users can take just the bits they need or distribute the load of processing lots of those bits (stored as chunks) across many machines.\nThe Zarr data format is a community-maintained format for large-scale n-dimensional data. A Zarr store consists of compressed and chunked n-dimensional arrays. Zarr’s flexible indexing and compatibility with object storage lends itself to parallel processing.\nA Zarr chunk is Zarr’s unit of data storage. Each chunk of a Zarr array is an equally-sized block of the array within a larger Zarr store comprised of one or more arrays and array groups. These blocks or chunks of data are stored separately to make reading and updating small chunks more efficient.\nRead more in the official tutorial: Zarr Tutorial\n\n\n\n\n\n\n\n\nImportant\n\n\n\nZarr Version 3 is underway but not released yet, so all the examples in this guide are for Zarr Version 2 data. The concepts in this page are consistent across both Zarr Version 2 and Zarr Version 3, however some metadata field names and organization are changing from Version 2 to version 3.\n\n\nVersion 3 changes from Version 2:\n\ndtype has been renamed to data_type,\nchunks has been replaced with chunk_grid,\ndimension_separator has been replaced with chunk_key_encoding,\norder has been replaced by the transpose codec,\nthe separate filters and compressor fields been combined into the single codecs field.\n\nRead more:\n\nZarr specification version 2\nZarr specification version 3.0\n\n\n\n\n\n\nZarr arrays are similar to numpy arrays, but chunked and compressed. We will add details about chunking and compression to this guide soon.\n\n\n\nZarr supports hierarchical organization via groups. Each node in the Zarr hierarchy is either a group or an array.\n\n\n\nA Zarr array has zero or more dimensions. A Zarr array’s shape is the tuple of the length of the array in each respective dimension.\n\n\n\nZarr indexing supports array subsetting (both reading and writing) without loading the whole array into memory. Advanced indexing operations, such as block indexing, are detailed in the Zarr tutorial: Advanced indexing.\n\n\n\n\n\n\nNote\n\n\n\nThe Zarr format is language-agnostic, but this indexing reference is specific to Python.\n\n\nThe Xarray library provides a rich API for slicing and subselecting data. In addition to providing a positional index to subselect data, xarray supports label-based indexing. Labels, or coordinates, in the case of geospatial data, often include latitude and longitude (or y and x). These coordinates (also called names or labels) can be used to read and write data when the position is unknown.\n\n\n\nEvery Zarr array has its own metadata. When considering cloud storage options, where latency is high so total requests should be limited, it is important to consolidate metadata so all metadata can be read from one object.\nRead more on consolidating metadata.\n\n\n\n\n\n\nZarr can be stored in memory, on disk, in Zip files, and in object storage like S3.\n\n\n\n\n\n\nNote\n\n\n\nAny backend that implements MutableMapping interface from the Python collections module can be used to store Zarr. Learn more and see all the options on the Storage (zarr.storage) documentation page.\n\n\nAs of Zarr version 2.5, Zarr store URLs can be passed to fsspec and it will create a MutableMapping automatically.\n\n\n\nChunking is the process of dividing the data arrays into smaller pieces. This allows for parallel processing and efficient storage.\nOnce data is chunked, applications may read in 1 or many chunks. Because the data is compressed, within-chunk reads are not possible.\n\n\n\nZarr supports compression algorithms to support efficient storage and retrieval.\nTo explore these concepts in practice, see the Zarr in Practice notebook.",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr"
    ]
  },
  {
    "objectID": "zarr/intro.html#zarr-version-2-and-version-3",
    "href": "zarr/intro.html#zarr-version-2-and-version-3",
    "title": "Zarr",
    "section": "",
    "text": "Important\n\n\n\nZarr Version 3 is underway but not released yet, so all the examples in this guide are for Zarr Version 2 data. The concepts in this page are consistent across both Zarr Version 2 and Zarr Version 3, however some metadata field names and organization are changing from Version 2 to version 3.\n\n\nVersion 3 changes from Version 2:\n\ndtype has been renamed to data_type,\nchunks has been replaced with chunk_grid,\ndimension_separator has been replaced with chunk_key_encoding,\norder has been replaced by the transpose codec,\nthe separate filters and compressor fields been combined into the single codecs field.\n\nRead more:\n\nZarr specification version 2\nZarr specification version 3.0",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr"
    ]
  },
  {
    "objectID": "zarr/intro.html#zarr-data-organization",
    "href": "zarr/intro.html#zarr-data-organization",
    "title": "Zarr",
    "section": "",
    "text": "Zarr arrays are similar to numpy arrays, but chunked and compressed. We will add details about chunking and compression to this guide soon.\n\n\n\nZarr supports hierarchical organization via groups. Each node in the Zarr hierarchy is either a group or an array.\n\n\n\nA Zarr array has zero or more dimensions. A Zarr array’s shape is the tuple of the length of the array in each respective dimension.\n\n\n\nZarr indexing supports array subsetting (both reading and writing) without loading the whole array into memory. Advanced indexing operations, such as block indexing, are detailed in the Zarr tutorial: Advanced indexing.\n\n\n\n\n\n\nNote\n\n\n\nThe Zarr format is language-agnostic, but this indexing reference is specific to Python.\n\n\nThe Xarray library provides a rich API for slicing and subselecting data. In addition to providing a positional index to subselect data, xarray supports label-based indexing. Labels, or coordinates, in the case of geospatial data, often include latitude and longitude (or y and x). These coordinates (also called names or labels) can be used to read and write data when the position is unknown.\n\n\n\nEvery Zarr array has its own metadata. When considering cloud storage options, where latency is high so total requests should be limited, it is important to consolidate metadata so all metadata can be read from one object.\nRead more on consolidating metadata.",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr"
    ]
  },
  {
    "objectID": "zarr/intro.html#zarr-data-storage",
    "href": "zarr/intro.html#zarr-data-storage",
    "title": "Zarr",
    "section": "",
    "text": "Zarr can be stored in memory, on disk, in Zip files, and in object storage like S3.\n\n\n\n\n\n\nNote\n\n\n\nAny backend that implements MutableMapping interface from the Python collections module can be used to store Zarr. Learn more and see all the options on the Storage (zarr.storage) documentation page.\n\n\nAs of Zarr version 2.5, Zarr store URLs can be passed to fsspec and it will create a MutableMapping automatically.\n\n\n\nChunking is the process of dividing the data arrays into smaller pieces. This allows for parallel processing and efficient storage.\nOnce data is chunked, applications may read in 1 or many chunks. Because the data is compressed, within-chunk reads are not possible.\n\n\n\nZarr supports compression algorithms to support efficient storage and retrieval.\nTo explore these concepts in practice, see the Zarr in Practice notebook.",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr"
    ]
  },
  {
    "objectID": "zarr/intro.html#what-zarr-is-not",
    "href": "zarr/intro.html#what-zarr-is-not",
    "title": "Zarr",
    "section": "What Zarr is not",
    "text": "What Zarr is not\nZarr is not designed for vector, point cloud or sparse data, although there is investigations into supporting a greater variety of data types.",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr"
    ]
  },
  {
    "objectID": "zarr/intro.html#zarr-is-in-development",
    "href": "zarr/intro.html#zarr-is-in-development",
    "title": "Zarr",
    "section": "Zarr is in Development",
    "text": "Zarr is in Development\nThere are some limitations of Zarr which is why there are Zarr Enhancement Proposals.\nZarr Version 3 was itself a ZEP, which has been accepted.\nDraft ZEPs are recommended reading for anyone considering creating a new Zarr store, since they address common challenges with Zarr data to date.",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This glossary aims to describe all the jargon of geospatial in the cloud! Are we missing something? Create an issue to suggest an improvement.\n\nAmazon S3 (S3)\nThe object storage service offered by Amazon. Part of Amazon Web Services.\n\n\nAmazon Web Services (AWS)\nCloud computing services offered by Amazon.\n\n\nArchive format\nA file format which stores one or more other files, possibly with compression. Examples include ZIP archives and PMTiles.\n\n\nArray Dimensions\nThe number of variables represented by an array. If an array represents longitude, latitude, time, and temperature, the array has four dimensions.\n\n\nAsynchronous\nA manner of scaling computing, to allow more operations to happen at the same time.\nThink of a glass of water. Synchronous computing is akin to having one straw: when you’ve finished drinking all you wish to drink, you give the straw to your friend for them to drink. Asynchronous computing is akin to sharing the straw between you and your friend. There’s still only one straw, but you can hand off sips. Parallel computing (like multithreading or multiprocessing) is like having two straws, where both you and your friend can drink out of the glass at the same time.\n\n\nBandwidth\nThe speed at which data travels over a network. Usually used in reference to downloading or uploading files.\nSee also: latency.\n\n\nChunk\nA grouping of data as part of a file format.\nIn a COG, this refers to a slice of the full array, usually 256 pixels high by 256 pixels wide (256x256), or 512 pixels high by 512 pixels wide (512x512).\nIn a GeoParquet file, this refers to a slice of a group of columns, where the slice has the same number of rows in each column.\n\n\nChunk size\nThe size of each chunk in a file format.\nThe chunk size plays a large part in how efficient random access within the file can be. If the chunk size is too small, then the metadata describing the file and the chunk byte ranges will be very large, and many HTTP range requests may have to be made for each small piece desired within the file. On the other hand, if the chunk size is too large, then a reader will have to read a large amount of data even for a very small query.\n\n\nCloud\nComputing services hosted by an external provider, where the provider pays for the upfront cost of buying hardware, earning a profit by selling services. This allows users to scale workloads efficiently because users do not need to pay large upfront costs for computers. These rented services can include compute time or object storage.\nUsually refers to services hosted by Amazon, Google, or Microsoft.\n\n\nCloud-Optimized\nThe property of a file format to be able to read a meaningful part of the file without needing to download all of the file. In particular, this means the file can be used efficiently from cloud storage via HTTP range requests.\n\n\nCloud-Optimized GeoTIFF (COG)\nAn extension of GeoTIFF with well-defined internal chunking, designed for efficient random access of the contained raster data.\n\n\nCloud-Optimized Point Cloud (COPC)\nA cloud-optimized file format for point cloud data.\n\n\nCompression\nAn algorithm that makes data smaller, at the cost of having to encode data into the compressed format before saving and having to decode data out of the compressed format before usage. In most cases, the benefits of smaller file sizes when stored outweigh the time it takes to encode and decode the compressed format.\nCompression can either be external or internal to a file, and can either be lossless or lossy.\n\n\nContent Delivery Network (CDN)\nA globally-distributed network of storage servers designed to cache HTTP requests so that future requests can use the cached copy instead of asking the origin server or storage.\n\n\nCoordinate Reference System (CRS)\nAlso called a projection\n\n\nData Type\nThe data type refers to the specific encoding in which values are stored in binary. Data types can be numeric or non-numeric, including string, binary, or nested data structures. The usual numeric data types used most often for scientific data include:\n\nByte or Int8: signed integer with 8 bit capacity, which can hold values from -128 to 127 (inclusive).\nUnsigned byte or Uint8: unsigned integer with 8 bit capacity, which can hold values from 0 to 255 (inclusive).\nInt or Int16: signed integer with 16 bit capacity, which can hold values from -32,768 to 32,767 (inclusive).\nUnsigned int or Uint16: unsigned integer with 16 bit capacity, which can hold values from 0 to 65,535 (inclusive).\nShort or Int32: signed integer with 32 bit capacity, which can hold values from -2,147,483,648 to 2,147,483,647 (inclusive).\nUnsigned short or Uint32: unsigned integer with 32 bit capacity, which can hold values from 0 to 4,294,967,295 (inclusive).\nLong or Int64: signed integer with 64 bit capacity, which can hold values from -9223372036854775808 to 9223372036854775807 (inclusive).\nUnsigned long or Uint64: unsigned integer with 64 bit capacity, which can hold values from 0 to 18446744073709551615 (inclusive).\nfloat: 32 bit floating point number.\ndouble: 64 bit floating point number.\n\nFor a good explainer on how floating point numbers work, refer to this blog post.\n\n\nDeflate\nA lossless compression codec used as part of ZIP archives and internally within COG and GeoTIFF files.\n\n\nEntwine Point Cloud (EPT)\nA cloud-optimized file format for point cloud data. Entwine has largely been superseded by COPC because COPC is backwards-compatible with previous point cloud data formats.\n\n\nEPSG code\nA projection definition referring to the EPSG database. EPSG codes tend to be four or five digits and tend to be easier to remember and use than longer definitions, such as WKT strings or PROJJSON. The downside of EPSG codes is that the program needs to have the EPSG database available so that it can perform a lookup from the EPSG code to the full projection definition.\n\n\nExternal compression\nCompression that is not part of a file format’s own specification, and which is added on after the main file has been saved. This tends to be used as part of ZIP archives (with file extension .zip) or with standalone gzip compression (file extension .gz).\nExternal compression tends to make a file no longer cloud-optimized, as it is usually no longer possible to read part of the file without fetching the entire file, as the entire file is necessary for decompression.\nThis is in contrast to internal compression.\n\n\nfsspec\nA Python library for abstracting across several different file storage solutions, including local file storage, cloud storage, and HTTP web urls. Allows uploading and downloading files to each backend with a consistent API.\n\n\nGDAL\nThe Geospatial Data Abstraction Library, a widely-used open-source library for converting between different raster data formats, as well as reprojecting between coordinate reference systems.\nIts common command-line tools include gdalinfo and gdal_translate. It can be used from Python with the rasterio library or from R with the terra library.\nGDAL includes OGR for processing vector data.\n\n\nGeoJSON\nA file format for vector data, built on top of JSON. GeoJSON is a common format for transferring vector data to web browsers, because it’s easy for most programming languages to read and write, but tends to have a large size. It’s not cloud-optimized because it can’t be partially parsed; the entire file needs to be downloaded in order to use.\n\n\nGeoPackage\nA file format for vector data. GeoPackage supports multiple layers as part of a single file. Because a GeoPackage is internally stored as a SQLite database, it is not cloud-optimized because the entire file must be downloaded in order to read any part of the file.\n\n\nGeoPandas\nA Python library for using and managing vector data, organized around geospatial data frames.\n\n\nGeoParquet\nAn extension of the Parquet file format to store geospatial vector data. Can be read and written by tools including GDAL and GeoPandas.\n\n\nGeospatial Data Frame\nA tabular data structure for storing geospatial vector data, where each geometry is paired with one or more attributes in a given row.\nA geospatial data frame structure works best when every feature has the same range of attributes, such as when there is a timestamp or other value associated with every geometry.\nGeoPandas in Python and sf in R are two common implementations of the geospatial data frame concept.\n\n\nGeoTIFF\nAn extension of TIFF to store geospatially-referenced image and raster data. Includes extra information such as the coordinate reference system and geotransform.\n\n\nGeotransform\nA set of six numbers that describe where a raster image lies within its coordinate reference system.\nThe geotransform describes the resolution and real-world location of each pixel. The geotransform needs to be used in conjunction with a projection definition for pixels to be located accurately.\nFor more information (in a Python context) read Python affine transforms.\n\n\nGoogle Cloud (GCP)\nCloud computing services provided by Google.\n\n\ngzip\nA type of lossless compression for general use. Gzip is based on the deflate algorithm and tends to be used standalone for external compression. Files that end with .gz have been encoded with gzip compression.\n\n\nHilbert curve\nA type of space-filling curve used as part of many spatial indexes that ensures that objects near each other in two-dimensional space (e.g. longitude-latitude) are also near each other when ordered in a file.\n\n\nHTTP Range Request\nHTTP is the protocol that governs how computers ask for data across a network. HTTP range requests is a part of the HTTP specification that defines how to ask for a specific byte range from a file, instead of the entire file.\nHTTP range requests is a core part of what makes a file format cloud-optimized, because it means that part of a geospatial data file can be read and used without needing to download the entire file.\n\n\nInternal compression\nCompression that is part of a file format’s own specification.\nFile formats such as COG, COPC, and GeoParquet include internal compression. Internal compression is useful for cloud-optimized data formats because it allows internal chunks to be fetched with range requests but still have smaller sizes from compression.\nFor files that have already been internally compressed, adding another layer of external compression, such as ZIP or gzip, will likely not make the file smaller, and only serve to reduce performance by requiring an extra decompression step before the data can be used.\n\n\nJPEG\nA lossy compression codec used for visual images. It tends to have a better compression ratio than lossless compression codecs like deflate or LZW.\n\n\nLatency\nThe time it takes for data to start being retrieved from a server.\nSee also: bandwidth.\n\n\nLERC\nLERC (Limited Error Raster Compression) is a lossy but very efficient compression algorithm for floating point raster data. This compression rounds values to a precision provided by the user and tends to be useful e.g. for elevation data where the source data is known to not have precision beyond a known value.\nLERC is a relatively new algorithm and may not be supported everywhere. For example, GDAL needs to be compiled with the LERC driver in order to load a GeoTIFF with LERC compression.\n\n\nLossless compression\nA type of compression where the exact original values can be recovered after decompression. This means that the compression process does not lose any information. Lossless compression codecs tend to give larger file sizes than lossy compression codecs.\nExamples include deflate, LZW, gzip, and ZSTD.\n\n\nLossy compression\nA type of compression where the exact original values cannot be recovered after decompression. This means that the compression process will lose information. Lossy compression codecs tend to give smaller file sizes than lossless compression codecs.\nExamples include LERC and JPEG.\n\n\nLZW\nA lossless compression codec for general use. It tends to be slightly slower than deflate.\n\n\nMapbox Vector Tile\nA file format for tiled vector data, usually used for visualization on web maps. PMTiles is a cloud-optimized archive format for storing millions of Mapbox Vector Tile files in an efficient manner, accessible via HTTP range requests.\n\n\nMetadata\nInformation about the actual data, saved as part of the file format. This allows for recreating the exact data that existed before saving and for cloud-optimized data formats, usually stores the byte ranges of relevant data sections within the file, which allows for using HTTP range requests for efficient random access to that data section.\n\n\nMicrosoft Azure\nCloud computing services offered by Microsoft.\n\n\nMulti-dimensional raster data\nA type of gridded raster data where multiple dimensions help conceptualize various attribtes. For example, a data value may exist for every longitude, latitude, time, and elevation, in which case the raster data would have four dimensions.\n\n\nMultithreading\nA manner of scaling computing, to allow more operations to happen at the same time.\nThink of a glass of water. Synchronous computing is akin to having one straw: when you’ve finished drinking all you wish to drink, you give the straw to your friend for them to drink. Asynchronous computing is akin to sharing the straw between you and your friend. There’s still only one straw, but you can hand off sips. Parallel computing (including multithreading and multiprocessing) is like having two straws, where both you and your friend can drink out of the glass at the same time.\n\n\nNumpy\nThe foundational Python library for managing multi-dimensional array data.\n\n\nObject storage (Cloud storage)\nObject storage, or cloud storage, refers to massively scalable cloud storage solutions like Amazon S3. It is relatively cheap, able to hold files small or large, and supports reading data via HTTP range requests. Most open geospatial data is hosted in such cloud storage solutions.\n\n\nOGR\nA widely-used open-source library for converting between different vector data formats, as well as reprojecting between coordinate reference systems.\nIts common command-line tools include ogrinfo and ogr2ogr. It can be used from Python with the pyogrio or fiona libraries or from R with the sf library.\nOGR is installed as part of GDAL.\n\n\nOverviews\nDownsampled (aggregated) data intended for visualization and stored as part of a file format. Overviews are part of the COG specification, and allow reading “zoomed out” data without needing to read and downsample from “full resolution” data.\nOverviews are also known as pyramids.\n\n\nParquet\nA file format for tabular data with internal chunking and internal compression. Data are stored per column instead of per row, making it very fast to select all data from a specific column.\n\n\nPDAL\nThe Point Data Abstraction Library, a widely-used open-source library for converting between different point cloud data formats and managing point cloud data.\n\n\nPMTiles\nA type of cloud-optimized archive format for tiled data. It can be used with either vector or raster tiled data, but is most often used with Mapbox Vector Tile files. The individual tiles in a PMTiles file are accessible via HTTP range requests.\n\n\nPoint Cloud Data\nA type of geospatial data storing three dimensional point locations along with attributes for each point. Point cloud data may come from LIDAR sensors or other photogrammetry, and may represent three-dimensional terrain or buildings.\n\n\nPROJJSON\nA projection definition that uses JSON for encoding.\nThe specification is defined as part of the PROJ project and used as part of the GeoParquet vector file format.\n\n\nRandom access\nThe ability to quickly fetch part of a file without reading the entire file.\nFor example, consider videos on YouTube. If you select to watch a video starting from the ten minute mark, YouTube does not need to download the video up to that point. Rather it is able to use the metadata from the video file to know what byte range in the video file corresponds to the ten minute mark, and then use HTTP range requests to download only the part of the video you’ve reqested.\nThe ability to perform efficient random access over a network is a core part of what makes data cloud-optimized.\n\n\nRaster data\nA type of geospatial data that stores regularly-gridded data with cells of known and constant size. This often comes from aerial or satellite imagery sensors.\n\n\nsf\nAn R library for using and managing vector data, organized around geospatial data frames.\n\n\nShapefile\nA vector data file format. There are many reasons to no longer use Shapefile.\n\n\nSpace-filling curve\nAn algorithm that translates two- or n- dimensional data into one-dimensional data. In practice, this is used as part of spatial indexes to group vector geometries nearby within a file according to their two-dimensional location.\nRead more at Wikipedia.\n\n\nSpatial index\nA data structure used for searching through spatial data more efficiently.\nFor further reading: A dive into spatial search algorithms.\n\n\nTagged Image File Format (TIFF)\nA file format for image and raster data that supports lossless compression.\n\n\nVector data\nA type of geospatial data to represent points, lines, and polygons.\n\n\nWeb Mercator\nA coordinate reference system often used with tiled data for web maps.\n\n\nWell-Known Binary (WKB)\nA binary encoding for vector geometries that many systems can read and write. For example, GeoParquet uses WKB in its definition of the geometry column.\n\n\nWKT (Geometry encoding)\nA text encoding for vector geometries that many systems can read and write. WKT tends to be larger in size and slower to read and write than WKB, so only use WKT if you need to store geometries in a text file, such as a CSV.\nNote that this is different than WKT (Projection definition).\n\n\nWKT (Projection definition)\nAn encoding to store coordinate reference system information.\nThere have been multiple versions of WKT; it is suggested to use WKT2 whenever possible.\n\n\nZarr\nA chunked, compressed file format for multi-dimensional raster data.\n\n\nZIP Archive\nA type of archive format that is used to group together existing files. It can also be used to apply external compression onto existing files.\n\n\nZSTD\nA very efficient lossless compression codec. ZSTD tends to give a very good compression ratio at very good performance, but may not be available everywhere. Check that your expected programs have access to ZSTD before using this on your data.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "",
    "text": "The packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda env create -f environment.yml\nor\nmamba env create -f environment.yml\nFinally, you may activate and select the kernel in the notebook (running in Jupyter)\nconda activate coguide-cog\nThe notebook has been tested to work with the listed Conda environment.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#environment",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#environment",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "",
    "text": "The packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda env create -f environment.yml\nor\nmamba env create -f environment.yml\nFinally, you may activate and select the kernel in the notebook (running in Jupyter)\nconda activate coguide-cog\nThe notebook has been tested to work with the listed Conda environment.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#setup",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#setup",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "Setup",
    "text": "Setup\nThis tutorial will explore accessing a regular GeoTIFF (Non-COG), converting it to Cloud-Optimized GeoTIFF (COG) format with Python and validate the data inside the COG and Non-COG.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#about-the-dataset",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#about-the-dataset",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "About the Dataset",
    "text": "About the Dataset\nWe will be using the NASADEM Merged DEM Global 1 arc second V001 from the NASA EarthData. To access NASA EarthData into Jupyter Notebook, you can create an account by visiting NASA’s Earthdata Login page. This will enable you to register for an account and retrieve the datasets used in the notebook.\nWe will use earthaccess library to set up credentials to fetch data from NASA’s EarthData catalog.\n\nimport earthaccess\nimport rasterio\nfrom rasterio.plot import show\nfrom rasterio.io import MemoryFile\nfrom rasterio.shutil import copy\nfrom rio_cogeo import cog_validate, cog_info\nfrom rio_cogeo.cogeo import cog_translate\nfrom rio_cogeo.profiles import cog_profiles\nimport os\nimport numpy\n\n/opt/homebrew/anaconda3/envs/coguide-cog/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nearthaccess.login()\n\n&lt;earthaccess.auth.Auth at 0x105269c10&gt;",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#creating-a-data-directory-for-this-tutorial",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#creating-a-data-directory-for-this-tutorial",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "Creating a Data Directory for this Tutorial",
    "text": "Creating a Data Directory for this Tutorial\nWe are creating a data directory for downloading all the required files locally.\n\n# set data directory path\ndata_dir = './data'\n\n# check if directory exists -&gt; if directory doesn't exist, directory is created\nif not os.path.exists(data_dir):\n    os.mkdir(data_dir)",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#downloading-the-dataset-from-earthdata",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#downloading-the-dataset-from-earthdata",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "Downloading the Dataset from EarthData",
    "text": "Downloading the Dataset from EarthData\nWe are using search_data method from the earthaccess module for searching the Granules from the selected collection.\n\n# Search Granules\nshort_name = 'NASADEM_HGT'\nversion = '001'\n\ndem_item_results = earthaccess.search_data(\n    short_name=short_name,\n    version=version,\n    count=2\n)\n\nGranules found: 14520\n\n\n\n# Download Data - Selecting the 2nd file from the `dem_item_results` list\nnasa_dem_files = earthaccess.download(dem_item_results[1], data_dir)\nnasa_dem_filename = f\"{nasa_dem_files[0]}\"\nprint(nasa_dem_filename)\n\n Getting 1 granules, approx download size: 0.01 GB\n\n\nQUEUEING TASKS | : 100%|██████████| 1/1 [00:00&lt;00:00, 871.09it/s]\n\n\nFile NASADEM_HGT_n57e105.zip already downloaded\n\n\nPROCESSING TASKS | : 100%|██████████| 1/1 [00:00&lt;00:00, 11214.72it/s]\nCOLLECTING RESULTS | : 100%|██████████| 1/1 [00:00&lt;00:00, 34379.54it/s]\n\n\ndata/NASADEM_HGT_n57e105.zip",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#loading-the-downloaded-file",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#loading-the-downloaded-file",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "Loading the Downloaded file",
    "text": "Loading the Downloaded file\nWe will use rasterio to read the downloaded zip file. The rasterio.open uses GDAL’s virtual filesystem interface to access local ZIP datasets. This requires prepending zip to the local zip file URI and adding the internal location to the dataset file after the ! character. For more details, refer here.\nIt is required to prepend zip:// for accessing the HGT file using a relative path. However, if we want to use an absolute path, then zip:/// should be preprended. Readers are encouraged to follow GDAL virtual file systems read capabilities for more details.\nAdditionally, we will be using rasterio.open in a rasterio.Env so as to define the GDAL_DRIVER_NAME for opening the SRTM files. GDAL has an in-built SRTMHGT driver for opening the SRTM HGT file formats. The SRTM zip files consists of HGT files named like nXXeYYY.hgt. For eg, the downloaded file NASADEM_HGT_n57e105.zip consists of n57e105.hgt. The readers are recommended to follow the SRTM documentation for more details.\n\n# Getting current working directory\nbase_dir = os.getcwd()\n\n#Extracting nXXXeXXX information from the filename\nzip_filename = nasa_dem_filename.split('/')[-1].split('_')[-1] # n57e105.hgt\nhgt_filename = zip_filename.replace('zip', 'hgt')\n\n# \"zip://data/NASADEM_HGT_n57e105.zip!n57e105.hgt\"\nhgt_file_path = \"zip://\" + nasa_dem_filename + f\"!{hgt_filename}\"\n\n\n#Reading the HGT file using `SRTMHGT` GDAL driver\n\nwith rasterio.Env(GDAL_DRIVER_NAME='SRTMHGT'):\n    with rasterio.open(hgt_file_path) as src:\n        arr = src.read()\n        kwargs = src.meta\n\nLet’s explore the dataset by printing the metadata and plotting it. Some of the parameter values from this metadata will be used later in this notebook while creating COGs, for eg. nodata value.\n\nprint(\"Metadata\", kwargs)\n\nMetadata {'driver': 'SRTMHGT', 'dtype': 'int16', 'nodata': -32768.0, 'width': 3601, 'height': 3601, 'count': 1, 'crs': CRS.from_epsg(4326), 'transform': Affine(0.0002777777777777778, 0.0, 104.99986111111112,\n       0.0, -0.0002777777777777778, 58.00013888888889)}\n\n\n\nfrom rasterio.plot import show\nshow(arr[0], cmap = 'terrain', title=\"NASADEM_HGT plot\")",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#converting-to-cloud-optimized-geotiff-cog",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#converting-to-cloud-optimized-geotiff-cog",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "Converting to Cloud-Optimized GeoTIFF (COG)",
    "text": "Converting to Cloud-Optimized GeoTIFF (COG)\nThere are multiple ways to write a COG in Python. We are presenting the recommended approach based on rio_cogeo.cog_translate method using the Memoryfile. This approach is found to be efficient for writing big GeoTIFF files in cloud-optimized format along with copying the overviews and input dataset metadata. Also, we use the default “deflate” COG profile for writing the COGs. This can be defined to any of the existing COG profiles.\nSome of the recommendations while generating COGs are noted below -\n\nUsing WEBP compression for RGB or RGBA dataset (there is a lossless option). Although, this is considered as a best option if you are looking for space saving, JPEG compression might be considered a safer choice.\nUsing Deflate compression with PREDICTOR=2 and ZLEVEL=9 options for non-Byte or non RGB datasets. The PREDICTOR value is set for LZW, DEFLATE and ZSTD compression. PREDICTOR=2 or standard predictor is used for integer data type and PREDICTOR=3 or floating-point predictor is used for floating point data type.\nUsing internal overviews.\nUsing a internal block size of 256 or 512\n\n\n# Defining the output COG filename\n# path = data/NASADEM_HGT_n57e105_COG.tif\ncog_filename = nasa_dem_filename.replace(\".zip\", \"_COG.tif\")\n\n# Setting to default GTiff driver as we will be using `rio-cogeo.cog_translate()`\n# predictor=2/standard predictor implies horizontal differencing\nkwargs.update(driver=\"GTiff\", predictor=2)\n\nwith MemoryFile() as memfile:\n    # Opening an empty MemoryFile for in memory operation - faster\n    with memfile.open(**kwargs) as mem:\n        # Writing the array values to MemoryFile using the rasterio.io module\n        # https://rasterio.readthedocs.io/en/stable/api/rasterio.io.html\n        mem.write(arr)\n\n        dst_profile = cog_profiles.get(\"deflate\")\n\n        # Creating destination COG\n        cog_translate(\n            mem,\n            cog_filename,\n            dst_profile,\n            use_cog_driver=True,\n            in_memory=False\n        )\n\n/var/folders/h7/l61pvww15kz03l7fxjhts_wr0000gp/T/ipykernel_40851/4074752627.py:18: RasterioDeprecationWarning: Source dataset should be opened in read-only mode. Use of datasets opened in modes other than 'r' will be disallowed in a future version.\n  cog_translate(\nReading input: &lt;open DatasetWriter name='/vsimem/ae8e0898-8d08-4394-a034-817d492e0e04/ae8e0898-8d08-4394-a034-817d492e0e04.tif' mode='w+'&gt;\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: data/NASADEM_HGT_n57e105_COG.tif",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#validating-the-generated-cog",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#validating-the-generated-cog",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "Validating the Generated COG",
    "text": "Validating the Generated COG\nWe can validate the generated COG using the rio_cogeo.cog_validate. The cog_validate method returns following outputs: * is_valid: bool\nTrue if ‘src_path’ is a valid COG. * errors: list\nList of validation errors if is_valid returns False. * warnings: list\nList of validation warnings if is_valid returns False.\n\ncog_validate(cog_filename)\n\n(True, [], [])\n\n\nAs we can see, the generated COG is a valid COG.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#run-validation-tests",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#run-validation-tests",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "Run validation tests",
    "text": "Run validation tests\nThe cog_validate method runs on the COG file. Let’s try to run a validation test on the arrays representing the COG and Non-COG file using Numpy’s assert_array_equal method.\n\n# Reading the generated COG file\nwith rasterio.open(cog_filename) as src:\n   arr_cog = src.read()\n\n\n# Would not output anything if both the COG and non-COG files have equal values\nnumpy.testing.assert_array_equal(arr, arr_cog)",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#generating-overviews-and-setting-nodata-values",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#generating-overviews-and-setting-nodata-values",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "Generating Overviews and Setting NoData Values",
    "text": "Generating Overviews and Setting NoData Values\nIt is recommended to generate COGs with overviews and define nodata values in the COGs. The Cloud Optimized Geotiff (COG) Overview Resampling notebook provides in-depth explanation for generating and visualizing overviews. Here, we will see how to -\n\nGenerate COGs with overviews in Python.\n\nOverviews are decimated views created for visualization. By default, rio-cogeo calculates optimal overview levels based on dataset size and internal tile size, since overviews should not be smaller than internal tile size.\n\nSet nodata values while generating COGs.\n\nTypically, while creating a COG from a Non-COG TIFF, the nodata value can be set to the value defined in the source GeoTIFF metadata (in this case -32768.0). Otherwise, the nodata value can be set based on the datatype used for storing the data. For int type, the largest negative value or -9999 is used. Similarly, float type has a nodata option and Byte has a mask band to define the nodata value.\n\n# Defining the output COG filename\n# path = data/NASADEM_HGT_n57e105_COG.tif\ncog_ovr_filename = nasa_dem_filename.replace(\".zip\", \"_ovr_COG.tif\")\n\n# Setting to default GTiff driver as we will be using `rio-cogeo.cog_translate()`\n# predictor=2 for INT type\nkwargs.update(driver=\"GTiff\", predictor=2)\n\n# Setting the recommended blocksize for internal tiling to 512 per recommendations\nkwargs.update({\"blockxsize\": \"512\", \"blockysize\": \"512\"})\n\n# Setting the overview level, by default it is inferred from the data size\n# Uncomment below to generate overviews on 2 levels\n# kwargs.update({\"overview_level\": 2})\n\nwith MemoryFile() as memfile:\n    # Opening an empty MemoryFile for in memory operation - faster\n    with memfile.open(**kwargs) as mem:\n        # Writing the array values to MemoryFile using the rasterio.io module\n        # https://rasterio.readthedocs.io/en/stable/api/rasterio.io.html\n        mem.write(arr)\n\n        dst_profile = cog_profiles.get(\"deflate\")\n\n        # Creating destination COG\n        cog_translate(\n            mem,\n            cog_ovr_filename,\n            dst_profile,\n            nodata=-32768.0,    #Set NoData= -32768.0 from the metadata\n            use_cog_driver=True,\n            in_memory=False\n        )\n\n/var/folders/h7/l61pvww15kz03l7fxjhts_wr0000gp/T/ipykernel_40851/3228781361.py:26: RasterioDeprecationWarning: Source dataset should be opened in read-only mode. Use of datasets opened in modes other than 'r' will be disallowed in a future version.\n  cog_translate(\nReading input: &lt;open DatasetWriter name='/vsimem/4fa25e3f-7741-4f4b-849a-fa1b295b6527/4fa25e3f-7741-4f4b-849a-fa1b295b6527.tif' mode='w+'&gt;\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: data/NASADEM_HGT_n57e105_ovr_COG.tif",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/writing-cogs-in-python.html#see-more",
    "href": "cloud-optimized-geotiffs/writing-cogs-in-python.html#see-more",
    "title": "Writing Cloud Optimized Geotiffs (COGs) with Python",
    "section": "See More",
    "text": "See More\n\nDo you really want people using your data?\nCOG Specification\nrio-cogeo",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Writing Cloud Optimized Geotiffs (COGs) with Python"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-details.html",
    "href": "cloud-optimized-geotiffs/cogs-details.html",
    "title": "Advanced COG/GeoTIFF Details",
    "section": "",
    "text": "The COG Intro page describes what makes a Cloud-Optimized GeoTIFF different from a plain, non-optimized GeoTIFF. The rest of this page details additional useful information (applicable to both COGs and plain GeoTIFF files) that can be relevant for making your files as useful and efficient as possible. Any reference to “GeoTIFF” below applies both to plain GeoTIFF files and to COG files.\n\n\nRecommendation The smallest possible data type that still represents the data appropriately should be used. It is not generally recommended to shift or quantize data from float to integer by multiplying, a space saving technique, as end users then need to undo this step to use the data. Data compression is preferred, see also Compression.\nThe GeoTIFF format supports many data types. The key is that all bands must be of the same data type. Unlike some other formats, you cannot mix and match integer (whole number) and float (decimal number) data types in the same file. If you have this use case consider splitting files by data type and using a catalog like STAC to keep track of them, or look at other formats like Zarr.\nScenario: If the COG is intended only for visualization, conversion to 3 band byte will improve performance.\n\nGDAL supported Data Types list\n\n\n\n\nThe compression used in a GeoTIFF is the greatest determinator of the file’s size. If storing a large amount of data, the right compression choice can lead to much smaller file sizes on disk, leading to lower costs.\n\n\nGeoTIFFs have compression internal to the file, meaning that the internal blocks in a GeoTIFF are already compressed. This internal compression is especially useful for COGs, compared to external compression (such as saving a COG inside a ZIP file), since a COG reader can decompress only the specific portion of the file requested, instead of needing to decompress the entire file.\nThe internal compression of a GeoTIFF also means that it does not need additional compression, and indeed that additional compression will decrease performance. Gzip or ZIP compression applied to a GeoTIFF with internal compression will not make the file smaller.\nIt is possible but not recommended to create COGs or GeoTIFFs with no internal compression.\n\n\n\nThere are a variety of compression codecs supported by GeoTIFF. Compression codecs tend to be split into two camps: lossy compression where the exact original values cannot be recovered or lossless compression which does not lose any information through the compression and decompression process. For most cases, a lossless compression is recommended, but in some cases a lossy compression can be useful and lead to smaller file sizes, such as if the COG is intended to be used only for visualization.\nDeflate or LZW are both lossless compression codecs and are the recommended algorithms for general use.\nJPEG is a lossy compression codec useful for true-color GeoTIFFs intended to be used only for visualization. Because it’s lossy, it tends to produce smaller file sizes than deflate or LZW. JPEG should only be used with RGB Byte data.\nLERC (Limited Error Raster Compression) is a very efficient compression algorithm for floating point data. This compression rounds values to a precision provided by the user and tends to be useful for data, such as elevation, where the source data is known to have precision to a known value. But note, this compression is not lossless when used with a precision greater than 0. Additionally, LERC is a relatively new algorithm and may not be supported everywhere. GDAL needs to be compiled with the LERC driver in order to load a GeoTIFF with LERC compression.\nSome other compression algorithms may be preferred depending on the data type and distribution, and if the goal is maximum compression or not. Codecs that produce the smallest file sizes tend to take longer to read into memory. If the network bandwidth to load the file is slow, then a very efficient compression algorithm may be most efficient, even if it takes longer to decompress when loaded. Alternatively, if the network speed is very fast, or if reading from a local disk, a slightly less efficient compression codec that decompresses faster may be preferred.\nThere are many posts on the internet exploring GeoTIFF compression and performance:\n\nGuide to GeoTIFF compression and optimization with GDAL\nGeoTiff Compression for Dummies\nGeoTiff compression comparison\n\n\n\n\n\nSetting a no data value makes it clear to users and visualization tools what pixels are not actually data. For visualization this allows these pixels to be easily hidden (transparent). Historically many values have been used, 0, -9999, etc… The key is to make sure the GDAL flag for no data is set. It is also suggested that the smallest negative value be used instead of a random value. For byte and unsigned integers data types this will be 0. For float data, setting NaN as the no data value is suggested. Make sure to use a no data value that does not have meaning in your data; otherwise use a different value (like the max possible value). Having the right nodata flag set is important for overview generation.\n\n\n\nRead performance can be greatly impacted by the choice of projection and the particular applications used for dynamic tile serving. Using a known CRS defined in the PROJ database (typically EPSG code) is preferred over custom projections. Load times can be 5-20 times greater when using a custom projection. Whenever applying projections make sure to use WKT2 representation. If using a database of known projections, i.e. EPSG codes, this should be fine, there are known issues around manually setting proj-strings.\n\n\n\nUp to this point, we’ve mentioned that a COG has internal tiling, but the exact layout of those internal tiles has been unspecified. A web-optimized COG exploits that internal tiling to enforce the same tiling as used in web maps. Additionally, overviews will also be aligned to the Web Mercator grid. Thus, a web-optimized COG is especially useful for relaying tiled image data to a browser. Because the internal tile layout exactly matches the tiling structure required in web mapping libraries, only one HTTP range request needs to be performed to access any tile.\nDownsides of web-optimized COGs include the fact that they must be in Web Mercator projection and that the file most likely will not line up exactly with the bounds of the original image data. This means that the COG will have a “collar” of invalid data around the edges of the file.\nWeb-Optimized COGs can be created via GDAL by using the COG driver with the creation option TILING_SCHEME=GoogleMapsCompatible or with rio-cogeo with the --web-optimized flag.\n\n\n\n\nThe optimum size of data at which splitting across files improves performance as a multi-file dataset instead of a single file.\nWhen to recommend particular internal tile sizes\nCompression impacts on http transfer rates.\nSupport for COG creation in all common geospatial tools varies.\n\n\n\n\n\nAn Introduction to Cloud Optimized GeoTIFFS (COGs) Part 1: Overview\nDo you really want people using your data?",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Advanced COG/GeoTIFF Details"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-details.html#data-type",
    "href": "cloud-optimized-geotiffs/cogs-details.html#data-type",
    "title": "Advanced COG/GeoTIFF Details",
    "section": "",
    "text": "Recommendation The smallest possible data type that still represents the data appropriately should be used. It is not generally recommended to shift or quantize data from float to integer by multiplying, a space saving technique, as end users then need to undo this step to use the data. Data compression is preferred, see also Compression.\nThe GeoTIFF format supports many data types. The key is that all bands must be of the same data type. Unlike some other formats, you cannot mix and match integer (whole number) and float (decimal number) data types in the same file. If you have this use case consider splitting files by data type and using a catalog like STAC to keep track of them, or look at other formats like Zarr.\nScenario: If the COG is intended only for visualization, conversion to 3 band byte will improve performance.\n\nGDAL supported Data Types list",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Advanced COG/GeoTIFF Details"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-details.html#compression",
    "href": "cloud-optimized-geotiffs/cogs-details.html#compression",
    "title": "Advanced COG/GeoTIFF Details",
    "section": "",
    "text": "The compression used in a GeoTIFF is the greatest determinator of the file’s size. If storing a large amount of data, the right compression choice can lead to much smaller file sizes on disk, leading to lower costs.\n\n\nGeoTIFFs have compression internal to the file, meaning that the internal blocks in a GeoTIFF are already compressed. This internal compression is especially useful for COGs, compared to external compression (such as saving a COG inside a ZIP file), since a COG reader can decompress only the specific portion of the file requested, instead of needing to decompress the entire file.\nThe internal compression of a GeoTIFF also means that it does not need additional compression, and indeed that additional compression will decrease performance. Gzip or ZIP compression applied to a GeoTIFF with internal compression will not make the file smaller.\nIt is possible but not recommended to create COGs or GeoTIFFs with no internal compression.\n\n\n\nThere are a variety of compression codecs supported by GeoTIFF. Compression codecs tend to be split into two camps: lossy compression where the exact original values cannot be recovered or lossless compression which does not lose any information through the compression and decompression process. For most cases, a lossless compression is recommended, but in some cases a lossy compression can be useful and lead to smaller file sizes, such as if the COG is intended to be used only for visualization.\nDeflate or LZW are both lossless compression codecs and are the recommended algorithms for general use.\nJPEG is a lossy compression codec useful for true-color GeoTIFFs intended to be used only for visualization. Because it’s lossy, it tends to produce smaller file sizes than deflate or LZW. JPEG should only be used with RGB Byte data.\nLERC (Limited Error Raster Compression) is a very efficient compression algorithm for floating point data. This compression rounds values to a precision provided by the user and tends to be useful for data, such as elevation, where the source data is known to have precision to a known value. But note, this compression is not lossless when used with a precision greater than 0. Additionally, LERC is a relatively new algorithm and may not be supported everywhere. GDAL needs to be compiled with the LERC driver in order to load a GeoTIFF with LERC compression.\nSome other compression algorithms may be preferred depending on the data type and distribution, and if the goal is maximum compression or not. Codecs that produce the smallest file sizes tend to take longer to read into memory. If the network bandwidth to load the file is slow, then a very efficient compression algorithm may be most efficient, even if it takes longer to decompress when loaded. Alternatively, if the network speed is very fast, or if reading from a local disk, a slightly less efficient compression codec that decompresses faster may be preferred.\nThere are many posts on the internet exploring GeoTIFF compression and performance:\n\nGuide to GeoTIFF compression and optimization with GDAL\nGeoTiff Compression for Dummies\nGeoTiff compression comparison",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Advanced COG/GeoTIFF Details"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-details.html#no-data",
    "href": "cloud-optimized-geotiffs/cogs-details.html#no-data",
    "title": "Advanced COG/GeoTIFF Details",
    "section": "",
    "text": "Setting a no data value makes it clear to users and visualization tools what pixels are not actually data. For visualization this allows these pixels to be easily hidden (transparent). Historically many values have been used, 0, -9999, etc… The key is to make sure the GDAL flag for no data is set. It is also suggested that the smallest negative value be used instead of a random value. For byte and unsigned integers data types this will be 0. For float data, setting NaN as the no data value is suggested. Make sure to use a no data value that does not have meaning in your data; otherwise use a different value (like the max possible value). Having the right nodata flag set is important for overview generation.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Advanced COG/GeoTIFF Details"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-details.html#projection",
    "href": "cloud-optimized-geotiffs/cogs-details.html#projection",
    "title": "Advanced COG/GeoTIFF Details",
    "section": "",
    "text": "Read performance can be greatly impacted by the choice of projection and the particular applications used for dynamic tile serving. Using a known CRS defined in the PROJ database (typically EPSG code) is preferred over custom projections. Load times can be 5-20 times greater when using a custom projection. Whenever applying projections make sure to use WKT2 representation. If using a database of known projections, i.e. EPSG codes, this should be fine, there are known issues around manually setting proj-strings.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Advanced COG/GeoTIFF Details"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-details.html#web-optimized-cog",
    "href": "cloud-optimized-geotiffs/cogs-details.html#web-optimized-cog",
    "title": "Advanced COG/GeoTIFF Details",
    "section": "",
    "text": "Up to this point, we’ve mentioned that a COG has internal tiling, but the exact layout of those internal tiles has been unspecified. A web-optimized COG exploits that internal tiling to enforce the same tiling as used in web maps. Additionally, overviews will also be aligned to the Web Mercator grid. Thus, a web-optimized COG is especially useful for relaying tiled image data to a browser. Because the internal tile layout exactly matches the tiling structure required in web mapping libraries, only one HTTP range request needs to be performed to access any tile.\nDownsides of web-optimized COGs include the fact that they must be in Web Mercator projection and that the file most likely will not line up exactly with the bounds of the original image data. This means that the COG will have a “collar” of invalid data around the edges of the file.\nWeb-Optimized COGs can be created via GDAL by using the COG driver with the creation option TILING_SCHEME=GoogleMapsCompatible or with rio-cogeo with the --web-optimized flag.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Advanced COG/GeoTIFF Details"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-details.html#what-we-dont-know-areas-of-research",
    "href": "cloud-optimized-geotiffs/cogs-details.html#what-we-dont-know-areas-of-research",
    "title": "Advanced COG/GeoTIFF Details",
    "section": "",
    "text": "The optimum size of data at which splitting across files improves performance as a multi-file dataset instead of a single file.\nWhen to recommend particular internal tile sizes\nCompression impacts on http transfer rates.\nSupport for COG creation in all common geospatial tools varies.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Advanced COG/GeoTIFF Details"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-details.html#additional-resources",
    "href": "cloud-optimized-geotiffs/cogs-details.html#additional-resources",
    "title": "Advanced COG/GeoTIFF Details",
    "section": "",
    "text": "An Introduction to Cloud Optimized GeoTIFFS (COGs) Part 1: Overview\nDo you really want people using your data?",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Advanced COG/GeoTIFF Details"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-overview_resampling.html",
    "href": "cloud-optimized-geotiffs/cogs-overview_resampling.html",
    "title": "Cloud Optimized Geotiff (COG) Overview Resampling",
    "section": "",
    "text": "When making Cloud-Optimized GeoTIFFs (COGs), you can select the resampling method to generate the overviews. Different types of data (e.g., nominal, ordinal, discrete, continuous) can drastically change appearance when more zoomed out based on the chosen method. This is most important when using software that renders from overviews (e.g., QGIS, ArcGIS), particularly web tilers like Titiler.\nAlso, when using a tiling service on a high-resolution dataset, you will often have many tiles of source data. The idea behind this notebook is that before you generate all your final output data, as COGs, or if you need to rebuild your overviews, you should first test on some representative samples. After testing, you’ll know which method to use in your data pipeline.\nThis notebook loops over the overview resampling methods available in GDAL applying it to the same sample tile to compare how the dataset will appear when zoomed out from the full resolution.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud Optimized Geotiff (COG) Overview Resampling"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-overview_resampling.html#setup-environment",
    "href": "cloud-optimized-geotiffs/cogs-overview_resampling.html#setup-environment",
    "title": "Cloud Optimized Geotiff (COG) Overview Resampling",
    "section": "Setup Environment",
    "text": "Setup Environment\n\nRequires gdal &gt;= 3.2 for Overview Resampling to work\nRMS resampling was added in GDAL 3.3\n\nThe packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda env create -f environment.yml\nOr:\nmamba env create -f environment.yml\nOr you can install the packages you need manually:\nmamba create -q -y -n coguide-cog -c conda-forge 'gdal&gt;=3.3' 'rasterio&gt;=1.3.3' 'rio-cogeo=3.5.0' ipykernel matplotlib earthaccess\nThen, you may activate and select the kernel in the notebook (running in Jupyter):\nconda activate coguide-cog\nThis notebook has been tested to work with the listed Conda environment.\nRemember to switch your notebook kernel if you made a new env; you may need to activate the new env first.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud Optimized Geotiff (COG) Overview Resampling"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-overview_resampling.html#setup-the-code",
    "href": "cloud-optimized-geotiffs/cogs-overview_resampling.html#setup-the-code",
    "title": "Cloud Optimized Geotiff (COG) Overview Resampling",
    "section": "Setup the code",
    "text": "Setup the code\nFirst, we should verify our GDAL version:\n\n!gdalinfo --version\n\nGDAL 3.7.3, released 2023/10/30\n\n\n\nimport os\n\nimport earthaccess\nimport matplotlib.pyplot as plt\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nfrom rasterio.plot import show\nfrom rio_cogeo import cog_translate, cog_profiles\n\n/home/ochotona/anaconda3/envs/coguide-cog/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud Optimized Geotiff (COG) Overview Resampling"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-overview_resampling.html#boreal-biomass-tiled-data",
    "href": "cloud-optimized-geotiffs/cogs-overview_resampling.html#boreal-biomass-tiled-data",
    "title": "Cloud Optimized Geotiff (COG) Overview Resampling",
    "section": "Boreal Biomass Tiled Data",
    "text": "Boreal Biomass Tiled Data\nThe example data comes from the Aboveground Biomass Density for High Latitude Forests from ICESat-2, 2020\nTo access and integrate NASA Earth data into your Jupyter Notebook, you can create an account by visiting NASA’s Earthdata Login page. This will enable you to register for an account and retrieve the datasets used in the notebook.\nYou can see the whole mosaic of all the data on the MAAP Biomass dashboard\nFor this example, we will pull a single 3000x3000 pixel tile that is part of the larger dataset of more than 3500 tiles.\nWe must retrieve a NASA Earth data session for downloading data:\n\nearthaccess.login(strategy=\"interactive\")\n\n&lt;earthaccess.auth.Auth at 0x7faa304b5e10&gt;\n\n\nWe can now query NASA CMR to retrieve a download URL:\n\nshort_name = 'Boreal_AGB_Density_ICESat2_2186'\nitem = 'Boreal_AGB_Density_ICESat2.boreal_agb_202302061675666220_3741.tif'\n\nitem_results = earthaccess.search_data(\n    short_name=short_name,\n    cloud_hosted=True,\n    granule_name=item\n    \n)\n\nGranules found: 1\n\n\nFinally, we can download the COG for local use:\n\ntest_data_dir = \"./test_data\"\nos.makedirs(test_data_dir, exist_ok=True)\n\nsample_files = earthaccess.download(item_results, test_data_dir)\ntile = f\"{sample_files[0]}\"\n\n Getting 1 granules, approx download size: 0.09 GB\n\n\nQUEUEING TASKS | : 100%|██████████| 2/2 [00:00&lt;00:00, 1733.18it/s]\n\n\nFile boreal_agb_202302061675666220_3741.tif already downloaded\nFile boreal_agb_202302061675666220_3741.tif.sha256 already downloaded\n\n\nPROCESSING TASKS | : 100%|██████████| 2/2 [00:00&lt;00:00, 15621.24it/s]\nCOLLECTING RESULTS | : 100%|██████████| 2/2 [00:00&lt;00:00, 34239.22it/s]\n\n\n\nNote: We downloaded the file because, it’s small (less than 100 MB), and we’re going to use the whole file several times.\n\nUsing rio cogeo, we can investigate if the file has tiles and what resampling was used:\n\n!rio cogeo info {tile}\n\nDriver: GTiff\nFile: /home/ochotona/code/devseed/cloud-optimized-geospatial-formats-guide/cloud-optimized-geotiffs/test_data/boreal_agb_202302061675666220_3741.tif\nCOG: True\nCompression: LZW\nColorSpace: None\n\nProfile\n    Width:            3000\n    Height:           3000\n    Bands:            2\n    Tiled:            True\n    Dtype:            float32\n    NoData:           -9999.0\n    Alpha Band:       False\n    Internal Mask:    False\n    Interleave:       PIXEL\n    ColorMap:         False\n    ColorInterp:      ('gray', 'undefined')\n    Scales:           (1.0, 1.0)\n    Offsets:          (0.0, 0.0)\n\nGeo\n    Crs:              PROJCS[\"unnamed\",GEOGCS[\"GRS 1980(IUGG, 1980)\",DATUM[\"unknown\",SPHEROID[\"GRS80\",6378137,298.257222101],TOWGS84[0,0,0,0,0,0,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",40],PARAMETER[\"longitude_of_center\",180],PARAMETER[\"standard_parallel_1\",50],PARAMETER[\"standard_parallel_2\",70],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n    Origin:           (1448521.9999999953, 2853304.0000000093)\n    Resolution:       (30.0, -30.0)\n    BoundingBox:      (1448521.9999999953, 2763304.0000000093, 1538521.9999999953, 2853304.0000000093)\n    MinZoom:          7\n    MaxZoom:          11\n\nImage Metadata\n    AREA_OR_POINT: Area\n\nImage Structure\n    COMPRESSION: LZW\n    INTERLEAVE: PIXEL\n\nBand 1\n    Description: aboveground_biomass_density (Mg ha-1)\n    ColorInterp: gray\n    Metadata:\n        STATISTICS_MAXIMUM: 347.71490478516\n        STATISTICS_MEAN: 44.388942987808\n        STATISTICS_MINIMUM: 2.5594062805176\n        STATISTICS_STDDEV: 30.371053229248\n        STATISTICS_VALID_PERCENT: 86.55\n\nBand 2\n    Description: standard_deviation\n    ColorInterp: undefined\n    Metadata:\n        STATISTICS_MAXIMUM: 121.33344268799\n        STATISTICS_MEAN: 6.1550359252236\n        STATISTICS_MINIMUM: 0.25763747096062\n        STATISTICS_STDDEV: 3.7940122373131\n        STATISTICS_VALID_PERCENT: 86.55\n\nIFD\n    Id      Size           BlockSize     Decimation           \n    0       3000x3000      256x256       0\n    1       1500x1500      128x128       2\n    2       750x750        128x128       4\n    3       375x375        128x128       8",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud Optimized Geotiff (COG) Overview Resampling"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-overview_resampling.html#generate-overviews",
    "href": "cloud-optimized-geotiffs/cogs-overview_resampling.html#generate-overviews",
    "title": "Cloud Optimized Geotiff (COG) Overview Resampling",
    "section": "Generate Overviews",
    "text": "Generate Overviews\nNow, let’s generate overviews with each resampling method possible in GDAL.\n\ndef generate_overview(src_path: str, out_dir: str, resample_method: str) -&gt; str:\n    ''' \n    Create a copy of original GeoTiff as COG with different overview resampling method\n    src_path = the original GeotTiff\n    out_dir = the folder for outputs (can differ from src)\n    method = the resampling method\n    return = the path to the new file\n    '''\n    #make sure the output folder exists\n    os.makedirs(out_dir, exist_ok=True)\n    dst_path = src_path.replace(\".tif\", f\"_{resample_method}.tif\")\n    dst_path = f\"{out_dir}/{os.path.basename(dst_path)}\"\n    \n    # Using multiple CPUS\n    # Using blocksize 512 per recommendations\n    config = {\"GDAL_NUM_THREADS\": \"ALL_CPUS\", \"GDAL_TIFF_OVR_BLOCKSIZE\": \"512\"} \n    output_profile = cog_profiles.get(\"deflate\")\n    output_profile.update({\"blockxsize\": \"512\", \"blockysize\": \"512\"})\n    \n    print(f\"Creating COG using '{resample_method}' method: {dst_path}\")\n    cog_translate(\n        src_path,\n        dst_path,\n        output_profile,\n        config=config,\n        overview_resampling=resample_method,\n        forward_band_tags=True,\n        use_cog_driver=True,\n        quiet=True,\n    )\n\n    return dst_path\n\nNow, we can make a list of the resampling methods offered by GDAL. Some resampling methods aren’t appropriate for the data, so we are doing to drop those from the list. You can see descriptions at https://gdal.org/programs/gdalwarp.html#cmdoption-gdalwarp-r\n\n# Make a list of resampling methods that GDAL 3.4+ allows\n\nfrom rasterio.enums import Resampling\n\n# Drop some irrelevant methods\nexcluded_resamplings = {\n    Resampling.max,\n    Resampling.min,\n    Resampling.med,\n    Resampling.q1,\n    Resampling.q3,\n}\noverview_resampling_methods = [method for method in Resampling if method not in excluded_resamplings]\nresample_methods = [method.name for method in overview_resampling_methods]\nprint(resample_methods)\n\n['nearest', 'bilinear', 'cubic', 'cubic_spline', 'lanczos', 'average', 'mode', 'gauss', 'sum', 'rms']\n\n\nFor each resampling method, we can create a copy of the data with it’s given overview method:\n\nfiles = [generate_overview(tile, test_data_dir, resample_method) for resample_method in resample_methods] \n\nCreating COG using 'nearest' method: ./test_data/boreal_agb_202302061675666220_3741_nearest.tif\nCreating COG using 'bilinear' method: ./test_data/boreal_agb_202302061675666220_3741_bilinear.tif\nCreating COG using 'cubic' method: ./test_data/boreal_agb_202302061675666220_3741_cubic.tif\nCreating COG using 'cubic_spline' method: ./test_data/boreal_agb_202302061675666220_3741_cubic_spline.tif\nCreating COG using 'lanczos' method: ./test_data/boreal_agb_202302061675666220_3741_lanczos.tif\nCreating COG using 'average' method: ./test_data/boreal_agb_202302061675666220_3741_average.tif\nCreating COG using 'mode' method: ./test_data/boreal_agb_202302061675666220_3741_mode.tif\nCreating COG using 'gauss' method: ./test_data/boreal_agb_202302061675666220_3741_gauss.tif\nCreating COG using 'rms' method: ./test_data/boreal_agb_202302061675666220_3741_rms.tif\n\n\nWe can validate that the overviews were created:\n\n!rio cogeo info {files[0]}\n\nDriver: GTiff\nFile: /home/ochotona/code/devseed/cloud-optimized-geospatial-formats-guide/cloud-optimized-geotiffs/test_data/boreal_agb_202302061675666220_3741_nearest.tif\nCOG: True\nCompression: DEFLATE\nColorSpace: None\n\nProfile\n    Width:            3000\n    Height:           3000\n    Bands:            2\n    Tiled:            True\n    Dtype:            float32\n    NoData:           -9999.0\n    Alpha Band:       False\n    Internal Mask:    False\n    Interleave:       PIXEL\n    ColorMap:         False\n    ColorInterp:      ('gray', 'undefined')\n    Scales:           (1.0, 1.0)\n    Offsets:          (0.0, 0.0)\n\nGeo\n    Crs:              PROJCS[\"unnamed\",GEOGCS[\"GRS 1980(IUGG, 1980)\",DATUM[\"unknown\",SPHEROID[\"GRS80\",6378137,298.257222101],TOWGS84[0,0,0,0,0,0,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",40],PARAMETER[\"longitude_of_center\",180],PARAMETER[\"standard_parallel_1\",50],PARAMETER[\"standard_parallel_2\",70],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n    Origin:           (1448521.9999999953, 2853304.0000000093)\n    Resolution:       (30.0, -30.0)\n    BoundingBox:      (1448521.9999999953, 2763304.0000000093, 1538521.9999999953, 2853304.0000000093)\n    MinZoom:          7\n    MaxZoom:          11\n\nImage Metadata\n    AREA_OR_POINT: Area\n    OVR_RESAMPLING_ALG: NEAREST\n\nImage Structure\n    COMPRESSION: DEFLATE\n    INTERLEAVE: PIXEL\n    LAYOUT: COG\n\nBand 1\n    Description: aboveground_biomass_density (Mg ha-1)\n    ColorInterp: gray\n    Metadata:\n        STATISTICS_MAXIMUM: 347.71490478516\n        STATISTICS_MEAN: 44.388942987808\n        STATISTICS_MINIMUM: 2.5594062805176\n        STATISTICS_STDDEV: 30.371053229248\n        STATISTICS_VALID_PERCENT: 86.55\n\nBand 2\n    Description: standard_deviation\n    ColorInterp: undefined\n    Metadata:\n        STATISTICS_MAXIMUM: 121.33344268799\n        STATISTICS_MEAN: 6.1550359252236\n        STATISTICS_MINIMUM: 0.25763747096062\n        STATISTICS_STDDEV: 3.7940122373131\n        STATISTICS_VALID_PERCENT: 86.55\n\nIFD\n    Id      Size           BlockSize     Decimation           \n    0       3000x3000      512x512       0\n    1       1500x1500      512x512       2\n    2       750x750        512x512       4\n    3       375x375        512x512       8",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud Optimized Geotiff (COG) Overview Resampling"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-overview_resampling.html#compare-the-results",
    "href": "cloud-optimized-geotiffs/cogs-overview_resampling.html#compare-the-results",
    "title": "Cloud Optimized Geotiff (COG) Overview Resampling",
    "section": "Compare the Results",
    "text": "Compare the Results\nNow that we’ve generated each of the overview methods we can plot the full data and each overview. The overviews are labelled by their magnification. Example and overview of 2, is the dimensions divided by 2. Typically in COGs you will keep making overviews until the one of the dimensions is less than 512 pixels.\n\ndef compare_overviews(image: str, fig, ax_list, row):\n    '''\n    Read the original data, and overviews an plot them.\n    image = the path to input COG to read\n    '''\n\n    with rio.open(image, 'r') as src:\n        #Plot the 1st band\n        oviews = src.overviews(1)\n        #fig, ax_list = plt.subplots(ncols=(len(oviews)+1), nrows=1, figsize=(16,4))\n        show(src, ax=ax_list[row, 0])\n        \n        bname = os.path.basename(image)\n        row_name = bname.split(\"_\")[-1].replace('.tif', '')\n        \n        ax_list[row,0].set_title(\"Full Resolution\")\n        ax_list[row,0].set_ylabel(row_name)\n        ax_list[row,0].xaxis.set_ticklabels([])\n        ax_list[row,0].yaxis.set_ticklabels([])\n        \n        #increment counter so overviews plot starting in the second column\n        k = 1\n        for oview in oviews:\n            height = int(src.height // oview)\n            width = int(src.width // oview)\n            thumbnail = src.read(1, out_shape=(1, height, width))\n            show(thumbnail, ax=ax_list[row,k])\n            ax_list[row,k].set_title(f'{height}x{width}')\n            ax_list[row,k].xaxis.set_ticklabels([])\n            ax_list[row,k].yaxis.set_ticklabels([])\n            k += 1\n\n    return\n\nFor each variat, we will add interpretation details below the plot. Notice we are using hard-coded values for the levels:\n\nfig, ax_list = plt.subplots(ncols=4, nrows=len(files), figsize=(16,32), constrained_layout=True)\nrow = 0\nfor file in files:\n    compare_overviews(file, fig, ax_list, row)\n    row += 1",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud Optimized Geotiff (COG) Overview Resampling"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-overview_resampling.html#interpretation",
    "href": "cloud-optimized-geotiffs/cogs-overview_resampling.html#interpretation",
    "title": "Cloud Optimized Geotiff (COG) Overview Resampling",
    "section": "Interpretation",
    "text": "Interpretation\nWith this particular example, you can see the default method (CUBIC) over-represents the amount of NoData cells as you zoom out (left to right). You can also see how some algorithms hide NoData when zoomed out, and the different methods vary in their level of smoothness.\nDepending on what impression you want viewers to get, comparing will help you choose which method to use.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud Optimized Geotiff (COG) Overview Resampling"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-examples.html",
    "href": "cloud-optimized-geotiffs/cogs-examples.html",
    "title": "Examples of Working with COGs",
    "section": "",
    "text": "The packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda env create -f environment.yml\nor\nmamba env create -f environment.yml\nFinally, you may activate and select the kernel in the notebook (running in Jupyter)\nconda activate coguide-cog\nThe notebook has been tested to work with the listed Conda environment.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Examples of Working with COGs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-examples.html#environment",
    "href": "cloud-optimized-geotiffs/cogs-examples.html#environment",
    "title": "Examples of Working with COGs",
    "section": "",
    "text": "The packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda env create -f environment.yml\nor\nmamba env create -f environment.yml\nFinally, you may activate and select the kernel in the notebook (running in Jupyter)\nconda activate coguide-cog\nThe notebook has been tested to work with the listed Conda environment.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Examples of Working with COGs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-examples.html#setup",
    "href": "cloud-optimized-geotiffs/cogs-examples.html#setup",
    "title": "Examples of Working with COGs",
    "section": "Setup",
    "text": "Setup\nTo demonstrate some COG concepts, we will download a regular GeoTIFF, create a Cloud-Optimized GeoTIFF, and explore their differences.\nTo access and integrate NASA Earth data into your Jupyter Notebook, you can create an account by visiting NASA’s Earthdata Login page. This will enable you to register for an account and retrieve the datasets used in the notebook.\nFirst, we use the earthaccess library to set up credentials to fetch data from NASA’s EarthData catalog.\n\nimport earthaccess\nimport rasterio\nfrom rasterio.plot import show\nfrom rio_cogeo import cog_validate, cog_info\n\n/Users/kyle/local/micromamba/envs/coguide-cog/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nearthaccess.login()",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Examples of Working with COGs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-examples.html#download-a-geotiff-from-earthdata",
    "href": "cloud-optimized-geotiffs/cogs-examples.html#download-a-geotiff-from-earthdata",
    "title": "Examples of Working with COGs",
    "section": "Download a GeoTIFF from EarthData",
    "text": "Download a GeoTIFF from EarthData\nNote: The whole point of “cloud-optimized” is that we don’t download entire files. So, in future examples, we will demonstrate how to access just subsets of data from COGs and compare that with a GeoTIFF.\n\n# Download data\nshort_name = 'VCF5KYR'\nversion = '001'\n\nveg_item_results = earthaccess.search_data(\n    short_name=short_name,\n    version=version,\n    count=1\n)\n\nGranules found: 33\n\n\n\ntest_data_dir = \"./test_data\"\nveg_files = earthaccess.download(veg_item_results, test_data_dir)\nveg_gtiff_filename = f\"{test_data_dir}/{veg_files[0]}\"\n\n Getting 1 granules, approx download size: 0.07 GB\n\n\nQUEUEING TASKS | : 100%|██████████| 1/1 [00:00&lt;00:00, 900.84it/s]\n\n\nFile VCF5KYR_1982001_001_2018224204211.tif already downloaded\n\n\nPROCESSING TASKS | : 100%|██████████| 1/1 [00:00&lt;00:00, 15887.52it/s]\nCOLLECTING RESULTS | : 100%|██████████| 1/1 [00:00&lt;00:00, 29330.80it/s]\n\n\n\nTo learn more about the example data see the Vegetation Continuous Fields (VCF) information page.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Examples of Working with COGs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-examples.html#is-it-a-valid-cog",
    "href": "cloud-optimized-geotiffs/cogs-examples.html#is-it-a-valid-cog",
    "title": "Examples of Working with COGs",
    "section": "Is it a valid COG?",
    "text": "Is it a valid COG?\nWe can use rio_cogeo.cog_validate to check. It returns is_valid, errors, and warnings:\n\ncog_validate(veg_gtiff_filename)\n\nThe following warnings were found:\n- The file is greater than 512xH or 512xW, it is recommended to include internal overviews\n\nThe following errors were found:\n- The file is greater than 512xH or 512xW, but is not tiled\n\n\n(False,\n ['The file is greater than 512xH or 512xW, but is not tiled'],\n ['The file is greater than 512xH or 512xW, it is recommended to include internal overviews'])\n\n\nHere’s some more context on the output message:\n\nis_valid is False: this is not a valid COG.\nerrors are 'The file is greater than 512xH or 512xW, but is not tiled'. To be a valid COG, the file should be tiled since it has a height and width both greater than 512.\nwarnings are 'The file is greater than 512xH or 512xW, it is recommended to include internal overviews'. It is recommended to provide overviews.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Examples of Working with COGs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-examples.html#converting-a-geotiff-to-cog",
    "href": "cloud-optimized-geotiffs/cogs-examples.html#converting-a-geotiff-to-cog",
    "title": "Examples of Working with COGs",
    "section": "Converting a GeoTIFF to COG",
    "text": "Converting a GeoTIFF to COG\nWe can use rio_cogeo.cog_create to convert a GeoTIFF into a Cloud Optimized GeoTIFF:\n\nveg_cog_filename = veg_gtiff_filename.replace(\".tif\", \"_cog.tif\")\n\n!rio cogeo create {veg_gtiff_filename} {veg_cog_filename}\n\nReading input: /Users/kyle/ds/cloud-optimized-geospatial-formats-guide/cloud-optimized-geotiffs/test_data/VCF5KYR_1982001_001_2018224204211.tif\n  [####################################]  100%\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /Users/kyle/ds/cloud-optimized-geospatial-formats-guide/cloud-optimized-geotiffs/test_data/VCF5KYR_1982001_001_2018224204211_cog.tif\n\n\n\ncog_validate(veg_cog_filename)\n\n(True, [], [])\n\n\nThe file is a valid COG, so we will use it to compare with our GeoTIFF.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Examples of Working with COGs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-examples.html#data-structure",
    "href": "cloud-optimized-geotiffs/cogs-examples.html#data-structure",
    "title": "Examples of Working with COGs",
    "section": "Data Structure",
    "text": "Data Structure\nDimensions: the number of bands, rows, and columns stored in a GeoTIFF. More Info\nInternal Blocks (chunks or internal tiles): Internal blocks are required if the data dimensions are over 512x512. More Info\nLet’s check out the dimensions and blocks of our GeoTIFF and Cloud-Optimized GeoTIFF.\n\nveg_gtiff_rio = rasterio.open(veg_gtiff_filename)\nveg_cog_rio = rasterio.open(veg_cog_filename)\n\n\nprint(veg_gtiff_rio.shape)\nveg_cog_rio.shape\n\n(3600, 7200)\n\n\n(3600, 7200)\n\n\nThey have the exact dimensions that we expected, which is good!\nWe can also print information about the GeoTIFF’s IFD (Internal File Directory). Only one item is returned because the GeoTIFF needs overviews. We see more items returned when we print the IFD info for the COG, which has overviews.\n\ncog_info(veg_gtiff_filename).IFD\n\n[IFD(Level=0, Width=7200, Height=3600, Blocksize=(1, 7200), Decimation=0)]\n\n\n\ncog_info(veg_cog_filename).IFD\n\n[IFD(Level=0, Width=7200, Height=3600, Blocksize=(512, 512), Decimation=0),\n IFD(Level=1, Width=3600, Height=1800, Blocksize=(128, 128), Decimation=2),\n IFD(Level=2, Width=1800, Height=900, Blocksize=(128, 128), Decimation=4),\n IFD(Level=3, Width=900, Height=450, Blocksize=(128, 128), Decimation=8)]\n\n\nNote for IFD Level 0, the regular GeoTIFF has a blocksize of (1, 7200) which implies each row of data is a separate block. So whenever reading in data, even if only a few columns are required, the full row must be read.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Examples of Working with COGs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/cogs-examples.html#overviews",
    "href": "cloud-optimized-geotiffs/cogs-examples.html#overviews",
    "title": "Examples of Working with COGs",
    "section": "Overviews",
    "text": "Overviews\nOverviews are downsampled (aggregated) data intended for visualization.\nThe most miniature size overview should match the tiling components’ fetch size, typically 256x256. Due to aspect ratio variation, aim to have at least one dimension at or slightly less than 256. &gt; The COG driver in GDAL or rio cogeo tools should do this.\nThere are many resampling algorithms for generating overviews. The best resampling algorithm depends on the data’s range, type, and distribution. When creating overviews, several options should be compared before deciding which resampling method to apply.\nGDAL &gt;= 3.2 allows for the overview resampling method to be set directly.\nFor more information on overviews, see the COG overview resampling notebook.\n\nveg_gtiff_rio.overviews(1)\n\n[]\n\n\n\nveg_cog_rio.overviews(1)\n\n[2, 4, 8]\n\n\nBy displaying each overview, we can see how the dimensions get coarser for each overview level.\n\ndef show_overviews(geotiff):  \n    for overview in geotiff.overviews(1):\n        out_height = int(geotiff.height // overview)\n        out_width = int(geotiff.width // overview)\n        print(f\"out height: {out_height}\")\n        print(f\"out width: {out_width}\")    \n        # read first band of file and set shape of new output array\n        window_size_height = round(out_height/8)\n        window_size_width = round(out_width/8)\n        image = veg_cog_rio.read(1, out_shape=(1, out_height, out_width))[\n            window_size_height:(window_size_height*2),\n            window_size_width:(window_size_width*2),\n        ]\n        show(image)\n        \nshow_overviews(veg_cog_rio)\n\nout height: 1800\nout width: 3600\n\n\n\n\n\n\n\n\n\nout height: 900\nout width: 1800\n\n\n\n\n\n\n\n\n\nout height: 450\nout width: 900\n\n\n\n\n\n\n\n\n\nWe can generate more and different overviews, through different tilesizes and resampling.\n\nimport gen_overviews\n\n\ntmp_dst = gen_overviews.create_overviews_from_gtiff(veg_gtiff_rio)\ntmp_cog = rasterio.open(tmp_dst)\ncog_info(tmp_dst).IFD\n\n\n\n\n[IFD(Level=0, Width=7200, Height=3600, Blocksize=(1, 7200), Decimation=0),\n IFD(Level=1, Width=3600, Height=1800, Blocksize=(128, 128), Decimation=2),\n IFD(Level=2, Width=1800, Height=900, Blocksize=(128, 128), Decimation=4),\n IFD(Level=3, Width=900, Height=450, Blocksize=(128, 128), Decimation=8),\n IFD(Level=4, Width=450, Height=225, Blocksize=(128, 128), Decimation=16)]\n\n\nNote: Now we have overviews but there are still no tiles on the Level 0 IFD.\n\noverviews = tmp_cog.overviews(1)\noverviews\n\n[2, 4, 8, 16]\n\n\n\nshow_overviews(tmp_cog)\n\nout height: 1800\nout width: 3600\n\n\n\n\n\n\n\n\n\nout height: 900\nout width: 1800\n\n\n\n\n\n\n\n\n\nout height: 450\nout width: 900\n\n\n\n\n\n\n\n\n\nout height: 225\nout width: 450",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Examples of Working with COGs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/intro.html",
    "href": "cloud-optimized-geotiffs/intro.html",
    "title": "Cloud-Optimized GeoTIFFs",
    "section": "",
    "text": "Cloud-Optimized GeoTIFF (COG), a raster format, is a variant of the TIFF image format that specifies a particular layout of internal data in the GeoTIFF specification to allow for optimized (subsetted or aggregated) access over a network for display or data reading. All COG files are valid GeoTIFF files, but not all GeoTIFF files are valid COG files. The key components that differ between GeoTIFF and COG are overviews and internal tiling.\nFor more details see https://www.cogeo.org/\n\n\n\nDimensions are the number of bands, rows and columns stored in a GeoTIFF. There is a tradeoff between storing lots of data in one GeoTIFF and storing less data in many GeoTIFFs. The larger a single file, the larger the GeoTIFF header and the multiple requests may be required just to read the spatial index before data retrieval. The opposite problem occurs if you make too many small files, then it takes many reads to retrieve data, and when rendering a combined visualization can greatly impact load time.\nIf you plan to pan and zoom a large amount of data through a tiling service in a web browser, there is a tradeoff between 1 large file, or many smaller files. The current recommendation is to meet somewhere in the middle, a moderate amount of medium files.\n\n\n\n\nThis attribute is also sometimes called chunks or internal tiles.\n\nInternal blocks are required if the dimensions of data are over 512x512. However you can control the size of the internal blocks. 256x256 or 512x512 are recommended. When displaying data at full resolution, or doing partial reading of data this size will impact the number of reads required. A size of 256 will take less time to read, and read less data outside the desired bounding box, however for reading large parts of a file, it may take more total read requests. Some clients will aggregate neighboring block reads to reduce the total number of requests.\n\n\n\nOverviews are downsampled (aggregated) data intended for visualization. The best resampling algorithm depends on the range, type, and distribution of the data.\nThe smallest size overview should match the tiling components’ fetch size, typically 256x256. Due to aspect ratio variation just aim to have at least one dimension at or slightly less than 256. The COG driver in GDAL, or rio cogeo tools should do this.\nThere are many resampling algorithms for generating overviews. When creating overviews several options should be compared before deciding which resampling method to apply.\n\n\n\n\nAdditional COG details that can be helpful.\nMaking and using COG examples.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud-Optimized GeoTIFFs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/intro.html#dimensions",
    "href": "cloud-optimized-geotiffs/intro.html#dimensions",
    "title": "Cloud-Optimized GeoTIFFs",
    "section": "",
    "text": "Dimensions are the number of bands, rows and columns stored in a GeoTIFF. There is a tradeoff between storing lots of data in one GeoTIFF and storing less data in many GeoTIFFs. The larger a single file, the larger the GeoTIFF header and the multiple requests may be required just to read the spatial index before data retrieval. The opposite problem occurs if you make too many small files, then it takes many reads to retrieve data, and when rendering a combined visualization can greatly impact load time.\nIf you plan to pan and zoom a large amount of data through a tiling service in a web browser, there is a tradeoff between 1 large file, or many smaller files. The current recommendation is to meet somewhere in the middle, a moderate amount of medium files.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud-Optimized GeoTIFFs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/intro.html#internal-blocks",
    "href": "cloud-optimized-geotiffs/intro.html#internal-blocks",
    "title": "Cloud-Optimized GeoTIFFs",
    "section": "",
    "text": "This attribute is also sometimes called chunks or internal tiles.\n\nInternal blocks are required if the dimensions of data are over 512x512. However you can control the size of the internal blocks. 256x256 or 512x512 are recommended. When displaying data at full resolution, or doing partial reading of data this size will impact the number of reads required. A size of 256 will take less time to read, and read less data outside the desired bounding box, however for reading large parts of a file, it may take more total read requests. Some clients will aggregate neighboring block reads to reduce the total number of requests.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud-Optimized GeoTIFFs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/intro.html#overviews",
    "href": "cloud-optimized-geotiffs/intro.html#overviews",
    "title": "Cloud-Optimized GeoTIFFs",
    "section": "",
    "text": "Overviews are downsampled (aggregated) data intended for visualization. The best resampling algorithm depends on the range, type, and distribution of the data.\nThe smallest size overview should match the tiling components’ fetch size, typically 256x256. Due to aspect ratio variation just aim to have at least one dimension at or slightly less than 256. The COG driver in GDAL, or rio cogeo tools should do this.\nThere are many resampling algorithms for generating overviews. When creating overviews several options should be compared before deciding which resampling method to apply.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud-Optimized GeoTIFFs"
    ]
  },
  {
    "objectID": "cloud-optimized-geotiffs/intro.html#see-more",
    "href": "cloud-optimized-geotiffs/intro.html#see-more",
    "title": "Cloud-Optimized GeoTIFFs",
    "section": "",
    "text": "Additional COG details that can be helpful.\nMaking and using COG examples.",
    "crumbs": [
      "Formats",
      "Cloud Optimized GeoTIFFs (COG)",
      "Cloud-Optimized GeoTIFFs"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Get Involved",
    "section": "",
    "text": "We encourage contributions to this guide. The guide’s goal is to provide documentation on the best practices for the current state-of-the-art cloud-optimized formats. These formats are evolving, and so will the guide.",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#introduction",
    "href": "contributing.html#introduction",
    "title": "Get Involved",
    "section": "",
    "text": "We encourage contributions to this guide. The guide’s goal is to provide documentation on the best practices for the current state-of-the-art cloud-optimized formats. These formats are evolving, and so will the guide.",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#pre-requisites",
    "href": "contributing.html#pre-requisites",
    "title": "Get Involved",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nIf you wish to preview the site locally, install quarto. You will also need to be familiar with quarto markdown.",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#communication-channels",
    "href": "contributing.html#communication-channels",
    "title": "Get Involved",
    "section": "Communication Channels",
    "text": "Communication Channels\nDiscussions can occur in GitHub Discussions and issues can be raised at GitHub Issues.\n\nGitHub Discussions: Ideal for questions, feature requests, or general conversations about the project. Use this space for collaborative discussions or if you’re unsure where to start.\nGitHub Issues: Use this for reporting bugs, suggesting enhancements, or other tasks that require tracking and possibly code changes.",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#core-principles",
    "href": "contributing.html#core-principles",
    "title": "Get Involved",
    "section": "Core Principles",
    "text": "Core Principles\n\nThis guide intends to be opinionated but acknowledges no one-size-fits-all solution.\nThis guide should provide the best information and guidance available but acknowledge that experts develop many existing resources. Those resources should be linked as appropriate.",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#additional-criteria",
    "href": "contributing.html#additional-criteria",
    "title": "Get Involved",
    "section": "Additional Criteria",
    "text": "Additional Criteria\n\nAll examples should use open data. If an example uses data from NASA Earthdata, it must include an example of providing credentials (Earthdata registration is available to anyone).\nLanding pages with no code should use quarto markdown (.qmd).\nPages with executable code should be Jupyter Notebooks (.ipynb).",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#code-of-conduct",
    "href": "contributing.html#code-of-conduct",
    "title": "Get Involved",
    "section": "Code of Conduct",
    "text": "Code of Conduct\n\nBe inclusive, respectful, and understanding of others’ backgrounds and contexts.\nLook for and foster diverse perspectives.\nIf you experience any harmful behavior, please get in touch with Aimee or Alex.",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#bug-reporting-feature-requests",
    "href": "contributing.html#bug-reporting-feature-requests",
    "title": "Get Involved",
    "section": "Bug Reporting & Feature Requests",
    "text": "Bug Reporting & Feature Requests\nBefore submitting a bug report or a feature request, please start a GitHub Discussion to see if the issue has already been addressed or if it can be resolved through discussion.\n\nGeneral Steps\n\nFork the repository.\nClone your fork locally.\nCreate a new branch for your changes.\nMake your changes and use quarto preview to make sure they look good.\nOpen a pull request.\n\nOnce the pull request is opened, and the GitHub preview.yml workflow runs (“Deploy PR previews”), you should have a preview available for review at https://guide.cloudnativegeo.org/pr-preview/pr-&lt;YOUR-PR-NUMBER-HERE&gt;. A bot will comment on your PR when the PR preview is ready.\n\n\nSpecific Contributions\n\n1. Adding a New Format\nFollow the steps outlined in the General Steps, then:\n\nCreate a folder with the format’s name and, within that folder, an intro.qmd.\nLink to the intro.qmd page in the index.qmd (the Welcome page) file and _quarto.yml table of contents.\n\n\n\n2. Modify or Add to an Existing Format\nFeel free to modify or add to existing content if you think it could be improved.\n\n\n3. Adding a Cookbook\nCookbooks should address common questions and present solutions for cloud-optimized access and visualization. To create a cookbook, either add a notebook directly to this repository in the cookbooks directory OR use an external link and add it to cookbooks/index.qmd.\n\n\n4. (Optional) Update Slides\nIf you have made substantive changes, consider updating the Overview Slides. These slides are generated using Quarto and Reveal.js so can be updated with markdown syntax.\n\n\n5. Add Yourself to the List of Authors\nAdd yourself to the list of authors on the Welcome page.\n\n\n6. Final Steps Before Merging\nOnce your PR is approved and all checks have passed, a project maintainer will merge your changes into the main repository.",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#licensing",
    "href": "contributing.html#licensing",
    "title": "Get Involved",
    "section": "Licensing",
    "text": "Licensing\nThis work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/. For attribution requirements, please look at the license terms.\nPreferred citation: Barciauskas, A et al. 2023. Cloud Optimized Geospatial Formats Guide. CC-By-4.0.",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#contact",
    "href": "contributing.html#contact",
    "title": "Get Involved",
    "section": "Contact",
    "text": "Contact\nFor questions on how to contribute, start a discussion in the GitHub Discussions section.",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "contributing.html#thank-you-to-our-supporters",
    "href": "contributing.html#thank-you-to-our-supporters",
    "title": "Get Involved",
    "section": "Thank you to our supporters",
    "text": "Thank you to our supporters\nThis guide has been made possible through the support of:",
    "crumbs": [
      "Get Involved"
    ]
  },
  {
    "objectID": "zarr/zarr-in-practice.html",
    "href": "zarr/zarr-in-practice.html",
    "title": "Zarr in Practice",
    "section": "",
    "text": "This notebook demonstrates how to create, explore and modify a Zarr store.\nThese concepts are explored in more detail in the official Zarr Tutorial.\nIt also shows the use of public Zarr stores for geospatial data.",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr in Practice"
    ]
  },
  {
    "objectID": "zarr/zarr-in-practice.html#how-to-create-a-zarr-store",
    "href": "zarr/zarr-in-practice.html#how-to-create-a-zarr-store",
    "title": "Zarr in Practice",
    "section": "How to create a Zarr store",
    "text": "How to create a Zarr store\n\nimport sys\nimport numpy as np\nimport xarray as xr\nimport zarr\n\n# Here we create a simple Zarr store.\nzstore = zarr.array(np.arange(10))\n\nThis is an in-memory Zarr store. To persist it to disk, we can use .save.\n\nzarr.save(\"test.zarr\", zstore)\n\nWe can open the metadata about this dataset, which gives us some interesting information. The dataset has a shape of 10 chunks of 10, so we know all the data was stored in 1 chunk, and was compressed with the blosc compressor.\n\n!cat test.zarr/.zarray \n\n{\n    \"chunks\": [\n        10\n    ],\n    \"compressor\": {\n        \"blocksize\": 0,\n        \"clevel\": 5,\n        \"cname\": \"lz4\",\n        \"id\": \"blosc\",\n        \"shuffle\": 1\n    },\n    \"dtype\": \"&lt;i8\",\n    \"fill_value\": 0,\n    \"filters\": null,\n    \"order\": \"C\",\n    \"shape\": [\n        10\n    ],\n    \"zarr_format\": 2\n}\n\n\nThis was a pretty basic example. Let’s explore the other things we might want to do when creating Zarr.",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr in Practice"
    ]
  },
  {
    "objectID": "zarr/zarr-in-practice.html#how-to-create-a-group",
    "href": "zarr/zarr-in-practice.html#how-to-create-a-group",
    "title": "Zarr in Practice",
    "section": "How to create a group",
    "text": "How to create a group\n\nroot = zarr.group()\ngroup1 = root.create_group('group1')\ngroup2 = root.create_group('group2')\nz1 = group1.create_dataset('ds_in_group', shape=(100,100), chunks=(10,10), dtype='i4')\nz2 = group2.create_dataset('ds_in_group', shape=(1000,1000), chunks=(10,10), dtype='i4')\nroot.tree(expand=True)",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr in Practice"
    ]
  },
  {
    "objectID": "zarr/zarr-in-practice.html#how-to-examine-and-modify-the-chunk-shape",
    "href": "zarr/zarr-in-practice.html#how-to-examine-and-modify-the-chunk-shape",
    "title": "Zarr in Practice",
    "section": "How to Examine and Modify the Chunk Shape",
    "text": "How to Examine and Modify the Chunk Shape\nIf your data is sufficiently large, Zarr will chose a chunksize for you.\n\nzarr_no_chunks = zarr.array(np.arange(100), chunks=True)\nzarr_no_chunks.chunks, zarr_no_chunks.shape\n\n((100,), (100,))\n\n\n\nzarr_with_chunks = zarr.array(np.arange(10000000), chunks=True)\nzarr_with_chunks.chunks, zarr_with_chunks.shape\n\n((156250,), (10000000,))\n\n\nFor zarr_with_chunks we see the chunks are smaller than the shape, so we know the data has been chunked. Other ways to examine the chunk structure are zarr.info and zarr.cdata_shape.\n\n?zarr_no_chunks.cdata_shape\n\n\nType:        property\nString form: &lt;property object at 0x7efde6ecfb00&gt;\nDocstring:  \nA tuple of integers describing the number of chunks along each\ndimension of the array.\n\n\n\n\nzarr_no_chunks.cdata_shape, zarr_with_chunks.cdata_shape\n\n((1,), (64,))\n\n\nThe zarr store with chunks has 64 chunks. The number of chunks multiplied by the chunk size equals the length of the whole array.\n\nzarr_with_chunks.cdata_shape[0] * zarr_with_chunks.chunks[0] == zarr_with_chunks.shape[0]\n\nTrue\n\n\n\nWhat’s the storage size of these chunks?\nThe default chunks are pretty small.\n\nsys.getsizeof(zarr_with_chunks.chunk_store['0']) # this is in bytes\n\n8049\n\n\n\nzarr_with_big_chunks = zarr.array(np.arange(10000000), chunks=(500000))\n\n\nzarr_with_big_chunks.chunks, zarr_with_big_chunks.shape, zarr_with_big_chunks.cdata_shape\n\n((500000,), (10000000,), (20,))\n\n\nThis Zarr store has 10 million values, stored in 20 chunks of 500,000 data values.\n\nsys.getsizeof(zarr_with_big_chunks.chunk_store['0'])\n\n24941\n\n\nThese chunks are still pretty small, but this is just a silly example. In the real world, you will likely want to deal in Zarr chunks of 1MB or greater, especially when dealing with remote storatge options where data is read over a network and the number of requests should be minimized.",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr in Practice"
    ]
  },
  {
    "objectID": "zarr/zarr-in-practice.html#exploring-and-modifying-data-compression",
    "href": "zarr/zarr-in-practice.html#exploring-and-modifying-data-compression",
    "title": "Zarr in Practice",
    "section": "Exploring and Modifying Data Compression",
    "text": "Exploring and Modifying Data Compression\nContinuing with data from the example above, we can tell that Zarr has also compressed the data for us using zarr.info or zarr.compressor.\n\nzarr_with_chunks.compressor\n\nBlosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)\n\n\nThe Blosc compressor is actually a meta compressor so actually implements multiple different internal compressors. In this case, it has implemented lz4 compression. We can also explore how much space was saved by using this compression method.\n\nzarr_with_chunks.info\n\n\n\n\nType\nzarr.core.Array\n\n\nData type\nint64\n\n\nShape\n(10000000,)\n\n\nChunk shape\n(156250,)\n\n\nOrder\nC\n\n\nRead-only\nFalse\n\n\nCompressor\nBlosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)\n\n\nStore type\nzarr.storage.KVStore\n\n\nNo. bytes\n80000000 (76.3M)\n\n\nNo. bytes stored\n514193 (502.1K)\n\n\nStorage ratio\n155.6\n\n\nChunks initialized\n64/64\n\n\n\n\n\nWe can see, from the storage ratio above, that compression has made our data 155 times smaller 😱 .\nYou can set compression=None when creating a Zarr array to turn off this behavior, but I’m not sure why you would do that.\nLet’s see what happens when we use a different compression method. We can checkout a full list of numcodecs compressors here: https://numcodecs.readthedocs.io/.\n\nfrom numcodecs import GZip\ncompressor = GZip()\nzstore_gzip_compressed = zarr.array(np.arange(10000000), chunks=True, compressor=compressor)\nzstore_gzip_compressed.info\n\n\n\n\nType\nzarr.core.Array\n\n\nData type\nint64\n\n\nShape\n(10000000,)\n\n\nChunk shape\n(156250,)\n\n\nOrder\nC\n\n\nRead-only\nFalse\n\n\nCompressor\nGZip(level=1)\n\n\nStore type\nzarr.storage.KVStore\n\n\nNo. bytes\n80000000 (76.3M)\n\n\nNo. bytes stored\n15086009 (14.4M)\n\n\nStorage ratio\n5.3\n\n\nChunks initialized\n64/64\n\n\n\n\n\nIn this case, the storage ratio is 5.3 - so not as good! How to chose a compression algorithm is a topic for future investigation.",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr in Practice"
    ]
  },
  {
    "objectID": "zarr/zarr-in-practice.html#consolidating-metadata",
    "href": "zarr/zarr-in-practice.html#consolidating-metadata",
    "title": "Zarr in Practice",
    "section": "Consolidating metadata",
    "text": "Consolidating metadata\nIt’s important to consolidate metadata to minimize requests. Each group and array will have a metadata file, so in order to limit requests to read the whole tree of metadata files, Zarr provides the ability to consolidate metdata into a metadata file at the of the store.\nSo far we have only been dealing in single array Zarr data stores. In this next example, we will create a zarr store with multiple arrays and then consolidate metadata. The speed up with local storage is insignificant, but becomes significant when dealing in remote storage options, which we will see in the following example on accessing cloud storage.\n\nroot = zarr.group()\nzarr_store = 'example.zarr'\n# Let's create many groups and many arrays\nnum_groups, num_arrays_per_group = 100, 100\nfor i in range(num_groups):\n    group = root.create_group(f'group-{i}')\n    for j in range(num_arrays_per_group):\n        group.create_dataset(f'array-{j}', shape=(1000,1000), dtype='i4')\n\nstore = zarr.DirectoryStore(zarr_store)\nzarr.save(store, root)\n\n\n# We don't expect it to exist yet!\n!cat {zarr_store}/.zmetadata\n\ncat: {zarr_store}/.zmetadata: No such file or directory\n\n\n\nzarr.consolidate_metadata(zarr_store)\n\n&lt;zarr.core.Array (100,) &lt;U8&gt;\n\n\n\nzarr.open_consolidated(zarr_store)\n\n&lt;zarr.core.Array (100,) &lt;U8&gt;\n\n\n\n!cat {zarr_store}/.zmetadata\n\n{\n    \"metadata\": {\n        \".zarray\": {\n            \"chunks\": [\n                100\n            ],\n            \"compressor\": {\n                \"blocksize\": 0,\n                \"clevel\": 5,\n                \"cname\": \"lz4\",\n                \"id\": \"blosc\",\n                \"shuffle\": 1\n            },\n            \"dtype\": \"&lt;U8\",\n            \"fill_value\": \"\",\n            \"filters\": null,\n            \"order\": \"C\",\n            \"shape\": [\n                100\n            ],\n            \"zarr_format\": 2\n        }\n    },\n    \"zarr_consolidated_format\": 1\n}",
    "crumbs": [
      "Formats",
      "Zarr",
      "Zarr in Practice"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Cloud-Optimized Geospatial Formats Overview",
    "section": "",
    "text": "These slides were generated with https://quarto.org/docs/presentations/revealjs. Source: https://github.com/cloudnativegeo/cloud-optimized-geospatial-formats-guide.",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "overview.html#what-does-cloud-optimized-mean-1",
    "href": "overview.html#what-does-cloud-optimized-mean-1",
    "title": "Cloud-Optimized Geospatial Formats Overview",
    "section": "What Does Cloud-Optimized Mean?",
    "text": "What Does Cloud-Optimized Mean?\n\nFile metadata in one read\nWhen accessing data over the internet, such as when data is in cloud storage, latency is high when compared with local storage so it is preferable to fetch lots of data in fewer reads.\nAn easy win is metadata in one read, which can be used to read a cloud-native dataset.\nA cloud-native dataset is one with small addressable chunks via files, internal tiles, or both.",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "overview.html#what-does-cloud-optimized-mean-2",
    "href": "overview.html#what-does-cloud-optimized-mean-2",
    "title": "Cloud-Optimized Geospatial Formats Overview",
    "section": "What Does Cloud-Optimized Mean?",
    "text": "What Does Cloud-Optimized Mean?\n\n\n\nAccessible over HTTP using range requests.\nThis makes it compatible with object storage (a file storage alternative to local disk) and thus accessible via HTTP, from many compute instances.\nSupports lazy access and intelligent subsetting.\nIntegrates with high-level analysis libraries and distributed frameworks.\n\n\n\n\n\n\n\nimage credit: Ryan Abernathey",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "overview.html#what-are-cogs-1",
    "href": "overview.html#what-are-cogs-1",
    "title": "Cloud-Optimized Geospatial Formats Overview",
    "section": "What are COGs?",
    "text": "What are COGs?\n\n\n\nCOGs have internal file directories (IFDs) which are used to tell clients where to find different overview levels and data within the file.\nClients can use this metadata to read only the data they need to visualize or calculate.\nThis internal organization is friendly for consumption by clients issuing HTTP GET range request (“bytes: start_offset-end_offset” HTTP header)\n\n\n\n\n\n\n\nimage source: https://medium.com/devseed/cog-talk-part-1-whats-new-941facbcd3d1",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "overview.html#zarr-specs-in-development",
    "href": "overview.html#zarr-specs-in-development",
    "title": "Cloud-Optimized Geospatial Formats Overview",
    "section": "Zarr Specs in Development",
    "text": "Zarr Specs in Development\n\nV2 and older specs exist, however,\nA cross-organization working group has just formed to establish a GeoZarr standards working group, organized by Brianna Pagán (NASA) and includes representatives from many other orgs in the industry.\nThe GeoZarr spec defines conventions for how geospatial data should be organized in a Zarr store. The spec details how the Zarr DataArray and DataSet metadata, and subsequent organization of data, must be in order to be conformant as GeoZarr archive.\nThere is a proposal for Zarr v3 which will address challenges in language support, and storage organization to address the issues of high-latency reads and volume of reads for the many objects stored.\nThere is recent work on a parquet alternative to JSON for indexing.",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "overview.html#copc-cloud-optimized-point-clouds",
    "href": "overview.html#copc-cloud-optimized-point-clouds",
    "title": "Cloud-Optimized Geospatial Formats Overview",
    "section": "COPC (Cloud-Optimized Point Clouds)",
    "text": "COPC (Cloud-Optimized Point Clouds)\n\n\n\nimage source: https://copc.io/\n\nPoint clouds are a set of data points in space, such as gathered from LiDAR measurements.\nCOPC is a valid LAZ file.\nSimilar to COGs but for point clouds: COPC is just one file, but data is reorganized into a clustered octree instead of regularly gridded overviews.\n2 key features:\n\nSupport for partial decompression via storage of data in a series of chunks\nVariable-length records (VLRs) can store application-specific metadata of any kind. VLRs describe the octree structure.\n\nLimitation: Not all attribute types are compatible.",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "overview.html#not-quite",
    "href": "overview.html#not-quite",
    "title": "Cloud-Optimized Geospatial Formats Overview",
    "section": "Not Quite",
    "text": "Not Quite\n\nThese formats and their tooling are in active development\nSome formats were not mentioned, such as EPT, geopkg, tiledb, Cloud-Optimized HDF5. This presentation was scoped to those known best by the authors.\nThis site will continue to be updated with new content.",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pmtiles/intro.html",
    "href": "pmtiles/intro.html",
    "title": "PMTiles",
    "section": "",
    "text": "PMTiles is a single-file archive format for tiled data, usually used for visualization.\nAs an “archive format”, PMTiles is similar to a ZIP file: it contains the contents of many individual files inside of one PMTiles file. A single file is often much easier to use and keep track of than many very small files.\nPMTiles is designed for tiled data. That is, data where one inner file represents a small square somewhere on a map, usually representing the Web Mercator grid. PMTiles can be used for any format of tiled data. PMTiles is used most often with vector data, where each tile data contained within the archive is encoded as a Mapbox Vector Tile (MVT), but can also be used with e.g. raster data or terrain mesh data.\n\n\nTo understand PMTiles, it’s important to understand the difference between “analytical” data and “tiled” data. Analytical data refers to data in its original form, without any modifications to geometry. Tiled data formats apply a variety of modifications to geometries, including clipping and simplification, to save space and make it faster to visualize.\n\nConsider the above diagram. In an analytical format, every coordinate of the complex polygon would be included in one single file. In a tiled format, there are predefined tile sets (or grids) and the geometry would be split into one or more files, where each file represents one cell of the grid.\nThe analytical format is more useful for operations like a spatial join, because the entire geometry is available. It’s harder to perform such analyses on tiled data because given any one tile, it’s impossible to know whether the data contained in that tile represents the full geometry or not.\n\nKnow which other tiles contain part of this polygon (This is hard! It requires some other pre-generated attribute other than the geometry itself.)\nFetch each of those neighboring tiles\nAssemble the dissected geometries back into a single geometry\nApply the desired operation\n\nThe tiled format is more useful for visualization because a user who wants to visualize a small area only needs to download a few tiles. Additionally loading the data is faster because of simplification. It’s slower to visualize analytical data because the entire shape with all coordinates must be loaded, even if visualizing only a small area.\nThus analytical and visualization formats strive for different goals.\n\n\n\nPMTiles is designed to be a cloud-native file format: used directly from a client over a network via HTTP range requests, without having a server in the middle.\n\n\n\nPMTiles has a file header, one or more metadata regions, and a region of tile data.\nThe header is fixed length, located at the beginning of the file, and includes necessary information to decode the rest of the file accurately.\nPMTiles includes directories, or regions of bytes with metadata about tiles. It’s important for each directory to remain small, so while there will always be at least one directory, larger PMTiles archives with many tiles may include more than one directory.\nAt the end of the file is the tile data. This includes all data for all the tiles in the archive.\nThe full specification is defined here.\n\n\nInterally, tiles are oriented along a Hilbert Curve. This means that tiles that are spatially near each other are also located near each other in the file structure.\nThis is especially appropriate for PMTiles because visualization purposes most often request data within a specific geographic area. Because spatially-nearby tiles are likely to be nearby in the file as well, this allows the PMTiles client to merge multiple requests for tiles into one larger request, rather than needing to fetch a different area of the file for each tile.\n\n\n\n\nPMTiles archives support storing a full XYZ pyramid of tile data. This means that you can store multiple zoom levels of data inside a single file.\n\n\n\nPMTiles allows tiles to be stored in the file with compression.\n\n\n\n\n\nThe easiest way to generate PMTiles for vector data is through the tippecanoe tool. This will generate vector tiles that are ideal for visualization, removing small features at low zoom levels to keep tiles a manageable size.\n\n\n\nPMTiles has a command-line program for creating PMTiles if you already have an MBTiles file or a directory of tiles.\n\n\n\n\n\n\nIf you have an existing PMTiles archive, either as a local file or hosted on cloud storage, you can use the PMTiles Viewer to inspect the tiles hosted within the file.\n\n\n\nPMTiles doesn’t have a standalone JavaScript library, but rather is designed to be used in conjunction with a JavaScript map rendering library.\nSee the docs on viewing PMTiles in Leaflet, MapLibre GL JS and OpenLayers.\n\n\n\nPMTiles has a Python package, which allows reading and writing PMTiles archives from Python.\n\n\n\n\n\n\nThe most common alternative for PMTiles is MBTiles, which was in many ways the precursor to PMTiles. MBTiles stores the included vector tiles in a table in a SQLite database. MBTiles has the benefit of being much easier to use than manually managing millions of tiny, individual files, but MBTiles is not serverless. In general, it’s impossible to read from a SQLite database without fetching the entire file’s content. This means that frontend clients like a web browser couldn’t fetch tiles directly using range requests, but rather a server has to be running to fetch tiles from the MBTiles file.\n\n\n\nIt’s also possible to upload the bare tiled data directly to cloud storage as individual files.\nThis has significant downsides of needing to manage many millions of tiny individual files. Uploading millions of files to a cloud storage provider such as S3 takes time and money. For example, AWS charges $5 per million files added to an S3 bucket. So a 10 million PMTiles archive would cost $50, compared to 5-millionths of a cent to upload the PMTiles file.",
    "crumbs": [
      "Formats",
      "PMTiles",
      "PMTiles"
    ]
  },
  {
    "objectID": "pmtiles/intro.html#analytical-vs-tiled-data-formats",
    "href": "pmtiles/intro.html#analytical-vs-tiled-data-formats",
    "title": "PMTiles",
    "section": "",
    "text": "To understand PMTiles, it’s important to understand the difference between “analytical” data and “tiled” data. Analytical data refers to data in its original form, without any modifications to geometry. Tiled data formats apply a variety of modifications to geometries, including clipping and simplification, to save space and make it faster to visualize.\n\nConsider the above diagram. In an analytical format, every coordinate of the complex polygon would be included in one single file. In a tiled format, there are predefined tile sets (or grids) and the geometry would be split into one or more files, where each file represents one cell of the grid.\nThe analytical format is more useful for operations like a spatial join, because the entire geometry is available. It’s harder to perform such analyses on tiled data because given any one tile, it’s impossible to know whether the data contained in that tile represents the full geometry or not.\n\nKnow which other tiles contain part of this polygon (This is hard! It requires some other pre-generated attribute other than the geometry itself.)\nFetch each of those neighboring tiles\nAssemble the dissected geometries back into a single geometry\nApply the desired operation\n\nThe tiled format is more useful for visualization because a user who wants to visualize a small area only needs to download a few tiles. Additionally loading the data is faster because of simplification. It’s slower to visualize analytical data because the entire shape with all coordinates must be loaded, even if visualizing only a small area.\nThus analytical and visualization formats strive for different goals.",
    "crumbs": [
      "Formats",
      "PMTiles",
      "PMTiles"
    ]
  },
  {
    "objectID": "pmtiles/intro.html#cloud-native",
    "href": "pmtiles/intro.html#cloud-native",
    "title": "PMTiles",
    "section": "",
    "text": "PMTiles is designed to be a cloud-native file format: used directly from a client over a network via HTTP range requests, without having a server in the middle.",
    "crumbs": [
      "Formats",
      "PMTiles",
      "PMTiles"
    ]
  },
  {
    "objectID": "pmtiles/intro.html#internal-format",
    "href": "pmtiles/intro.html#internal-format",
    "title": "PMTiles",
    "section": "",
    "text": "PMTiles has a file header, one or more metadata regions, and a region of tile data.\nThe header is fixed length, located at the beginning of the file, and includes necessary information to decode the rest of the file accurately.\nPMTiles includes directories, or regions of bytes with metadata about tiles. It’s important for each directory to remain small, so while there will always be at least one directory, larger PMTiles archives with many tiles may include more than one directory.\nAt the end of the file is the tile data. This includes all data for all the tiles in the archive.\nThe full specification is defined here.\n\n\nInterally, tiles are oriented along a Hilbert Curve. This means that tiles that are spatially near each other are also located near each other in the file structure.\nThis is especially appropriate for PMTiles because visualization purposes most often request data within a specific geographic area. Because spatially-nearby tiles are likely to be nearby in the file as well, this allows the PMTiles client to merge multiple requests for tiles into one larger request, rather than needing to fetch a different area of the file for each tile.",
    "crumbs": [
      "Formats",
      "PMTiles",
      "PMTiles"
    ]
  },
  {
    "objectID": "pmtiles/intro.html#multiple-resolution",
    "href": "pmtiles/intro.html#multiple-resolution",
    "title": "PMTiles",
    "section": "",
    "text": "PMTiles archives support storing a full XYZ pyramid of tile data. This means that you can store multiple zoom levels of data inside a single file.",
    "crumbs": [
      "Formats",
      "PMTiles",
      "PMTiles"
    ]
  },
  {
    "objectID": "pmtiles/intro.html#internal-compression",
    "href": "pmtiles/intro.html#internal-compression",
    "title": "PMTiles",
    "section": "",
    "text": "PMTiles allows tiles to be stored in the file with compression.",
    "crumbs": [
      "Formats",
      "PMTiles",
      "PMTiles"
    ]
  },
  {
    "objectID": "pmtiles/intro.html#generating-pmtiles",
    "href": "pmtiles/intro.html#generating-pmtiles",
    "title": "PMTiles",
    "section": "",
    "text": "The easiest way to generate PMTiles for vector data is through the tippecanoe tool. This will generate vector tiles that are ideal for visualization, removing small features at low zoom levels to keep tiles a manageable size.\n\n\n\nPMTiles has a command-line program for creating PMTiles if you already have an MBTiles file or a directory of tiles.",
    "crumbs": [
      "Formats",
      "PMTiles",
      "PMTiles"
    ]
  },
  {
    "objectID": "pmtiles/intro.html#using-pmtiles",
    "href": "pmtiles/intro.html#using-pmtiles",
    "title": "PMTiles",
    "section": "",
    "text": "If you have an existing PMTiles archive, either as a local file or hosted on cloud storage, you can use the PMTiles Viewer to inspect the tiles hosted within the file.\n\n\n\nPMTiles doesn’t have a standalone JavaScript library, but rather is designed to be used in conjunction with a JavaScript map rendering library.\nSee the docs on viewing PMTiles in Leaflet, MapLibre GL JS and OpenLayers.\n\n\n\nPMTiles has a Python package, which allows reading and writing PMTiles archives from Python.",
    "crumbs": [
      "Formats",
      "PMTiles",
      "PMTiles"
    ]
  },
  {
    "objectID": "pmtiles/intro.html#alternatives",
    "href": "pmtiles/intro.html#alternatives",
    "title": "PMTiles",
    "section": "",
    "text": "The most common alternative for PMTiles is MBTiles, which was in many ways the precursor to PMTiles. MBTiles stores the included vector tiles in a table in a SQLite database. MBTiles has the benefit of being much easier to use than manually managing millions of tiny, individual files, but MBTiles is not serverless. In general, it’s impossible to read from a SQLite database without fetching the entire file’s content. This means that frontend clients like a web browser couldn’t fetch tiles directly using range requests, but rather a server has to be running to fetch tiles from the MBTiles file.\n\n\n\nIt’s also possible to upload the bare tiled data directly to cloud storage as individual files.\nThis has significant downsides of needing to manage many millions of tiny individual files. Uploading millions of files to a cloud storage provider such as S3 takes time and money. For example, AWS charges $5 per million files added to an S3 bucket. So a 10 million PMTiles archive would cost $50, compared to 5-millionths of a cent to upload the PMTiles file.",
    "crumbs": [
      "Formats",
      "PMTiles",
      "PMTiles"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html",
    "href": "cloud-optimized-netcdf4-hdf5/index.html",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "",
    "text": "The following provides guidance on how to assess and create cloud-optimized HDF5 and NetCDF4 files. This assumes one of those formats is a requirement, usually for archival purposes. If these formats are not a requirement, a cloud-native format like Zarr should be considered. If HDF5/NetCDF-4 formats are a requirement, consider zarr-readable chunk indexes such as kerchunk and VirtualiZarr.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#why-accessing-hdf5-on-the-cloud-is-slow",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#why-accessing-hdf5-on-the-cloud-is-slow",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "Why accessing HDF5 on the cloud is slow",
    "text": "Why accessing HDF5 on the cloud is slow\nIn the diagram below, R0, ..., Rn represent metadata requests. A large number of these requests slows down working with these files in the cloud.\n\n(Barrett et al. 2024)\nWhen reading and writing data from disk, small blocks of metadata and raw data chunks were preferred because access was fast, and retrieving any part of a chunk involved reading the entire chunk (H5py Developers n.d.). However, when this same data is stored in the cloud, performance can suffer due to the high number of requests required to access both metadata and raw data. With network access, reducing the number of requests makes access much more efficient.\nA detailed explanation of current best practices for cloud-optimized HDF5 and NetCDF-4 is provided below, followed by a checklist and some how-to guidance for assessing file layout.\n\n\n\n\n\n\nNote\n\n\n\nNote: NetCDF-4 are valid HDF5 files, see Reading and Editing NetCDF-4 Files with HDF5.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#format",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#format",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "Format",
    "text": "Format\nTo be considered cloud-optimized, the format should support chunking and compression. NetCDF3 and HDF4 prior to v4.1 do not support chunking and chunk-level compression, and thus cannot be reformatted to be cloud optimized. The lack of support for chunking and compression along with other limitations led to the development of NetCDF-4 and HDF5.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#consolidated-internal-file-metadata",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#consolidated-internal-file-metadata",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "Consolidated Internal File Metadata",
    "text": "Consolidated Internal File Metadata\nConsolidated metadata is a key characteristic of cloud-optimized data and enables “lazy loading” (see the Lazy Loading block below). Client libraries use file metadata to understand what’s in the file and where it is stored. When metadata is scattered across a file (which is the default for HDF5 writing), client applications have to make multiple requests for metadata information.\nFor HDF5 files, to consolidate metadata, files should be written with the paged aggregation file space management strategy (see also H5F_FSPACE_STRATEGY_PAGE). When using this strategy, HDF5 will write data in pages where metadata is separated from raw data chunks. Note the page size should also be set, as the default size is 4096 bytes (or 4KB, source). Further, only files using paged aggregation can use the HDF5 page buffer cache – a low-level library cache (Jelenak 2022) – to reduce subsequent data access.\n\n\n\n\n\n\nLazy loading\n\n\n\nLazy loading is a common term for first loading only metadata, and deferring reading of data values until required by computation.\n\n\n\n\n\n\n\n\nHDF5 File Space Management Strategies\n\n\n\nHDF5 file organization—data, metadata, and free space—depends on the file space management strategy. Details on these strategies are in HDF Support: File Space Management.\nHere are a few additional considerations for understanding and implementing the H5F_FSPACE_STRATEGY_PAGE strategy:\n\nChunks vs. Pages: In HDF5, datasets can be chunked, meaning the dataset is divided into smaller blocks of data that can be individually compressed (see also Chunking in HDF5). Pages, on the other hand, represent the smallest unit HDF5 uses for reading and writing data. To optimize performance, chunk sizes should ideally align with the page size or be a multiple thereof. A chunk does not have to fit within a single page, however misalignment leads to chunks spanning multiple pages, which increases read latency. Entire pages are read into memory when accessing chunks or metadata. Only the relevant data (e.g., a specific chunk) is decompressed.\nPage Size Considerations: The page size applies to both metadata and raw data. Therefore, the chosen page size should strike a balance: it must consolidate metadata efficiently while minimizing unused space in raw data chunks. Excess unused space can significantly increase file size. File size is typically not a concern for I/O performance when accessing parts of a file. However, increased file size can become a concern for storage costs.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#chunk-size",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#chunk-size",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "Chunk Size",
    "text": "Chunk Size\nAs described in Chunking in HDF5, datasets in HDF5 can be split into chunks and stored in discrete compressed blocks.\n\nHow to determine chunk size\nThe uncompressed chunk size is calculated by multiplying the chunk dimensions by the size of the data type. For example, a 3-dimensional chunk with dimension lengths 10x100x100 and a float64 data type (8 bytes) results in an uncompressed chunk size of 0.8 MB.\n\n\n\n\n\n\nUncompressed Chunk Size\n\n\n\nWhen designing chunk size, usually the size is for the uncompressed chunk. This is because:\n\nData variability: Because of data variability, you cannot deterministically know the size of each compressed chunk.\nMemory considerations: The uncompressed size determines how much memory must be available for reading and writing each chunk.\n\n\n\n\n\nHow to choose a chunk size\nThere is no one-size-fits all chunk size and shape as files, use cases, and storage systems vary. However, chunks should not be “too big” or “too small”.\n\n\nWhen chunks are too small:\n\nExtra metadata may increase file size.\nIt takes extra time to look up each chunk.\nMore network I/O is incurred because each chunk is stored and accessed independently (although contiguous chunks may be accessed by extending the byte range into one request).\n\n\n\nWhen chunks are too big:\n\nAn entire chunk must be read and decompressed to read even a small portion of the data.\nManaging large chunks in memory slows down processing and is more likely to exceed memory and chunk caches.\n\nA chunk size should be selected that is large enough to reduce the number of tasks that parallel schedulers have to think about (which affects overhead) but also small enough so many can fit in memory at once. The Amazon S3 Best Practices says the typical size for byte-range requests is 8-16MB. However, requests for data from contiguous chunks can be merged into 1 HTTP request, so chunks could be much smaller (one recommendation is 100kb to 2mb) (Jelenak 2024).\n\n\n\n\n\n\nNote\n\n\n\nPerformance greatly depends on libraries used to access the data and how they are configured to cache data as well.\n\n\n\n\nChunk shape vs chunk size\nThe chunk size must be differentiated from the chunk shape, which is the number of values stored along each dimension in a given chunk. Recommended chunk size depends on a storage system’s (such as S3) characteristics and its interaction with the data access library.\nIn contrast, an optimal chunk shape is use case dependent. For a 3-dimensional dataset (latitude, longitude, time) with a chunk size of 1000, chunk shapes could vary, such as:\n\n10 lat x 10 lon x 10 time,\n20 lat x 50 lon x 1 time, or,\n5 lat x 5 lon x 40 time.\n\nLarger chunks in a given dimension improve read performance in that direction: (3) is best for time-series analysis, (2) for mapping, and (1) is balanced for both. Thus, chunk shape should be chosen based on how the data is expected to be used, as there are trade-offs. A useful approach is to think in terms of the chunks’ aspect ratio, adjusting relative dimension lengths to fit the desired optimization for spatial versus time-series analyses (see https://github.com/jbusecke/dynamic_chunks).\n (Shiklomanov 2024)\nA best practice to help determine both chunk size and shape would be to specify some “benchmark use cases” for the data. With these use cases in mind, evaluate what chunk shape and size is large enough such that the computation doesn’t result in thousands of jobs and small enough that multiple chunks can be stored in-memory and a library’s buffer cache, such as HDF5’s buffer cache.\n\n\nAdditional chunk shape and size resources\n\nUnidata Blog: “Chunking Data: Choosing Shapes”\nHDF Support Site: “Chunking in HDF5”\nThe dynamic_chunks module by Julius Busecke may help in determing a chunk shape based on a target size and dimension aspect ratio.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#compression",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#compression",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "Compression",
    "text": "Compression\nCompression is the process of minimizing the size of data stored using an algorithm which can condense data through various methods. This can include scale and offset parameters which reduce the size of each byte that needs to be stored. There are many algorithms for compressing data and users can even define their own compression algorithms. Data product owners should evaluate what compression algorithm is right for their data.\n (Quinn 2024)\nNASA satellite data is predominantly compressed with the zlib (a.k.a., gzip, deflate) method. However, other methods should be explored as a higher compression ratio is often possible, and in the case of HDF5, fills file pages better(Jelenak 2023a).",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#data-product-usage-documentation-tutorials-and-examples",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#data-product-usage-documentation-tutorials-and-examples",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "Data Product Usage Documentation (Tutorials and Examples)",
    "text": "Data Product Usage Documentation (Tutorials and Examples)\nTutorials and examples are starting points for many data users. These documents should include information on how to read data directly from cloud storage (as opposed to downloading over HTTPS) and how to configure popular libraries for optimizing performance.\nFor example, the following library defaults will impact performance and are important to consider:\n\nHDF5 library:\n\nChunk cache: The size of the HDF5’s chunk cache by default is 1MB. This value is configurable. Chunks that don’t fit into the chunk cache are discarded and must be re-read from the storage location each time. See also: Improve HDF5 performance using caching and h5py documentation: Chunk cache.\nPage buffer size: See H5Pset_page_buffer_size and the page_buf_size argument to h5py.File.\n\nS3FS library: The S3FS library is a popular library for accessing data on AWS’s cloud object storage S3. It has a default block size of 5MB (S3FS API docs).\nAdditional guidance on h5py, fsspec, and ROS3 libraries for creating and reading HDF5 can be found in Jelenak (2024).\n\n\nAdditional research\nHere is some additional research done on caching for specific libraries and datasets that may be helpful in understanding the impact of caching and developing product guidance:\n\nIn this issue Optimize s3fs read cache settings for the GEDI Subsetter (findings to be formalized), Chuck Daniels found the “all” cache type (cache entire contents), a block size of 8MB and fill cache=True to deliver the best performance. NOTE: This is for non-cloud-optimized data.\nIn HDF at the Speed of Zarr, Luis Lopez demonstrates, using ICESat-2 data, the importance of using similar arguments with fsspec (blockcache instead of all, but the results in the issue above were not significantly different between these 2 options) as well as the importance of using nearly equivalent arguments in for h5py (raw data chunk cache nbytes and page_buff_size).",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#commands-in-brief",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#commands-in-brief",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "Commands in brief:",
    "text": "Commands in brief:\n\nh5stat prints stats from an existing HDF5 file.\nh5repack writes a new file with a new layout.\nh5dump displays objects from an HDF5 file.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#how-to-check-for-consolidated-metadata",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#how-to-check-for-consolidated-metadata",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "How to check for consolidated metadata",
    "text": "How to check for consolidated metadata\nTo be considered cloud-optimized, HDF5 files should be written with the PAGE file space management strategy (see also File Space Management Strategies). When using this strategy, HDF5 will write aggregate metadata and raw data into fixed-size pages (Jelenak 2023b).\nYou can check the file space management strategy with the command line h5stat tool:\nh5stat -S infile.h5\nThis returns output such as:\nFilename: infile.h5\nFile space management strategy: H5F_FSPACE_STRATEGY_FSM_AGGR\nFile space page size: 4096 bytes\nSummary of file space information:\n  File metadata: 2157376 bytes\n  Raw data: 37784376 bytes\n  Amount/Percent of tracked free space: 0 bytes/0.0%\n  Unaccounted space: 802424 bytes\nTotal space: 40744176 bytes\nNotice the strategy: File space management strategy: H5F_FSPACE_STRATEGY_FSM_AGGR. This is the default option. The best choice for cloud-optimized access is H5F_FSPACE_STRATEGY_PAGE. Learn more about the options in the HDF docs: File Space Management (HDF Group).",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#how-to-change-the-file-space-management-strategy",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#how-to-change-the-file-space-management-strategy",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "How to change the file space management strategy",
    "text": "How to change the file space management strategy\nYou can use the h5repack to reorganize the metadata (Jelenak 2023b). When repacking to use the PAGE file space management strategy, you will also need to specify a page size that will indicate the block size for metadata storage. This should be at least as big as the File metadata value returned from h5stat -S.\n$ h5repack -S PAGE -G 4000000 infile.h5 outfile.h5\nThe HDF5 library needs to be configured to use the page aggregated files. When using the HDF5 library you can set H5Pset_page_buffer_size and for h5py File objects you can set page_buf_size when instantiating the File object.\n\n\n\n\n\n\nLibrary limitations\n\n\n\n\nh5repack’s aggregation is fast but rechunking is slow. You may want to use the h5py library directly to repack. See an example of how to do so in NSIDC’s cloud-optimized ICESat-2 repo: optimize-atl03.py.\nThe NetCDF library doesn’t expose the low-level HDF5 API so one must first create the file with the NetCDF library and then repack it with h5repack or python. You can also create NetCDF files with HDF5 libraries, however required NetCDF properties must be set. See also: Using the HDF5 file space strategy property Unidata/netcdf-c #2871.\n\nauthor credit: Luis Lopez",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#how-to-check-chunk-size-and-shape",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#how-to-check-chunk-size-and-shape",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "How to check chunk size and shape",
    "text": "How to check chunk size and shape\nReplace infile.h5 with a filename on your system and dataset_name with the name of a dataset in that file.\nh5dump -pH infile.h5 | grep dataset_name -A 10\n-p shows the properties (dataset layout, chunk size and compression) of objects in the HDF5 file. -H prints the header information only. Together, we get metadata about the structure of the file.\nExample command with output and how to read it, in comments:\n$ h5dump -pH ~/Downloads/ATL03_20240510223215_08142301_006_01.h5 | grep h_ph -A 10\nDATASET \"h_ph\" {\n  DATATYPE  H5T_IEEE_F32LE # Four-byte, little-endian, IEEE floating point\n  DATASPACE  SIMPLE { ( 15853996 ) / ( H5S_UNLIMITED ) } # simple (n-dimensional) dataspace, this is a 1-dimensional array of length 15,853,996\n  STORAGE_LAYOUT {\n      CHUNKED ( 10000 ) # Dataset is stored in chunks of 10,000 elements per chunk\n      SIZE 57067702 (1.111:1 COMPRESSION) # Compressed datasets compression ratio of 1.111 to 1 - only slightly compresed\n  }\n  FILTERS {\n      COMPRESSION DEFLATE { LEVEL 6 } # Compressed with deflate algorithm with a compression level of 6\n  }\n  ...",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "cloud-optimized-netcdf4-hdf5/index.html#how-to-change-the-chunk-size-and-shape",
    "href": "cloud-optimized-netcdf4-hdf5/index.html#how-to-change-the-chunk-size-and-shape",
    "title": "Cloud-Optimized HDF/NetCDF",
    "section": "How to change the chunk size and shape",
    "text": "How to change the chunk size and shape\n$ h5repack \\\n  # dataset:CHUNK=DIM[xDIM...xDIM]\n  /path/to/dataset:CHUNK=2000 infile.h5 outfile.h5",
    "crumbs": [
      "Formats",
      "Cloud-Optimized HDF/NetCDF",
      "Cloud-Optimized HDF/NetCDF"
    ]
  },
  {
    "objectID": "geoparquet/index.html",
    "href": "geoparquet/index.html",
    "title": "GeoParquet",
    "section": "",
    "text": "GeoParquet is an encoding for how to store geospatial vector data (point, lines, polygons) in Apache Parquet, a popular columnar storage format for tabular data.\nParquet has a wide ecosystem of tools and support; GeoParquet builds on this success by defining how to store geometries in the Parquet format. Because GeoParquet is not a separate format, any program that can read Parquet is able to load GeoParquet as well, even if it can’t make sense of the geometry information. This is very similar to how GeoTIFF layers geospatial information on top of the existing TIFF image standard.\nThe two main things that GeoParquet defines on top of Parquet are how to encode geometries in the geometry column and how to include metadata like the geometries’ Coordinate Reference System (CRS).\nIn September 2023, GeoParquet published a 1.0 release, and now any changes to the specification are expected to be backwards compatible.\nReading and writing GeoParquet has been supported in GDAL since version 3.5, and thus can be used in programs like GeoPandas and QGIS.\n\n\n\n\n\n\nWarning\n\n\n\nIn GeoPandas use read_parquet and to_parquet to read and write GeoParquet, not read_file and to_file as one would use with most other formats. 1\n\n\nBecause GeoParquet stores geometries in standard Well-Known Binary (WKB), it supports any vector geometry type defined in the OGC Simple Features specification. This includes the standard building blocks of Point, LineString, Polygon, MultiPoint, MultiLineString, MultiPolygon, and GeometryCollection. A best practice is to store only geometries with the same type, as that allows readers to know which geometry type is stored without scanning the entire file.\nSome of the sections below will discuss strengths of Parquet in general. Keep in mind that because GeoParquet is built on top of Parquet, GeoParquet inherits all of these strengths.\n\n\nParquet files are laid out differently than other tabular formats like CSV or FlatGeobuf, so it’s helpful to see a diagram:\n\n\n\nSchematic of Parquet file layout\n\n\nA Parquet file consists of a sequence of chunks called row groups. These are logical groups of columns with the same number of rows. A row group consists of multiple columns, each of which is called a column chunk. These are sequences of raw column values that are guaranteed to be contiguous in the file. All row groups in the file must have the same schema, meaning that the data type of each column must be the same for every row group.\nA Parquet file includes metadata describing the internal chunking. This metadata includes the byte range of every column chunk in the dataset. This allows a Parquet reader to fetch any given column chunk once they have the file metadata.\nThe Parquet metadata also includes column statistics (the minimum and maximum value) for each column chunk. This means that if a user is interested in data where column “A” has values greater than 100, the Parquet reader can skip loading and parsing any column chunks where the maximum is known to be less than 100.\nIn Parquet, the metadata is located at the end of the file rather than at the beginning. This makes it much easier to write, as you don’t need to know how many total rows you have at the beginning, but makes it slightly harder to read. In practice, this is not too much more difficult to read: a Parquet reader first reads the end of the file, then makes reads for select columns.\n\n\nThe bytes of each column are contiguous, instead of each row. This means that it’s easy to filter on columns — fetching all rows of a single column — but not possible to filter on individual rows.\n\n\n\nBecause Parquet is column-oriented, a Parquet reader can fetch only specific columns that the user is interested in.\n\n\n\nBecause Parquet is internally chunked, Parquet can fetch only specific row groups that meet a specific filtering condition.\nNote that row group filtering on a specific column tends to only work well if the Parquet file was sorted on that column when saved. Non-sorted columns tend to have random values, and so the column statistics won’t tend to filter out many row groups.\n\n\n\n\n\n\nNote\n\n\n\nIn general it’s only possible to optimize filtering row groups by one column. This is the biggest difference between file formats and databases. Databases can have multiple indexes on whatever columns you want, and then when you run a query, and it will use all of the indexes. But that’s why it’s hard to make databases work as cloud-native files, because if you have high latency, you don’t want to make lots of tiny fetches.\n\n\n\n\n\nParquet is internally compressed by default and Parquet compression is more efficient compared to other formats.\nCompression algorithms are more effective when nearby bytes are more similar to each other. Data within a column tends to be much more similar than data across a row. Since Parquet is column-oriented, compression algorithms work better and result in smaller file sizes than a comparable row-based format.\nIt’s possible to have random access to one of the internal chunks inside the file at large, even though that chunk is compressed. Note that it isn’t possible to fetch partial data inside one chunk without loading and decompressing the entire chunk.\n\n\n\nFor maximum compatibility with existing systems, geometries are stored as ISO-standard WKB. Most geospatial programs are able to read and write WKB.\n\n\n\nGeoParquet is a young specification, and spatial indices are not yet part of the standard. Future revisions of GeoParquet are expected to add support for spatial indexes.\nOne way around this is to store multiple GeoParquet files according to some region identifier, cataloging each file with the SpatioTemporal Asset Catalog (STAC) specification.\n\n\n\nIn a streaming download, you read bytes starting at the beginning of the file, progressing towards the end. In Parquet, this is not helpful because the metadata is in the footer of the file instead of the header.\nInstead, we can replicate something similar to streaming by first fetching only the metadata region at the end of the file, and then making multiple requests for each internal chunk.\n\n\n\nOnce written, a Parquet file is immutable. No modification or appending can happen to that Parquet file. Instead, create a new Parquet file.\n\n\n\nWhile at medium data sizes GeoParquet is most easily distributed as a single file, at large data sizes a single dataset is often split into multiple files. Sometimes multiple files can be easier to write, such as if the data is output from a distributed system.\nA best practice when writing multiple files is to store a top-level metadata file, often named _metadata, with the metadata of all Parquet files in the directory. Without a top-level metadata file, a reader must read the Parquet footer of every individual file in the directory before reading any data. With a metadata file, a Parquet reader can read just that one metadata file, and then read the relevant chunks in the directory. For more information on this, read the “Partitioned Datasets” and “Writing _metadata and _common_metadata files” of the pyarrow documentation. As of August 2023, GeoPandas has no way to write multiple GeoParquet files out of the box, though you may be able to pass a * glob with multiple paths into geopandas.read_parquet.\nStoring Parquet data in multiple files makes it possible to in effect append to the dataset by adding a new file to the directory, but you must be careful to ensure that the new file has the exact same data schema as the existing files, and if a top-level metadata file exists, it must be rewritten to reflect the new file.\nSome elements of how to store GeoParquet-specific metadata in a multi-file layout have not yet been standardized.\n\n\n\nParquet supports a very extensive type system, including nested types such as lists and maps (i.e. like a Python dict). This means that you can store a key-value mapping or a multi-dimensional array within an attribute column of a GeoParquet dataset.\n\n\n\n\n\nDemystifying the Parquet File Format",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet"
    ]
  },
  {
    "objectID": "geoparquet/index.html#file-layout",
    "href": "geoparquet/index.html#file-layout",
    "title": "GeoParquet",
    "section": "",
    "text": "Parquet files are laid out differently than other tabular formats like CSV or FlatGeobuf, so it’s helpful to see a diagram:\n\n\n\nSchematic of Parquet file layout\n\n\nA Parquet file consists of a sequence of chunks called row groups. These are logical groups of columns with the same number of rows. A row group consists of multiple columns, each of which is called a column chunk. These are sequences of raw column values that are guaranteed to be contiguous in the file. All row groups in the file must have the same schema, meaning that the data type of each column must be the same for every row group.\nA Parquet file includes metadata describing the internal chunking. This metadata includes the byte range of every column chunk in the dataset. This allows a Parquet reader to fetch any given column chunk once they have the file metadata.\nThe Parquet metadata also includes column statistics (the minimum and maximum value) for each column chunk. This means that if a user is interested in data where column “A” has values greater than 100, the Parquet reader can skip loading and parsing any column chunks where the maximum is known to be less than 100.\nIn Parquet, the metadata is located at the end of the file rather than at the beginning. This makes it much easier to write, as you don’t need to know how many total rows you have at the beginning, but makes it slightly harder to read. In practice, this is not too much more difficult to read: a Parquet reader first reads the end of the file, then makes reads for select columns.\n\n\nThe bytes of each column are contiguous, instead of each row. This means that it’s easy to filter on columns — fetching all rows of a single column — but not possible to filter on individual rows.\n\n\n\nBecause Parquet is column-oriented, a Parquet reader can fetch only specific columns that the user is interested in.\n\n\n\nBecause Parquet is internally chunked, Parquet can fetch only specific row groups that meet a specific filtering condition.\nNote that row group filtering on a specific column tends to only work well if the Parquet file was sorted on that column when saved. Non-sorted columns tend to have random values, and so the column statistics won’t tend to filter out many row groups.\n\n\n\n\n\n\nNote\n\n\n\nIn general it’s only possible to optimize filtering row groups by one column. This is the biggest difference between file formats and databases. Databases can have multiple indexes on whatever columns you want, and then when you run a query, and it will use all of the indexes. But that’s why it’s hard to make databases work as cloud-native files, because if you have high latency, you don’t want to make lots of tiny fetches.\n\n\n\n\n\nParquet is internally compressed by default and Parquet compression is more efficient compared to other formats.\nCompression algorithms are more effective when nearby bytes are more similar to each other. Data within a column tends to be much more similar than data across a row. Since Parquet is column-oriented, compression algorithms work better and result in smaller file sizes than a comparable row-based format.\nIt’s possible to have random access to one of the internal chunks inside the file at large, even though that chunk is compressed. Note that it isn’t possible to fetch partial data inside one chunk without loading and decompressing the entire chunk.\n\n\n\nFor maximum compatibility with existing systems, geometries are stored as ISO-standard WKB. Most geospatial programs are able to read and write WKB.\n\n\n\nGeoParquet is a young specification, and spatial indices are not yet part of the standard. Future revisions of GeoParquet are expected to add support for spatial indexes.\nOne way around this is to store multiple GeoParquet files according to some region identifier, cataloging each file with the SpatioTemporal Asset Catalog (STAC) specification.\n\n\n\nIn a streaming download, you read bytes starting at the beginning of the file, progressing towards the end. In Parquet, this is not helpful because the metadata is in the footer of the file instead of the header.\nInstead, we can replicate something similar to streaming by first fetching only the metadata region at the end of the file, and then making multiple requests for each internal chunk.\n\n\n\nOnce written, a Parquet file is immutable. No modification or appending can happen to that Parquet file. Instead, create a new Parquet file.\n\n\n\nWhile at medium data sizes GeoParquet is most easily distributed as a single file, at large data sizes a single dataset is often split into multiple files. Sometimes multiple files can be easier to write, such as if the data is output from a distributed system.\nA best practice when writing multiple files is to store a top-level metadata file, often named _metadata, with the metadata of all Parquet files in the directory. Without a top-level metadata file, a reader must read the Parquet footer of every individual file in the directory before reading any data. With a metadata file, a Parquet reader can read just that one metadata file, and then read the relevant chunks in the directory. For more information on this, read the “Partitioned Datasets” and “Writing _metadata and _common_metadata files” of the pyarrow documentation. As of August 2023, GeoPandas has no way to write multiple GeoParquet files out of the box, though you may be able to pass a * glob with multiple paths into geopandas.read_parquet.\nStoring Parquet data in multiple files makes it possible to in effect append to the dataset by adding a new file to the directory, but you must be careful to ensure that the new file has the exact same data schema as the existing files, and if a top-level metadata file exists, it must be rewritten to reflect the new file.\nSome elements of how to store GeoParquet-specific metadata in a multi-file layout have not yet been standardized.\n\n\n\nParquet supports a very extensive type system, including nested types such as lists and maps (i.e. like a Python dict). This means that you can store a key-value mapping or a multi-dimensional array within an attribute column of a GeoParquet dataset.",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet"
    ]
  },
  {
    "objectID": "geoparquet/index.html#references",
    "href": "geoparquet/index.html#references",
    "title": "GeoParquet",
    "section": "",
    "text": "Demystifying the Parquet File Format",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet"
    ]
  },
  {
    "objectID": "geoparquet/index.html#footnotes",
    "href": "geoparquet/index.html#footnotes",
    "title": "GeoParquet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs pointed out by GDAL developer Even Rouault, reading GeoParquet through GDAL is just as fast as reading through the geopandas.read_parquet function if you’re using GDAL’s Arrow API. As of September 2023, this is not the default, so you need to opt into the pyogrio engine and opt into the Arrow API:\nimport geopandas as gpd\ngpd.read_file(\"file.parquet\", engine=\"pyogrio\", use_arrow=True)\nIt’s also necessary to note that the Python wheels distributed by pyogrio do not include the Arrow and Parquet drivers by default. In order to use the pyogrio driver for a GeoParquet file, you need to compile from source when installing. You’ll need to have a GDAL installation version 3.6 or later (and built with Arrow and Parquet support, as seen by ogrinfo --formats) on your computer already, and then you can build pyogrio from source with:\npip install pyogrio --no-binary pyogrio\n↩︎",
    "crumbs": [
      "Formats",
      "GeoParquet",
      "GeoParquet"
    ]
  },
  {
    "objectID": "kerchunk/intro.html",
    "href": "kerchunk/intro.html",
    "title": "Kerchunk",
    "section": "",
    "text": "Kerchunk is a python library for creating reference files (see next paragraph for an explanation) to support cloud-optimized access to traditional geospatial file formats, like NetCDF. Kerchunk negates the need to create and store copies of data for cloud-optimized access. Given the challenge of creating and maintaining copies of data, Kerchunk is a great tool.\nReference files can be used to instantiate an fsspec.FileSystemReference instance. Kerchunk reference files are json files with key value pairs for reading the underlying data as a Zarr data store. The keys are Zarr metadata paths or paths to zarr data chunks. The values for each key will either be raw data values or a list of the file URL, starting byte, and byte length where the data can be read.\nAs Kerchunk creates Zarr metadata for non-Zarr data, Kerchunk is compatible with Zarr tools that can use Zarr. Kerchunk enables a unified way to access chunked, compressed n-dimensionsional data across a variety of conventional data formats. The kerchunk library now supports NetCDF/HDF5, GRIB2, TIFF. Check the File format backends section of the kerchunk documentation for updates to supported formats.\n\n\n\n\n\n\nWarning\n\n\n\nA major limitation of kerchunk is the chunking of data will always be constrained to the chunk structure of the underlying data format. Read about zarr chunks on the Zarr page.\n\n\nLearn more about kerchunk at kerchunk.readthedocs.io.",
    "crumbs": [
      "Formats",
      "Kerchunk",
      "Kerchunk"
    ]
  },
  {
    "objectID": "kerchunk/intro.html#what-is-kerchunk",
    "href": "kerchunk/intro.html#what-is-kerchunk",
    "title": "Kerchunk",
    "section": "",
    "text": "Kerchunk is a python library for creating reference files (see next paragraph for an explanation) to support cloud-optimized access to traditional geospatial file formats, like NetCDF. Kerchunk negates the need to create and store copies of data for cloud-optimized access. Given the challenge of creating and maintaining copies of data, Kerchunk is a great tool.\nReference files can be used to instantiate an fsspec.FileSystemReference instance. Kerchunk reference files are json files with key value pairs for reading the underlying data as a Zarr data store. The keys are Zarr metadata paths or paths to zarr data chunks. The values for each key will either be raw data values or a list of the file URL, starting byte, and byte length where the data can be read.\nAs Kerchunk creates Zarr metadata for non-Zarr data, Kerchunk is compatible with Zarr tools that can use Zarr. Kerchunk enables a unified way to access chunked, compressed n-dimensionsional data across a variety of conventional data formats. The kerchunk library now supports NetCDF/HDF5, GRIB2, TIFF. Check the File format backends section of the kerchunk documentation for updates to supported formats.\n\n\n\n\n\n\nWarning\n\n\n\nA major limitation of kerchunk is the chunking of data will always be constrained to the chunk structure of the underlying data format. Read about zarr chunks on the Zarr page.\n\n\nLearn more about kerchunk at kerchunk.readthedocs.io.",
    "crumbs": [
      "Formats",
      "Kerchunk",
      "Kerchunk"
    ]
  },
  {
    "objectID": "kerchunk/intro.html#why-kerchunk",
    "href": "kerchunk/intro.html#why-kerchunk",
    "title": "Kerchunk",
    "section": "Why Kerchunk?",
    "text": "Why Kerchunk?\nIt is burdensome to create and maintain copies of data. The other pages in this guide introduce data formats which require processing and creating new data products. This process of creating and maintaining new data products, which are essentially copies of existing data, requires time and money. Kerchunk provides a method of providing cloud-optimized access to data that is more traditional archival formats.",
    "crumbs": [
      "Formats",
      "Kerchunk",
      "Kerchunk"
    ]
  },
  {
    "objectID": "kerchunk/intro.html#how-to-kerchunk",
    "href": "kerchunk/intro.html#how-to-kerchunk",
    "title": "Kerchunk",
    "section": "How to kerchunk",
    "text": "How to kerchunk\nAs noted above, kerchunk is a python library you can use to create a reference file from any of the file formats it supports. The reference file is used by the fsspec.ReferenceFileSystem to read data from local or remote storage.\nHere’s an example:\nimport fsspec\nimport json\nfrom kerchunk.hdf import SingleHdf5ToZarr\n\nlocal_file = 'some_data.nc'\nout_file = 'some_references.json'\n\n# Instantiate the local file system with fsspec to save kerchunk's reference data as json.\nfs = fsspec.filesystem('')\nin_file = fs.open(local_file)\n\n# The inline threshold adjusts the size below which binary blocks are included directly in the output.\n# A higher inline threshold can result in a larger json file but faster loading time overally, since fewer requests are made.\nh5chunks = SingleHdf5ToZarr(in_file, local_file, inline_threshold=300)\nwith fs.open(out_file, 'wb') as f:\n    f.write(json.dumps(h5chunks.translate()).encode())\n\n\n\n\n\n\nNote\n\n\n\nThe powerful fsspec library provides a uniform file system interface to many different storage backends and protocols. In addition to abstracting existing protocols, its ReferenceFileSystem class lets you view byte ranges of some other file as a file system. Kerchunk generates these ReferenceFileSystem objects.\n\n\nKerchunk generates a “reference set” which is a set of references to data or URLs under a key value store that matches the Zarr spec. For example, a simple reference file for a NetCDF file might look like:\n{\n  \".zgroup\": \"{\\n    \\\"zarr_format\\\": 2\\n}\",\n  \".zattrs\": \"{\\n    \\\"Conventions\\\": \\\"UGRID-0.9.0\\n\\\"}\",\n  \"x/.zattrs\": \"{\\n    \\\"_ARRAY_DIMENSIONS\\\": [\\n        \\\"node\\\"\\n ...\",\n  \"x/.zarray\": \"{\\n    \\\"chunks\\\": [\\n        9228245\\n    ],\\n    \\\"compressor\\\": null,\\n    \\\"dtype\\\": \\\"&lt;f8\\\",\\n  ...\",\n  \"x/0\": [\"s3://bucket/path/file.nc\", 294094376, 73825960]\n}\nThe [\"s3://bucket/path/file.nc\", 294094376, 73825960] is the key part, which says that to load the first chunk in the x dimension, the Zarr reader needs to fetch a byte range starting at 294094376 with a length of 73825960 bytes. This allows for efficient cloud-native data access without using the standard NetCDF driver.\nLearn more about how to read and write kerchunk reference files in the Kerchunk in Practice notebook.",
    "crumbs": [
      "Formats",
      "Kerchunk",
      "Kerchunk"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html",
    "href": "copc/lidar-las-to-copc.html",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "",
    "text": "The packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda env create -f environment.yml\nor\nmamba env create -f environment.yml\nFinally, you may activate and select the kernel in the notebook (running in Jupyter)\nconda activate coguide-copc\nThe notebook has been tested to work with the listed Conda environment.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html#environment",
    "href": "copc/lidar-las-to-copc.html#environment",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "",
    "text": "The packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda env create -f environment.yml\nor\nmamba env create -f environment.yml\nFinally, you may activate and select the kernel in the notebook (running in Jupyter)\nconda activate coguide-copc\nThe notebook has been tested to work with the listed Conda environment.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html#setup",
    "href": "copc/lidar-las-to-copc.html#setup",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "Setup",
    "text": "Setup\nThis tutorial will explore how to-\n\nRead a LiDAR LAS file using PDAL in Python\nConvert the LiDAR LAS file to Cloud-Optimized Point Cloud (COPC) format\nValidate the generated COPC file",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html#about-the-dataset",
    "href": "copc/lidar-las-to-copc.html#about-the-dataset",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "About the Dataset",
    "text": "About the Dataset\nWe will be using the G-LiHT Lidar Point Cloud V001 from the NASA EarthData. To access NASA EarthData in Jupyter you need to register for an Earthdata account.\nWe will use earthaccess library to set up credentials to fetch data from NASA’s EarthData catalog.\n\nimport earthaccess\nimport os\nimport pdal\n\n/opt/homebrew/anaconda3/envs/coguide-copc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nearthaccess.login()\n\n&lt;earthaccess.auth.Auth at 0x10bfc7d90&gt;",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html#creating-a-data-directory-for-this-tutorial",
    "href": "copc/lidar-las-to-copc.html#creating-a-data-directory-for-this-tutorial",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "Creating a Data Directory for this Tutorial",
    "text": "Creating a Data Directory for this Tutorial\nWe are creating a data directory for downloading all the required files locally.\n\n# set data directory path\ndata_dir = './data'\n\n# check if directory exists -&gt; if directory doesn't exist, directory is created\nif not os.path.exists(data_dir):\n    os.mkdir(data_dir)",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html#downloading-the-dataset-from-earthdata",
    "href": "copc/lidar-las-to-copc.html#downloading-the-dataset-from-earthdata",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "Downloading the Dataset from EarthData",
    "text": "Downloading the Dataset from EarthData\nWe are using search_data method from the earthaccess module for searching the Granules from the selected collection. The temporal argument defines the temporal range for\n\n# Search Granules\n\nlas_item_results = earthaccess.search_data(\n    short_name=\"GLLIDARPC\",\n    version=\"001\",\n    temporal = (\"2020\"), \n    count=3\n)\n\nGranules found: 72\n\n\n\nlas_item_results\n\n[Collection: {'EntryTitle': 'G-LiHT Lidar Point Cloud V001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -81.03452828650298, 'Latitude': 25.50220025425373}, {'Longitude': -81.01391715300757, 'Latitude': 25.50220365895999}, {'Longitude': -81.01391819492625, 'Latitude': 25.5112430715201}, {'Longitude': -81.03453087148995, 'Latitude': 25.511239665437053}, {'Longitude': -81.03452828650298, 'Latitude': 25.50220025425373}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2020-03-11T04:00:00.000Z', 'EndingDateTime': '2020-03-12T03:59:59.000Z'}}\n Size(MB): 238.623\n Data: ['https://e4ftl01.cr.usgs.gov//GWELD1/COMMUNITY/GLLIDARPC.001/2020.03.11/GLLIDARPC_FL_20200311_FIA8_l0s47.las'],\n Collection: {'EntryTitle': 'G-LiHT Lidar Point Cloud V001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -81.02242648723991, 'Latitude': 25.493163090615468}, {'Longitude': -80.99410838333016, 'Latitude': 25.49316468678571}, {'Longitude': -80.99410794242846, 'Latitude': 25.502204110708817}, {'Longitude': -81.02242816553566, 'Latitude': 25.50220251389295}, {'Longitude': -81.02242648723991, 'Latitude': 25.493163090615468}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2020-03-11T04:00:00.000Z', 'EndingDateTime': '2020-03-12T03:59:59.000Z'}}\n Size(MB): 248.383\n Data: ['https://e4ftl01.cr.usgs.gov//GWELD1/COMMUNITY/GLLIDARPC.001/2020.03.11/GLLIDARPC_FL_20200311_FIA8_l0s46.las'],\n Collection: {'EntryTitle': 'G-LiHT Lidar Point Cloud V001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -80.94099075054905, 'Latitude': 25.276201329530473}, {'Longitude': -80.9355627247816, 'Latitude': 25.276199059361314}, {'Longitude': -80.9355579494582, 'Latitude': 25.285238744206318}, {'Longitude': -80.94098637748567, 'Latitude': 25.285241015299494}, {'Longitude': -80.94099075054905, 'Latitude': 25.276201329530473}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2020-03-11T04:00:00.000Z', 'EndingDateTime': '2020-03-12T03:59:59.000Z'}}\n Size(MB): 91.0422\n Data: ['https://e4ftl01.cr.usgs.gov//GWELD1/COMMUNITY/GLLIDARPC.001/2020.03.11/GLLIDARPC_FL_20200311_FIA8_l0s22.las']]\n\n\nLet’s use the file with size 91.04 MB and convert it to a COPC format.\n\n# Download Data - Selecting the 3rd file from the `las_item_results` list\ngliht_las_file = earthaccess.download(las_item_results[2], data_dir)\nlas_filename = gliht_las_file[0]\nprint(las_filename)\n\n Getting 1 granules, approx download size: 0.09 GB\n\n\nQUEUEING TASKS | : 100%|██████████| 1/1 [00:00&lt;00:00, 1869.12it/s]\n\n\nFile GLLIDARPC_FL_20200311_FIA8_l0s22.las already downloaded\n\n\nPROCESSING TASKS | : 100%|██████████| 1/1 [00:00&lt;00:00, 16131.94it/s]\nCOLLECTING RESULTS | : 100%|██████████| 1/1 [00:00&lt;00:00, 33554.43it/s]\n\n\ndata/GLLIDARPC_FL_20200311_FIA8_l0s22.las",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html#a-brief-introduction-to-pdal",
    "href": "copc/lidar-las-to-copc.html#a-brief-introduction-to-pdal",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "A Brief Introduction to PDAL",
    "text": "A Brief Introduction to PDAL\nPDAL (Point Data Abstraction Library) is a C/C++ based open-source library for processing point cloud data. Additionally, it also has a PDAL-Python wrapper to work in a Pythonic environment.\n\nAccessing and Getting Metadata Information\nPDAL CLI provides multiple applications for processing point clouds. Also, it allows chaining of these applications for processing point clouds. Similar to gdal info for TIFFs, we can run pdal info &lt;filename&gt; on the command line for getting metadata from a point cloud file without reading it in memory.\n\n!pdal info {las_filename}\n\n{\n  \"file_size\": 95464691,\n  \"filename\": \"data/GLLIDARPC_FL_20200311_FIA8_l0s22.las\",\n  \"now\": \"2024-03-20T12:30:57-0500\",\n  \"pdal_version\": \"2.6.3 (git-version: Release)\",\n  \"reader\": \"readers.las\",\n  \"stats\":\n  {\n    \"bbox\":\n    {\n      \"EPSG:4326\":\n      {\n        \"bbox\":\n        {\n          \"maxx\": -80.93555795,\n          \"maxy\": 25.28524102,\n          \"maxz\": 69.99,\n          \"minx\": -80.94099075,\n          \"miny\": 25.27619906,\n          \"minz\": -12.54\n        },\n        \"boundary\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -80.940990750549048, 25.276201329530473, -12.54 ], [ -80.940986377485672, 25.285241015299494, -12.54 ], [ -80.9355579494582, 25.285238744206318, 69.99 ], [ -80.935562724781605, 25.276199059361314, 69.99 ], [ -80.940990750549048, 25.276201329530473, -12.54 ] ] ] }\n      },\n      \"native\":\n      {\n        \"bbox\":\n        {\n          \"maxx\": 506487.7363,\n          \"maxy\": 2796533.993,\n          \"maxz\": 69.99,\n          \"minx\": 505941.2263,\n          \"miny\": 2795533.003,\n          \"minz\": -12.54\n        },\n        \"boundary\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ 505941.226302567636594, 2795533.003240843303502, -12.54 ], [ 505941.226302567636594, 2796533.993240843061358, -12.54 ], [ 506487.736302567645907, 2796533.993240843061358, 69.99 ], [ 506487.736302567645907, 2795533.003240843303502, 69.99 ], [ 505941.226302567636594, 2795533.003240843303502, -12.54 ] ] ] }\n      }\n    },\n    \"statistic\":\n    [\n      {\n        \"average\": 506237.8598,\n        \"count\": 3409439,\n        \"maximum\": 506487.7363,\n        \"minimum\": 505941.2263,\n        \"name\": \"X\",\n        \"position\": 0,\n        \"stddev\": 101.3857552,\n        \"variance\": 10279.07135\n      },\n      {\n        \"average\": 2795977.637,\n        \"count\": 3409439,\n        \"maximum\": 2796533.993,\n        \"minimum\": 2795533.003,\n        \"name\": \"Y\",\n        \"position\": 1,\n        \"stddev\": 274.313888,\n        \"variance\": 75248.10912\n      },\n      {\n        \"average\": 2.192797205,\n        \"count\": 3409439,\n        \"maximum\": 69.99,\n        \"minimum\": -12.54,\n        \"name\": \"Z\",\n        \"position\": 2,\n        \"stddev\": 1.788122887,\n        \"variance\": 3.197383461\n      },\n      {\n        \"average\": 30205.96606,\n        \"count\": 3409439,\n        \"maximum\": 65535,\n        \"minimum\": 14789,\n        \"name\": \"Intensity\",\n        \"position\": 3,\n        \"stddev\": 5497.346879,\n        \"variance\": 30220822.71\n      },\n      {\n        \"average\": 1.200336478,\n        \"count\": 3409439,\n        \"maximum\": 5,\n        \"minimum\": 1,\n        \"name\": \"ReturnNumber\",\n        \"position\": 4,\n        \"stddev\": 0.4267302243,\n        \"variance\": 0.1820986843\n      },\n      {\n        \"average\": 1.400683808,\n        \"count\": 3409439,\n        \"maximum\": 5,\n        \"minimum\": 1,\n        \"name\": \"NumberOfReturns\",\n        \"position\": 5,\n        \"stddev\": 0.5529822902,\n        \"variance\": 0.3057894133\n      },\n      {\n        \"average\": 0.5248757933,\n        \"count\": 3409439,\n        \"maximum\": 1,\n        \"minimum\": 0,\n        \"name\": \"ScanDirectionFlag\",\n        \"position\": 6,\n        \"stddev\": 0.4993808847,\n        \"variance\": 0.249381268\n      },\n      {\n        \"average\": 0.002420339534,\n        \"count\": 3409439,\n        \"maximum\": 1,\n        \"minimum\": 0,\n        \"name\": \"EdgeOfFlightLine\",\n        \"position\": 7,\n        \"stddev\": 0.04913738087,\n        \"variance\": 0.002414482199\n      },\n      {\n        \"average\": 1.239596602,\n        \"count\": 3409439,\n        \"maximum\": 2,\n        \"minimum\": 1,\n        \"name\": \"Classification\",\n        \"position\": 8,\n        \"stddev\": 0.4268373506,\n        \"variance\": 0.1821901239\n      },\n      {\n        \"average\": 2.569738893,\n        \"count\": 3409439,\n        \"maximum\": 33,\n        \"minimum\": -31,\n        \"name\": \"ScanAngleRank\",\n        \"position\": 9,\n        \"stddev\": 16.33805559,\n        \"variance\": 266.9320603\n      },\n      {\n        \"average\": 1.475124207,\n        \"count\": 3409439,\n        \"maximum\": 2,\n        \"minimum\": 1,\n        \"name\": \"UserData\",\n        \"position\": 10,\n        \"stddev\": 0.4993808847,\n        \"variance\": 0.249381268\n      },\n      {\n        \"average\": 192.5227238,\n        \"count\": 3409439,\n        \"maximum\": 65535,\n        \"minimum\": 0,\n        \"name\": \"PointSourceId\",\n        \"position\": 11,\n        \"stddev\": 680.5012031,\n        \"variance\": 463081.8874\n      },\n      {\n        \"average\": 310476.1839,\n        \"count\": 3409439,\n        \"maximum\": 310485.0477,\n        \"minimum\": 310469.0825,\n        \"name\": \"GpsTime\",\n        \"position\": 12,\n        \"stddev\": 4.240892261,\n        \"variance\": 17.98516717\n      },\n      {\n        \"average\": 0,\n        \"count\": 3409439,\n        \"maximum\": 0,\n        \"minimum\": 0,\n        \"name\": \"Synthetic\",\n        \"position\": 13,\n        \"stddev\": 0,\n        \"variance\": 0\n      },\n      {\n        \"average\": 0,\n        \"count\": 3409439,\n        \"maximum\": 0,\n        \"minimum\": 0,\n        \"name\": \"KeyPoint\",\n        \"position\": 14,\n        \"stddev\": 0,\n        \"variance\": 0\n      },\n      {\n        \"average\": 0,\n        \"count\": 3409439,\n        \"maximum\": 0,\n        \"minimum\": 0,\n        \"name\": \"Withheld\",\n        \"position\": 15,\n        \"stddev\": 0,\n        \"variance\": 0\n      },\n      {\n        \"average\": 0,\n        \"count\": 3409439,\n        \"maximum\": 0,\n        \"minimum\": 0,\n        \"name\": \"Overlap\",\n        \"position\": 16,\n        \"stddev\": 0,\n        \"variance\": 0\n      }\n    ]\n  }\n}\n\n\n\n\nPDAL Pipelines\nFor converting the LiDAR LAS file to COPC format, we will define a pdal pipeline. A pipeline defines data processing within pdal for reading (using pdal readers), processing (using pdal filters) and writing operations (using pdal writers). The pipelines can also represent sequential operations and can be executed as stages.\nA pdal pipeline is defined in a JSON format either as a JSON object or a JSON array. Below is an example of a pdal pipeline taking a .las file as input, generating stats and writing it to a COPC format.\n{\n  \"pipeline\": [\n    {\n        \"filename\":las_filename,\n        \"type\":\"readers.las\"\n    },\n    {\n        \"type\":\"filters.stats\",\n    },\n    {\n        \"type\":\"writers.copc\",\n        \"filename\":copc_filename\n    }\n]\n}\nThis pipeline can be executed using the pdal pipeline &lt;path_to_json_file&gt; from the command line for a pipeline saved as a local JSON file.\n\n\nProgrammatic Pipeline Construction\nHowever, here we will explore a comparatively easier and Pythonic approach to define a pipeline and execute it. This is based on the PDAL Python extension which provides a programmatic pipeline construction approach in addition to the simple pipeline construction approach discussed above.\nThis approach utilizes the | operator to pipe various stages together representing a pipeline. For eg., the above pipeline can be represented as -\npipeline = pdal.Reader.las(filename=las_filename) | pdal.Writer.copc(filename=copc_filename) | pdal.Filter.stats()\nThis pipeline can be executed using pipeline.execute.",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html#las-to-copc-conversion",
    "href": "copc/lidar-las-to-copc.html#las-to-copc-conversion",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "LAS to COPC Conversion",
    "text": "LAS to COPC Conversion\nNow, let’s dive into converting the LAS file to a COPC format based on the programmatic pipeline construction.\n\n# Defining output filename. Usually, COPC files are saved as .copc.laz\ncopc_filename = las_filename.replace('.las', '.copc.laz')\ncopc_filename\n\n'data/GLLIDARPC_FL_20200311_FIA8_l0s22.copc.laz'\n\n\n\n# pipe = stage 1 | stage 2 | stage 3\n# Or, pipeline = pipeline 1 | stage 2\n\n# Once the pipeline is executed successfully, it prints the count of number of points\npipe = pdal.Reader.las(filename=las_filename) | pdal.Writer.copc(filename=copc_filename)\npipe.execute()\n\n3409439",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html#validation",
    "href": "copc/lidar-las-to-copc.html#validation",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "Validation",
    "text": "Validation\nAs we can see from output of the below cell, the .copc.laz file is created in the destination directory.\n\n# using -go for removing user details and h for getting memory size in MBs\n!ls -goh ./data\n\ntotal 239888\n-rw-r--r--  1     26M Mar 20 11:55 GLLIDARPC_FL_20200311_FIA8_l0s22.copc.laz\n-rw-r--r--  1     91M Feb 29 11:27 GLLIDARPC_FL_20200311_FIA8_l0s22.las\n\n\nLet’s read the created COPC file again and check the value of copc flag from the metadata. If the generated LiDAR file is a valid COPC file, then this flag should be set to True.\n\nvalid_pipe = pdal.Reader.copc(filename=copc_filename) | pdal.Filter.stats()\nvalid_pipe.execute()\n\n# Getting value for the \"copc\" key under the metadata\n# Output is True for a valid COPC\nvalue = valid_pipe.metadata[\"metadata\"][\"readers.copc\"].get(\"copc\")\nprint(value)\n\nTrue",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "copc/lidar-las-to-copc.html#accessing-data",
    "href": "copc/lidar-las-to-copc.html#accessing-data",
    "title": "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)",
    "section": "Accessing Data",
    "text": "Accessing Data\nThe data values can be accessed from the executed pipeline using valid_pipe.arrays. The values in the arrays represent the LiDAR point cloud attributes such as X, Y, Z, and Intensity, etc.\n\narr_values = valid_pipe.arrays\n\n# Print the array values as a dataframe\nprint(arr_values)\n\n[array([(506245.56, 2796471.44, 0.24, 40740, 1, 1, 1, 0, 2, 0, 0, 0, 0,  16.998, 1,   0, 310483.75227621, 0),\n       (506247.16, 2796471.58, 0.27, 35541, 2, 2, 1, 0, 2, 0, 0, 0, 0,  16.998, 1,   0, 310483.75229014, 0),\n       (506247.95, 2796471.65, 0.24, 17716, 2, 2, 1, 0, 2, 0, 0, 0, 0,  16.998, 1,   0, 310483.75229699, 0),\n       ...,\n       (506066.58, 2796032.75, 2.34, 31587, 1, 1, 0, 0, 1, 0, 0, 0, 0, -24.   , 2, 203, 310477.36925451, 0),\n       (506067.37, 2796033.29, 2.52, 32876, 1, 1, 0, 0, 1, 0, 0, 0, 0, -22.998, 2, 216, 310477.37590641, 0),\n       (506062.6 , 2796033.27, 1.4 , 27393, 1, 1, 0, 0, 1, 0, 0, 0, 0, -24.   , 2, 108, 310477.38259945, 0)],\n      dtype=[('X', '&lt;f8'), ('Y', '&lt;f8'), ('Z', '&lt;f8'), ('Intensity', '&lt;u2'), ('ReturnNumber', 'u1'), ('NumberOfReturns', 'u1'), ('ScanDirectionFlag', 'u1'), ('EdgeOfFlightLine', 'u1'), ('Classification', 'u1'), ('Synthetic', 'u1'), ('KeyPoint', 'u1'), ('Withheld', 'u1'), ('Overlap', 'u1'), ('ScanAngleRank', '&lt;f4'), ('UserData', 'u1'), ('PointSourceId', '&lt;u2'), ('GpsTime', '&lt;f8'), ('ScanChannel', 'u1')])]\n\n\nSimilarly, we can get COPC file statistic and log from the executed pipeline using valid_pipe.metadata[\"metadata\"][\"filters.stats\"][\"statistic\"] and valid_pipe.log. The readers are encouraged to explore the results of these operations on their own.\n\n# Getting statistic from the metadata\nvalid_pipe.metadata[\"metadata\"][\"filters.stats\"][\"statistic\"]\n\n[{'average': 506237.8635,\n  'count': 3409439,\n  'maximum': 506487.74,\n  'minimum': 505941.23,\n  'name': 'X',\n  'position': 0,\n  'stddev': 101.3857552,\n  'variance': 10279.07135},\n {'average': 2795977.634,\n  'count': 3409439,\n  'maximum': 2796533.99,\n  'minimum': 2795533,\n  'name': 'Y',\n  'position': 1,\n  'stddev': 274.313888,\n  'variance': 75248.10914},\n {'average': 2.192797205,\n  'count': 3409439,\n  'maximum': 69.99,\n  'minimum': -12.54,\n  'name': 'Z',\n  'position': 2,\n  'stddev': 1.788122887,\n  'variance': 3.197383461},\n {'average': 30205.96606,\n  'count': 3409439,\n  'maximum': 65535,\n  'minimum': 14789,\n  'name': 'Intensity',\n  'position': 3,\n  'stddev': 5497.346879,\n  'variance': 30220822.71},\n {'average': 1.200336478,\n  'count': 3409439,\n  'maximum': 5,\n  'minimum': 1,\n  'name': 'ReturnNumber',\n  'position': 4,\n  'stddev': 0.4267302243,\n  'variance': 0.1820986843},\n {'average': 1.400683808,\n  'count': 3409439,\n  'maximum': 5,\n  'minimum': 1,\n  'name': 'NumberOfReturns',\n  'position': 5,\n  'stddev': 0.5529822902,\n  'variance': 0.3057894133},\n {'average': 0.5248757933,\n  'count': 3409439,\n  'maximum': 1,\n  'minimum': 0,\n  'name': 'ScanDirectionFlag',\n  'position': 6,\n  'stddev': 0.4993808847,\n  'variance': 0.249381268},\n {'average': 0.002420339534,\n  'count': 3409439,\n  'maximum': 1,\n  'minimum': 0,\n  'name': 'EdgeOfFlightLine',\n  'position': 7,\n  'stddev': 0.04913738087,\n  'variance': 0.002414482199},\n {'average': 1.239596602,\n  'count': 3409439,\n  'maximum': 2,\n  'minimum': 1,\n  'name': 'Classification',\n  'position': 8,\n  'stddev': 0.4268373506,\n  'variance': 0.1821901239},\n {'average': 2.569735542,\n  'count': 3409439,\n  'maximum': 33,\n  'minimum': -31.00200081,\n  'name': 'ScanAngleRank',\n  'position': 9,\n  'stddev': 16.33805538,\n  'variance': 266.9320534},\n {'average': 1.475124207,\n  'count': 3409439,\n  'maximum': 2,\n  'minimum': 1,\n  'name': 'UserData',\n  'position': 10,\n  'stddev': 0.4993808847,\n  'variance': 0.249381268},\n {'average': 192.5227238,\n  'count': 3409439,\n  'maximum': 65535,\n  'minimum': 0,\n  'name': 'PointSourceId',\n  'position': 11,\n  'stddev': 680.5012031,\n  'variance': 463081.8874},\n {'average': 310476.1839,\n  'count': 3409439,\n  'maximum': 310485.0477,\n  'minimum': 310469.0825,\n  'name': 'GpsTime',\n  'position': 12,\n  'stddev': 4.240892262,\n  'variance': 17.98516718},\n {'average': 0,\n  'count': 3409439,\n  'maximum': 0,\n  'minimum': 0,\n  'name': 'ScanChannel',\n  'position': 13,\n  'stddev': 0,\n  'variance': 0},\n {'average': 0,\n  'count': 3409439,\n  'maximum': 0,\n  'minimum': 0,\n  'name': 'Synthetic',\n  'position': 14,\n  'stddev': 0,\n  'variance': 0},\n {'average': 0,\n  'count': 3409439,\n  'maximum': 0,\n  'minimum': 0,\n  'name': 'KeyPoint',\n  'position': 15,\n  'stddev': 0,\n  'variance': 0},\n {'average': 0,\n  'count': 3409439,\n  'maximum': 0,\n  'minimum': 0,\n  'name': 'Withheld',\n  'position': 16,\n  'stddev': 0,\n  'variance': 0},\n {'average': 0,\n  'count': 3409439,\n  'maximum': 0,\n  'minimum': 0,\n  'name': 'Overlap',\n  'position': 17,\n  'stddev': 0,\n  'variance': 0}]",
    "crumbs": [
      "Formats",
      "Cloud-Optimized Point Clouds (COPC)",
      "Converting LiDAR LAS Files to Cloud-Optimized Point Clouds (COPCs)"
    ]
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "Template",
    "section": "",
    "text": "Format Basics (or What is a XX?)\n\n\nExample of Creating this Format\n\n\nExample of Cloud-Optimized Access for this Format"
  },
  {
    "objectID": "flatgeobuf/flatgeobuf.html",
    "href": "flatgeobuf/flatgeobuf.html",
    "title": "FlatGeobuf example",
    "section": "",
    "text": "This notebook will give an overview of how to read and write FlatGeobuf files with GeoPandas, putting an emphasis on cloud-native operations where possible.\nThe primary way to interact with FlatGeobuf in Python is via bindings to GDAL, as there is no pure-Python implementation of FlatGeobuf.\nThere are two different Python libraries for interacting between Python and GDAL’s vector support: fiona and pyogrio. Both of these are integrated into geopandas.read_file via the engine keyword, but pyogrio is much faster. Set engine=\"pyogrio\" when using read_file or GeoDataFrame.to_file to speed up reading and writing significantly. We also suggest passing use_arrow=True when reading for a slight extra speedup (this is not supported when writing).",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf example"
    ]
  },
  {
    "objectID": "flatgeobuf/flatgeobuf.html#local-vs-remote-reading",
    "href": "flatgeobuf/flatgeobuf.html#local-vs-remote-reading",
    "title": "FlatGeobuf example",
    "section": "Local vs Remote reading",
    "text": "Local vs Remote reading\nThe cloud-native vector ecosystem is young and doesn’t work as seamlessly as the COG and Zarr for subsetted access. We download data here to demonstrate important concepts but look to update these methods once better datasets and tools are available for working with FlatGeobuf without downloading entire files.\nAt the end of the notebook we have an example with reading via a spatial filter. Keep in mind that using a large spatial filter will cause the read to take a long time.",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf example"
    ]
  },
  {
    "objectID": "flatgeobuf/flatgeobuf.html#environment",
    "href": "flatgeobuf/flatgeobuf.html#environment",
    "title": "FlatGeobuf example",
    "section": "Environment",
    "text": "Environment\nThe packages needed for this notebook can be installed with conda or mamba. Using the environment.yml from this folder run:\nconda create -f environment.yml\nor\nmamba create -f environment.yml\nAlternatively, you can install the versions of pyogrio and geopandas used in this notebook with pip:\npip install pyogrio==0.6.0 geopandas==0.13.2",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf example"
    ]
  },
  {
    "objectID": "flatgeobuf/flatgeobuf.html#imports",
    "href": "flatgeobuf/flatgeobuf.html#imports",
    "title": "FlatGeobuf example",
    "section": "Imports",
    "text": "Imports\n\nfrom tempfile import TemporaryDirectory\nfrom urllib.request import urlretrieve\n\nimport geopandas as gpd\nimport pyogrio",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf example"
    ]
  },
  {
    "objectID": "flatgeobuf/flatgeobuf.html#reading-from-local-disk",
    "href": "flatgeobuf/flatgeobuf.html#reading-from-local-disk",
    "title": "FlatGeobuf example",
    "section": "Reading from local disk",
    "text": "Reading from local disk\nFirst we’ll cover reading FlatGeobuf from local disk storage. As a first example, we’ll use the US counties FlatGeobuf data from this example. This file is only 13 MB in size, which we’ll download to cover simple loading from disk.\n\n# Create a temporary directory in which to save the file\ntmpdir = TemporaryDirectory()\n\n# URL to download\nurl = \"https://flatgeobuf.org/test/data/UScounties.fgb\"\n\n# Download, saving the output path\nlocal_fgb_path, _ = urlretrieve(url, f\"{tmpdir.name}/countries.fgb\")\n\nIn each of the cases below, we use geopandas.read_file to read the file into a GeoDataFrame.\nFirst we’ll show that reading this file with engine=\"fiona\" (the default) is slower. Taking an extra 500 milliseconds might not seem like a lot, but this file contains only 3,000 rows, so this difference gets magnified with larger files.\n\n%time gdf = gpd.read_file(local_fgb_path, engine=\"fiona\")\n\nCPU times: user 565 ms, sys: 16.9 ms, total: 582 ms\nWall time: 685 ms\n\n\nPassing engine=\"pyogrio\" speeds up loading by 18x here!\n\n%time gdf = gpd.read_file(local_fgb_path, engine=\"pyogrio\")\n\nCPU times: user 25.3 ms, sys: 6.84 ms, total: 32.1 ms\nWall time: 31.3 ms\n\n\nUsing use_arrow=True often makes loading slightly faster again! We’re now 21x faster than using fiona.\n\n%time gdf = gpd.read_file(local_fgb_path, engine=\"pyogrio\", use_arrow=True)\n\nCPU times: user 19.7 ms, sys: 10.1 ms, total: 29.7 ms\nWall time: 29.1 ms",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf example"
    ]
  },
  {
    "objectID": "flatgeobuf/flatgeobuf.html#writing-to-local-disk",
    "href": "flatgeobuf/flatgeobuf.html#writing-to-local-disk",
    "title": "FlatGeobuf example",
    "section": "Writing to local disk",
    "text": "Writing to local disk\nSimilarly, we can use GeoDataFrame.to_file to write to a local FlatGeobuf file. As expected, writing can be much faster if you pass engine=\"pyogrio\".\nBy default, this writes a spatial index to the created FlatGeobuf file.\n\n%time gdf.to_file(f\"{tmpdir.name}/out_fiona.fgb\")\n\nCPU times: user 362 ms, sys: 44.4 ms, total: 407 ms\nWall time: 418 ms\n\n\n\n%time gdf.to_file(f\"{tmpdir.name}/out_pyogrio.fgb\", engine=\"pyogrio\")\n\nCPU times: user 60.8 ms, sys: 23.4 ms, total: 84.2 ms\nWall time: 83.5 ms",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf example"
    ]
  },
  {
    "objectID": "flatgeobuf/flatgeobuf.html#reading-from-the-cloud",
    "href": "flatgeobuf/flatgeobuf.html#reading-from-the-cloud",
    "title": "FlatGeobuf example",
    "section": "Reading from the cloud",
    "text": "Reading from the cloud\nKnowing how to read and write local files is important, but given that FlatGeobuf is a cloud-optimized format, it’s important to be able to read from cloud-hosted files as well.\nFor this example, we’ll use the EuroCrops data hosted on Source Cooperative because it has versions of the same data in both FlatGeobuf and GeoParquet format. Hopefully using the same dataset for both the FlatGeobuf and GeoParquet example notebooks will be helpful.\n\nurl = \"https://data.source.coop/cholmes/eurocrops/unprojected/flatgeobuf/FR_2018_EC21.fgb\"\n\nUsually when reading from the cloud, you want to filter on some spatial extent. Pyogrio offers a read_info function to access many pieces of information about the file:\n\npyogrio.read_info(url)\n\n{'crs': 'EPSG:4326',\n 'encoding': 'UTF-8',\n 'fields': array(['ID_PARCEL', 'SURF_PARC', 'CODE_CULTU', 'CODE_GROUP', 'CULTURE_D1',\n        'CULTURE_D2', 'EC_org_n', 'EC_trans_n', 'EC_hcat_n', 'EC_hcat_c'],\n       dtype=object),\n 'dtypes': array(['object', 'float64', 'object', 'object', 'object', 'object',\n        'object', 'object', 'object', 'object'], dtype=object),\n 'geometry_type': 'MultiPolygon',\n 'features': 9517874,\n 'driver': 'FlatGeobuf',\n 'capabilities': {'random_read': 1,\n  'fast_set_next_by_index': 0,\n  'fast_spatial_filter': 1},\n 'layer_metadata': None,\n 'dataset_metadata': None}\n\n\n\n\n\n\n\n\nNote\n\n\n\nSadly the output of read_info does not yet include the bounding box of the file, even though the FlatGeobuf file contains that information in the header. This may be a reason to consider externalizing metadata using Spatio-Temporal Asset Catalog files (STAC) in the future.\n\n\nFor now we’ll hard-code a region around Valence in the south of France, which we know the be within our dataset.\n\n# The order of bounds is\n# (min longitude, min latitude, max longitude, max latitude)\nbounds = (4.301042, 44.822783, 4.410535, 44.877149)\n\nWe can fetch a dataframe containing only the records in these bounds by passing a bbox argument to read_file. Note that the Coordinate Reference System of this bounding box must match the CRS of the dataset. Here, we know from the output of read_info that the CRS of the dataset is EPSG:4326, so we can pass a longitude-latitude bounding box.\n\n%time crops_gdf = gpd.read_file(url, bbox=bounds)\n\nCPU times: user 144 ms, sys: 21.4 ms, total: 165 ms\nWall time: 6 s\n\n\nPassing engine=\"pyogrio\" is only slightly faster, which may mean that most of the time is taken up in network requests, not in parsing the actual data into Python.\n\n%time crops_gdf = gpd.read_file(url, bbox=bounds, engine=\"pyogrio\")\n\nCPU times: user 26.9 ms, sys: 2.98 ms, total: 29.9 ms\nWall time: 490 ms\n\n\nThis gives us a much smaller dataset of only 400 rows (down from 9.5 million rows in the original dataset).\n\ncrops_gdf.head()\n\n\n\n\n\n\n\n\nID_PARCEL\nSURF_PARC\nCODE_CULTU\nCODE_GROUP\nCULTURE_D1\nCULTURE_D2\nEC_org_n\nEC_trans_n\nEC_hcat_n\nEC_hcat_c\ngeometry\n\n\n\n\n0\n9484573\n11.08\nSPL\n17\nNone\nNone\nSurface pastorale - ressources fourragères lig...\nPastoral area - predominant woody fodder resou...\nother_tree_wood_forest\n3306990000\nMULTIPOLYGON (((4.41142 44.85441, 4.41145 44.8...\n\n\n1\n487218\n2.53\nPPH\n18\nNone\nNone\nPrairie permanente - herbe prédominante (resso...\nPermanent pasture - predominantly grass (woody...\npasture_meadow_grassland_grass\n3302000000\nMULTIPOLYGON (((4.41366 44.85898, 4.41373 44.8...\n\n\n2\n487224\n0.89\nCTG\n22\nNone\nNone\nChâtaigne\nChestnut\nsweet_chestnuts\n3303030500\nMULTIPOLYGON (((4.41159 44.85891, 4.41034 44.8...\n\n\n3\n9484542\n1.31\nCTG\n22\nNone\nNone\nChâtaigne\nChestnut\nsweet_chestnuts\n3303030500\nMULTIPOLYGON (((4.40904 44.85805, 4.41034 44.8...\n\n\n4\n487216\n1.70\nBOP\n17\nNone\nNone\nBois pâturé\nGrazed wood\nother_tree_wood_forest\n3306990000\nMULTIPOLYGON (((4.41135 44.85476, 4.41134 44.8...\n\n\n\n\n\n\n\n\ncrops_gdf.shape\n\n(415, 11)\n\n\nThere are other useful keyword arguments to read_file. Since we’re using the pyogrio engine, we can pass specific column names into read_file, and only those columns will be parsed. In the case of FlatGeobuf, this doesn’t save us much time, because the same amount of data needs to be fetched. (Though if running this cell soon after the previous one, the relevant data will be cached and won’t be downloaded again.)\n\ncolumn_names = [\"ID_PARCEL\", \"SURF_PARC\", \"CODE_CULTU\", \"geometry\"]\n%time crops_gdf = gpd.read_file(url, bbox=bounds, columns=column_names, engine=\"pyogrio\")\n\nCPU times: user 25 ms, sys: 2.47 ms, total: 27.4 ms\nWall time: 706 ms\n\n\n\ncrops_gdf.head()\n\n\n\n\n\n\n\n\nCODE_CULTU\nID_PARCEL\nSURF_PARC\ngeometry\n\n\n\n\n0\nSPL\n9484573\n11.08\nMULTIPOLYGON (((4.41142 44.85441, 4.41145 44.8...\n\n\n1\nPPH\n487218\n2.53\nMULTIPOLYGON (((4.41366 44.85898, 4.41373 44.8...\n\n\n2\nCTG\n487224\n0.89\nMULTIPOLYGON (((4.41159 44.85891, 4.41034 44.8...\n\n\n3\nCTG\n9484542\n1.31\nMULTIPOLYGON (((4.40904 44.85805, 4.41034 44.8...\n\n\n4\nBOP\n487216\n1.70\nMULTIPOLYGON (((4.41135 44.85476, 4.41134 44.8...",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf example"
    ]
  },
  {
    "objectID": "flatgeobuf/intro.html",
    "href": "flatgeobuf/intro.html",
    "title": "FlatGeobuf",
    "section": "",
    "text": "FlatGeobuf\nFlatGeobuf is a binary file format for geographic vector data, such as points, lines, and polygons.\nUnlike some formats like Cloud-Optimized GeoTIFF, which builds on the previous success of TIFF and GeoTIFF, FlatGeobuf is a new format, designed from the ground up to be faster for geospatial data.\nFlatGeobuf is widely supported — via its GDAL implementation — in many programming languages as well as applications like QGIS.\nFlatGeobuf supports any vector geometry type defined in the OGC Simple Features specification. This includes the standard building blocks of Point, LineString, Polygon, MultiPoint, MultiLineString, MultiPolygon, and GeometryCollection, but also includes more obscure types such as CircularString, Surface, and TIN (Triangulated irregular network). A best practice is to store only geometries with the same type, as that allows readers to know which geometry type is stored without scanning the entire file.\nAn optional row-based spatial index optimizes for remote reading.\n\nFile layout\nThe internal layout of the file has four sections: magic bytes (aka signature), header, index, and data (aka features).\n\n\nImage source: Horace Williams, Kicking the Tires: Flatgeobuf\n\n\nThe file signature is 8 “magic bytes” indicating the file type and specification version, which allows readers to know a file is FlatGeobuf, even if it’s missing a file extension.\nNext comes the header, which stores the bounding box of the dataset, the geometry type of the features (if known and unique), the attribute schema, the number of features, and coordinate reference system information.\nAfter the file header is an optional spatial index. If included, this lets a reader skip reading features that are not within a provided spatial query.\nLast come the individual features. The rest of the file is a sequence of feature records, placed end to end in a row-wise fashion.\n\n\n\nRow based\nInternally, features are laid out in a row-oriented fashion rather than a column-oriented fasion. This means that it’s relatively cheap to select specific records from the file, but relatively expensive to select a specific column. This is ideal for a small spatial query (assuming an index exists in the file) but to load all geometries requires loading all attribute information as well.\n\n\nNo internal compression\nFlatGeobuf does not support compression while maintaining the ability to seek within the file. In particular, FlatGeobuf’s spatial index describes the byte ranges in the uncompressed file. Those byte ranges will be incorrect if the file is compressed.\nA compression like gzip can be applied to the FlatGeobuf file in full, but keep in mind that storing the compressed file will eliminate random access support.\n\n\nNo append support\nFlatGeobuf is a write-only format, and doesn’t support appending, as that would invalidate the spatial index in the file.\n\n\nRandom access supported via spatial index\nFlatGeobuf optionally supports a spatial index at the beginning of the file, which can speed up reading portions of a file based on a spatial query. For more information on how this spatial index works, refer to the Hilbert R Tree page.\n\n\n\n\n\n\nNote\n\n\n\nNote that because FlatGeobuf has no internal chunking, the spatial index references every single object in the file. This means that for datasets with many small geometries, like points, the spatial index will be very large as a proportion of the file size.\n\n\n\n\n\nStreaming features is supported\nFlatGeobuf supports streaming, meaning that you can use part of the file before the entire file has finished downloading. This is different than random access, because you have no ability to skip around in the file.\nStreaming can be valuable because it makes an application seem more responsive; you can have something happen without having to wait for the full download to complete. A good example of this is this example by FlatGeobuf’s author Björn Harrtell. As the file is downloaded to the browser, portions of the file get rendered progressively in parts.\nThis works even with full-file compression like gzip or deflate because those compression algorithms support streaming decompression.\n\n\nBroad type system\nFlatGeobuf supports attributes with a range of types:\n\nByte: Signed 8-bit integer\nUByte: Unsigned 8-bit integer\nBool: Boolean\nShort: Signed 16-bit integer\nUShort: Unsigned 16-bit integer\nInt: Signed 32-bit integer\nUInt: Unsigned 32-bit integer\nLong: Signed 64-bit integer\nULong: Unsigned 64-bit integer\nFloat: Single precision floating point number\nDouble: Double precision floating point number\nString: UTF8 string\nJson: General JSON type intended to be application specific\nDateTime: ISO 8601 date time\nBinary: General binary type intended to be application specific\n\n\n\n\n\n\n\nNote\n\n\n\nNote that FlatGeobuf is unable to store nested types without overhead. It doesn’t support a “list” or “dict” type apart from JSON, which has a parsing overhead.\nIn some situations, having strong nested type support can be useful. For example STAC stored as GeoParquet has columns that are nested, such as the assets column that needs to store a dictionary-like mapping from asset names to their information. FlatGeobuf is able to store such data by serializing it to JSON, but it’s not possible to see the nested schema before parsing the full dataset.\n\n\n\n\nKnown table schema\nFlatGeobuf declares the schema of properties at the beginning of the file. This makes it much easier to read the file — compared to a fully schemaless format like GeoJSON — because the reader knows what data type each attribute has in advance.\n\n\nReferences\n\nflatgeobuf.org: Official project website.\nFlatgeobuf: Implementer’s Guide",
    "crumbs": [
      "Formats",
      "FlatGeobuf",
      "FlatGeobuf"
    ]
  }
]